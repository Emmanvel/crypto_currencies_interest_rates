{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iq5dgkr8edh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# Cryptocurrencies and Interest Rates: Inferring Yield Curves in a Bondless Market\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2509.03964-b31b1b.svg)](https://arxiv.org/abs/2509.03964)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/crypto_currencies_interest_rates)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Mathematical%20Finance-blue)](https://github.com/chirindaopensource/crypto_currencies_interest_rates)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Robust%20Statistics%20%26%20Derivatives%20Pricing-orange)](https://github.com/chirindaopensource/crypto_currencies_interest_rates)\n",
        "[![Data Source](https://img.shields.io/badge/Data-Deribit%20%7C%20Aave-lightgrey)](https://www.deribit.com/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/Statsmodels-15588D.svg?style=flat)](https://www.statsmodels.org/stable/index.html)\n",
        "[![PyYAML](https://img.shields.io/badge/PyYAML-4B5F6E.svg?style=flat)](https://pyyaml.org/)\n",
        "\n",
        "--\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/crypto_currencies_interest_rates`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Cryptocurrencies and Interest Rates: Inferring Yield Curves in a Bondless Market\"** by:\n",
        "\n",
        "*   Philippe Bergault\n",
        "*   Sébastien Bieber\n",
        "*   Olivier Guéant\n",
        "*   Wenkai Zhang\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for constructing cryptocurrency yield curves from derivatives market data. It delivers a modular, auditable, and extensible pipeline that replicates the paper's entire workflow: from rigorous data validation and cleansing, through robust outlier detection via RANSAC, to the closed-form analytical estimation of zero-coupon bond prices and their conversion to a continuous, daily time series of interest rates.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: run_full_analysis](#key-callable-run_full_analysis)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Cryptocurrencies and Interest Rates: Inferring Yield Curves in a Bondless Market.\" The core of this repository is the iPython Notebook `crypto_currencies_interest_rates_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final generation and analysis of yield curve time series.\n",
        "\n",
        "The paper addresses a fundamental challenge in decentralized finance: how to determine a term structure of interest rates for a currency that lacks a traditional, liquid bond market. This codebase operationalizes the paper's derivatives-based approach, allowing users to:\n",
        "-   Rigorously validate and cleanse high-frequency options market data.\n",
        "-   Apply the Random Sample Consensus (RANSAC) algorithm to robustly filter outliers from option price data based on a modified put-call parity relationship.\n",
        "-   Solve a weighted least-squares problem with a closed-form analytical solution to jointly estimate the prices of synthetic zero-coupon bonds in both the cryptocurrency and a reference currency (USD).\n",
        "-   Convert these bond prices into annualized, continuously compounded interest rates.\n",
        "-   Aggregate and interpolate these granular estimates to construct a continuous, daily time series of the yield curve for standardized tenors.\n",
        "-   Perform a full suite of robustness checks, including parameter sensitivity analysis and bootstrap confidence intervals, to validate the stability and precision of the estimates.\n",
        "-   Conduct economic validation by comparing the derived rates to external benchmarks and performing formal econometric tests.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in Arbitrage Pricing Theory, Financial Engineering, and Robust Statistics.\n",
        "\n",
        "**1. Modified Put-Call Parity for Inverse Options:**\n",
        "The cornerstone of the methodology is a static replication argument. A specific portfolio of an inverse call, an inverse put, and an inverse future on a cryptocurrency `C` creates a deterministic payoff in a reference currency (USD). Under the no-arbitrage principle, the cost of this portfolio today must equal the discounted value of its future payoff. This leads to the central pricing relationship, a modified form of put-call parity:\n",
        "$$ C_t(K, T) - P_t(K, T) = \\frac{F_t(T) - K}{S_t} \\cdot ZC_t^{ref}(T) $$\n",
        "where $ZC_t^{ref}(T)$ is the unknown price of a zero-coupon bond in the reference currency.\n",
        "\n",
        "**2. Robust Regression with RANSAC:**\n",
        "Market data is noisy and contains outliers. To robustly identify the underlying relationship in the data before estimation, the paper uses the Random Sample Consensus (RANSAC) algorithm. This iterative method fits a model to minimal subsets of the data to find the largest \"consensus set\" of inliers, effectively isolating and ignoring outliers. This is applied to a linear relationship derived from put-call parity for bid-ask prices.\n",
        "\n",
        "**3. Joint Weighted Least-Squares Estimation:**\n",
        "The prices of the crypto zero-coupon bond ($ZC_t^C(T) = \\alpha_t$) and the reference currency zero-coupon bond ($ZC_t^{ref}(T) = \\beta_t$) are estimated by jointly minimizing the errors in two parity relationships across all inlier strike prices for a given expiry. This is formulated as the minimization of the objective function:\n",
        "$$ \\arg\\min_{\\alpha_t, \\beta_t} \\sum_{i=1}^{n_T} \\left(y_t^i - \\left(\\alpha_t - \\frac{K_i}{S_t} \\beta_t\\right)\\right)^2 + \\lambda_n^T \\left(\\alpha_t - \\frac{F_t}{S_t} \\beta_t\\right)^2 $$\n",
        "The paper provides a closed-form analytical solution to this problem, which is implemented directly.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`crypto_currencies_interest_rates_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Task-Based Architecture:** The entire pipeline is broken down into 22 distinct, modular tasks, from data validation to economic analysis.\n",
        "-   **Professional-Grade Data Validation:** A comprehensive validation suite ensures all inputs (data and configurations) conform to the required schema before execution.\n",
        "-   **Auditable Data Filtering:** A multi-stage filtering pipeline for maturity and liquidity, returning detailed diagnostic reports at each step.\n",
        "-   **From-Scratch RANSAC Implementation:** A numerically stable, from-scratch implementation of the RANSAC algorithm for maximum control and fidelity to the paper's methodology.\n",
        "-   **High-Fidelity Analytical Solver:** A direct and numerically robust implementation of the closed-form solution for the zero-coupon bond prices.\n",
        "-   **Complete Term Structure Construction:** A full suite of functions for temporal aggregation, maturity interpolation, and time-series gap-filling to construct a continuous daily yield curve panel.\n",
        "-   **Advanced Robustness Toolkit:**\n",
        "    -   A parallelized framework for conducting **parameter sensitivity analysis** across a grid of hyperparameter values.\n",
        "    -   A parallelized framework for constructing **bootstrap confidence intervals** to quantify the statistical uncertainty of the estimates.\n",
        "-   **Econometric Validation Suite:** Functions to perform economic plausibility checks, compare results to external benchmarks, and conduct formal statistical tests (ADF, Cointegration).\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Data Validation and Filtering (Tasks 1-6):** Ingests and rigorously validates the raw data and `config.yaml` file, then filters the data based on maturity, liquidity, and the existence of complete call-put pairs.\n",
        "2.  **RANSAC Outlier Rejection (Tasks 7-9):** Prepares the data for RANSAC, executes the algorithm for each timestamp-expiry group, and validates the results to find a clean set of inlier options.\n",
        "3.  **Closed-Form Estimation (Tasks 10-12):** Constructs feature vectors from the inlier data and applies the analytical solution to estimate the zero-coupon bond prices (`α*`, `β*`).\n",
        "4.  **Rate Conversion and Validation (Tasks 13-14):** Converts the bond prices to continuously compounded interest rates and performs final economic plausibility checks.\n",
        "5.  **Time Series Construction (Tasks 15-18):** Aggregates the granular estimates into a daily panel, interpolates across maturities to standardized tenors, and fills temporal gaps to create a continuous time series.\n",
        "6.  **Robustness and Economic Analysis (Tasks 20-22):** Executes the optional, advanced analyses for parameter sensitivity, bootstrap CIs, and economic validation.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `crypto_currencies_interest_rates_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: run_full_analysis\n",
        "\n",
        "The central function in this project is `run_full_analysis`. It orchestrates the entire analytical workflow, providing a single entry point for running the baseline study replication and the advanced robustness checks.\n",
        "\n",
        "```python\n",
        "def run_full_analysis(\n",
        "    df_master: pd.DataFrame,\n",
        "    fused_input_state: Dict[str, Any],\n",
        "    parameter_grid: Optional[Dict[str, List[Any]]] = None,\n",
        "    bootstrap_group: Optional[Tuple] = None,\n",
        "    n_bootstrap: int = 1000,\n",
        "    benchmark_df: Optional[pd.DataFrame] = None,\n",
        "    verbose: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the entire end-to-end yield curve analysis.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `statsmodels`, `pyyaml`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/crypto_currencies_interest_rates.git\n",
        "    cd crypto_currencies_interest_rates\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy statsmodels pyyaml\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires two primary inputs:\n",
        "1.  **`df_master`:** A `pandas.DataFrame` containing high-frequency options market data. It **must** have a `MultiIndex` with the levels `['timestamp', 'asset', 'expiry_date', 'strike_price', 'option_type']` and the required data columns.\n",
        "2.  **`fused_input_state`:** A Python dictionary loaded from the `config.yaml` file, which controls all hyperparameters of the pipeline.\n",
        "\n",
        "A mock data generation code block is provided in the main notebook to create a valid example `df_master` for testing the pipeline.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `crypto_currencies_interest_rates_draft.ipynb` notebook provides a complete, step-by-step guide. The core workflow is:\n",
        "\n",
        "1.  **Prepare Inputs:** Load your `df_master` DataFrame. Ensure the `config.yaml` file is present and configured.\n",
        "2.  **Execute Pipeline:** Call the grand orchestrator function.\n",
        "\n",
        "    ```python\n",
        "    # This single call runs the entire project.\n",
        "    full_report = run_full_analysis(\n",
        "        df_master=my_market_data_df,\n",
        "        fused_input_state=my_config_dict,\n",
        "        verbose=True\n",
        "    )\n",
        "    ```\n",
        "3.  **Inspect Outputs:** Programmatically access any result from the returned dictionary. For example, to view the final yield curve time series:\n",
        "    ```python\n",
        "    final_yield_curves = full_report['core_pipeline_results']['yield_curves_df']\n",
        "    print(final_yield_curves.head())\n",
        "    ```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `run_full_analysis` function returns a single, comprehensive dictionary containing all generated artifacts, including:\n",
        "-   `core_pipeline_results`: A dictionary containing the final `yield_curves_df` and a nested dictionary of diagnostics from every step of the main pipeline run.\n",
        "-   `sensitivity_analysis`: (Optional) A dictionary containing summary DataFrames for rate and coverage stability.\n",
        "-   `bootstrap_analysis`: (Optional) A dictionary with the point estimate and confidence intervals for the targeted group.\n",
        "-   `economic_validation`: (Optional) A dictionary with the results of the theoretical, benchmark, and statistical tests.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "crypto_currencies_interest_rates/\n",
        "│\n",
        "├── crypto_currencies_interest_rates_draft.ipynb   # Main implementation notebook\n",
        "├── config.yaml                                    # Master configuration file\n",
        "├── requirements.txt                               # Python package dependencies\n",
        "├── LICENSE                                        # MIT license file\n",
        "└── README.md                                      # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can easily modify all hyperparameters, such as filter thresholds, RANSAC parameters, and target tenors, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "\n",
        "-   **Alternative Interpolation Methods:** Implementing more advanced term structure interpolation methods, such as cubic splines or the Nelson-Siegel model, as alternatives to linear interpolation.\n",
        "-   **Generalization to Other Derivatives:** Extending the framework to incorporate data from other relevant derivatives, such as perpetual futures or different option structures.\n",
        "-   **Automated Reporting:** Creating a function that takes the final `full_analysis_report` dictionary and generates a full PDF or HTML report summarizing the findings with plots and tables.\n",
        "-   **Live Data Integration:** Building a data ingestion module to connect the pipeline to a live market data feed from an exchange API for real-time yield curve monitoring.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{bergault2025cryptocurrencies,\n",
        "  title={{Cryptocurrencies and Interest Rates: Inferring Yield Curves in a Bondless Market}},\n",
        "  author={Bergault, Philippe and Bieber, S{\\'e}bastien and Gu{\\'e}ant, Olivier and Zhang, Wenkai},\n",
        "  journal={arXiv preprint arXiv:2509.03964},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation for Inferring Cryptocurrency Yield Curves.\n",
        "GitHub repository: https://github.com/chirindaopensource/crypto_currencies_interest_rates\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Philippe Bergault, Sébastien Bieber, Olivier Guéant, and Wenkai Zhang** for their foundational research, which forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, and Statsmodels**, whose work makes complex computational finance accessible and robust.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `crypto_currencies_interest_rates_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "7oTkG_XJDWOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Cryptocurrencies and Interest Rates: Inferring Yield Curves in a Bondless Market*\"\n",
        "\n",
        "Authors: Philippe Bergault, Sébastien Bieber, Olivier Guéant, Wenkai Zhang\n",
        "\n",
        "E-Journal Submission Date: 4 September 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2509.03964\n",
        "\n",
        "Abstract:\n",
        "\n",
        "In traditional financial markets, yield curves are widely available for countries (and, by extension, currencies), financial institutions, and large corporates. These curves are used to calibrate stochastic interest rate models, discount future cash flows, and price financial products. Yield curves, however, can be readily computed only because of the current size and structure of bond markets. In cryptocurrency markets, where fixed-rate lending and bonds are almost nonexistent as of early 2025, the yield curve associated with each currency must be estimated by other means. In this paper, we show how mathematical tools can be used to construct yield curves for cryptocurrencies by leveraging data from the highly developed markets for cryptocurrency derivatives."
      ],
      "metadata": {
        "id": "zXcFHncW8zRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Summary of \"Cryptocurrencies and Interest Rates: Inferring Yield Curves in a Bondless Market\"**\n",
        "\n",
        "This paper presents a novel framework for constructing yield curves for cryptocurrencies and their associated reference currencies (in this case, a form of digital USD) in a market environment that completely lacks traditional fixed-income instruments like government or corporate bonds.\n",
        "\n",
        "--\n",
        "\n",
        "#### **The Fundamental Problem and Motivation**\n",
        "\n",
        "In traditional finance (\"TradFi\"), the term structure of interest rates, or the yield curve, is the bedrock of valuation. It tells us the \"price of money\" over different time horizons. We typically derive it by \"bootstrapping\" from the prices of default-free government bonds of various maturities. This curve is essential for:\n",
        "*   Discounting future cash flows to find their present value.\n",
        "*   Pricing all manner of derivatives.\n",
        "*   Calibrating stochastic interest rate models for risk management.\n",
        "\n",
        "The cryptocurrency ecosystem, however, has a structural void: there is no sovereign issuer of crypto-denominated bonds. As of early 2025, a liquid market for fixed-rate, fixed-term debt in assets like Bitcoin (BTC) or Ethereum (ETH) is nonexistent. This raises a critical question: **How can one rationally discount a future cash flow denominated in BTC or ETH?**\n",
        "\n",
        "The authors' central thesis is that while a bond market is absent, a highly liquid and sophisticated *derivatives* market has emerged. Their work proposes to use the prices from this market to reverse-engineer the very interest rate information that is normally a prerequisite for pricing those derivatives.\n",
        "\n",
        "The paper also provides an excellent, detailed historical overview of interest rates, from ancient Sumeria to the modern era. The key takeaway from this section is that the existence of a clean, easily observable yield curve is a very recent historical development, contingent on the rise of standardized government debt. Cryptocurrencies, in a sense, return us to a state where the term structure is not directly observable and must be inferred.\n",
        "\n",
        "--\n",
        "\n",
        "#### **The Proposed Methodology - A Return to First Principles**\n",
        "\n",
        "The core of the paper's methodology is a clever adaptation of one of the most fundamental no-arbitrage relationships in finance: **call-put parity**.\n",
        "\n",
        "1.  **Revisiting Call-Put Parity for Inverse Options:** The authors focus on the Deribit exchange, the dominant platform for crypto options. Crucially, Deribit lists *inverse options*.\n",
        "    *   A standard (linear) option has a strike price `K` and payoff denominated in the reference currency (e.g., USD).\n",
        "    *   An inverse option has a strike `K` in USD, but its payoff is delivered in the underlying cryptocurrency (e.g., BTC). The payoff for an inverse call is `max(0, 1/K - 1/S_T)` in USD terms, or `max(0, S_T/K - 1)` in BTC terms.\n",
        "\n",
        "    By constructing a portfolio of a long inverse call, a short inverse put, and a short inverse futures contract (all with the same strike `K` and maturity `T`), the authors show that the terminal payoff is a deterministic amount of the *reference currency* (USD). This leads to a modified call-put parity relationship:\n",
        "\n",
        "    `C_t(K,T) – P_t(K,T) = (F_t(T) - K)/S_t * ZC_t^{ref}(T)`\n",
        "\n",
        "    Where:\n",
        "    *   `C_t` and `P_t` are the call and put prices in the crypto asset (e.g., BTC).\n",
        "    *   `F_t(T)` is the futures price.\n",
        "    *   `S_t` is the spot price.\n",
        "    *   `ZC_t^{ref}(T)` is the price of a zero-coupon bond in the **reference currency (USD)** maturing at `T`. This is the first quantity they want to find.\n",
        "\n",
        "2.  **Linking Reference and Crypto Yield Curves:** Next, they use another no-arbitrage argument involving a futures contract to link the USD zero-coupon bond to the cryptocurrency zero-coupon bond. This yields the second key relationship:\n",
        "\n",
        "    `ZC_t^{crypto}(T) = (F_t(T)/S_t) * ZC_t^{ref}(T)`\n",
        "\n",
        "    Where `ZC_t^{crypto}(T)` is the price of a zero-coupon bond in the **cryptocurrency** itself. This is the second, and ultimate, quantity of interest.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Econometric Estimation and Data Filtering**\n",
        "\n",
        "The theoretical parity relationships are elegant, but real-world market data is noisy, subject to bid-ask spreads, and contains pricing errors. A naive application would fail. The authors implement a robust two-stage estimation procedure:\n",
        "\n",
        "1.  **Robust Data Filtering with RANSAC:** Before estimation, they must clean the data. They employ a powerful algorithm from computer vision and statistics called **Random Sample Consensus (RANSAC)**. This iterative method fits a model to random subsets of the data to identify a consensus set of \"inliers\" that conform to the expected theoretical relationship, while systematically rejecting outliers. This is a critical step for dealing with the often-erratic nature of crypto market data.\n",
        "\n",
        "2.  **Joint Minimization:** For a given maturity `T`, there are dozens of options with different strikes `K`. The authors formulate a weighted least-squares problem. They simultaneously estimate the two unknown zero-coupon bond prices (`ZC^{ref}` and `ZC^{crypto}`) by minimizing the sum of squared errors across all available option strikes and the futures pricing relationship. This joint estimation provides more stable and robust estimates than trying to solve for the rates from a single instrument.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Empirical Results and Key Findings**\n",
        "\n",
        "The authors apply this methodology to hourly top-of-book data from Deribit for BTC and ETH options from January 2022 to December 2024. Their findings are compelling:\n",
        "\n",
        "1.  **Deribit USD Yield Curve:** They successfully extract a dynamic, time-varying yield curve for USD as priced on the Deribit platform. This \"Deribit USD\" rate is not the same as the US Treasury rate; it implicitly contains the credit risk of the Deribit platform. The rates are shown to be economically sensible, rising during periods of market stress and exhibiting a general correlation with rates on decentralized lending platforms like Aave.\n",
        "\n",
        "2.  **Bitcoin (BTC) Implied Yield Curve:** The implied interest rates for BTC are consistently found to be very close to zero across all maturities. This aligns perfectly with economic intuition. BTC is a non-productive asset; it has no native yield mechanism (like staking). Holding it is analogous to holding digital gold, which also has a near-zero (or even negative) interest rate.\n",
        "\n",
        "3.  **Ethereum (ETH) Implied Yield Curve (The Puzzle):** This is the most surprising result. The implied interest rates for ETH are *also* found to be near-zero. This is puzzling because, unlike Bitcoin, Ethereum has a significant native staking yield (typically 2-4%) for validators who secure the network. One would expect the derivatives market to price this in, leading to a consistently positive interest rate. The authors acknowledge this discrepancy, suggesting it as a key area for future research. It may point to market segmentation, where derivatives traders do not fully incorporate on-chain yield opportunities into their pricing.\n",
        "\n",
        "\n",
        "#### **Conclusion and Implications**\n",
        "\n",
        "This paper makes a significant contribution by providing the first systematic, theoretically-grounded framework for estimating yield curves in a bondless market.\n",
        "\n",
        "*   **Main Contribution:** It demonstrates that the fundamental principles of no-arbitrage pricing can be adapted to the unique structure of cryptocurrency markets to extract vital economic information that is not directly observable.\n",
        "*   **Key Insight:** The derived interest rates are not universal \"risk-free\" rates but are inherently **platform-specific**. The \"Deribit USD\" rate reflects USD interest plus Deribit's credit risk. This opens the door to analyzing the spread between rates inferred from different platforms (e.g., Deribit vs. Binance) as a market-implied measure of relative platform risk.\n",
        "*   **Future Work:** The \"ETH staking puzzle\" remains the most significant open question. Resolving it will likely yield deeper insights into the efficiency and integration of on-chain and off-chain crypto financial markets.\n",
        "\n",
        "In essence, the paper provides a crucial toolkit for bringing rigorous quantitative analysis to the pricing and risk management of crypto assets, moving the field beyond speculative metrics and toward a more mature, valuation-driven framework."
      ],
      "metadata": {
        "id": "8N4TrUbr9aRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules\n"
      ],
      "metadata": {
        "id": "TpculyXuXkdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  Cryptocurrencies and Interest Rates: Inferring Yield Curves in a Bondless Market\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Cryptocurrencies and Interest Rates:\n",
        "#  Inferring Yield Curves in a Bondless Market\" by Bergault, Bieber, Guéant,\n",
        "#  and Zhang (2025). It delivers a robust system for constructing cryptocurrency\n",
        "#  yield curves from derivatives data in the absence of a traditional bond market,\n",
        "#  enabling consistent pricing, risk management, and the development of more\n",
        "#  sophisticated financial instruments in the digital asset ecosystem.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Synthetic Zero-Coupon Bond Replication via Inverse Options and Futures.\n",
        "#  • Modified Call-Put Parity relationship for inverse derivative contracts.\n",
        "#  • Robust Outlier Rejection using the Random Sample Consensus (RANSAC) algorithm.\n",
        "#  • Joint Weighted Least-Squares Estimation with a closed-form analytical solution.\n",
        "#  • Conversion of zero-coupon bond prices to continuously compounded interest rates.\n",
        "#  • Temporal aggregation and interpolation to construct a continuous term structure.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Comprehensive, multi-stage data validation (structural, economic, completeness).\n",
        "#  • Modular, auditable pipeline with detailed diagnostics at each stage.\n",
        "#  • Numerically stable implementation of the RANSAC algorithm and analytical solver.\n",
        "#  • Parallelized frameworks for computationally intensive robustness analyses,\n",
        "#    including parameter sensitivity and bootstrap confidence intervals.\n",
        "#  • Suite of economic validation and econometric tests for final model assessment.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Bergault, P., Bieber, S., Guéant, O., & Zhang, W. (2025). Cryptocurrencies\n",
        "#  and Interest Rates: Inferring Yield Curves in a Bondless Market.\n",
        "#  arXiv preprint arXiv:2509.03964.\n",
        "#  https://arxiv.org/abs/2509.03964\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# ==============================================================================#\n",
        "# Consolidated Imports for the End-to-End Yield Curve Pipeline\n",
        "# ==============================================================================#\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "# For robust type hinting and annotation across all functions.\n",
        "from typing import List, Dict, Any, Set, Tuple, Optional, Union\n",
        "# For creating detailed diagnostic reports on rejected items.\n",
        "from collections import Counter\n",
        "# For creating deep copies of configuration objects in parallel processing.\n",
        "import copy\n",
        "# For generating parameter grids for sensitivity analysis.\n",
        "import itertools\n",
        "# For robust floating-point comparisons.\n",
        "import math\n",
        "# For parallel execution of computationally intensive tasks.\n",
        "from multiprocessing import Pool, cpu_count\n",
        "# For pretty-printing diagnostic dictionaries.\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "# --- Third-Party Library Imports ---\n",
        "# The core library for numerical operations, array manipulation, and linear algebra.\n",
        "import numpy as np\n",
        "# The core library for data manipulation, time-series analysis, and data structures.\n",
        "import pandas as pd\n",
        "# Provides robust, specific functions for data type validation.\n",
        "import pandas.api.types as ptypes\n",
        "# The primary library for advanced statistical and econometric testing.\n",
        "from statsmodels.tsa.stattools import adfuller, coint\n"
      ],
      "metadata": {
        "id": "F83KlvfGXod9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "DCdIk206XqCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Functional Decomposition of Key Pipeline Orchestrators using the Inputs-Processes-Outputs (IPO) Model**\n",
        "\n",
        "#### **Task 1: `validate_dataframe_structure`**\n",
        "\n",
        "*   **Inputs:** A `pandas.DataFrame` (`df_master`) intended for analysis.\n",
        "*   **Processes:**\n",
        "    1.  **MultiIndex Validation:** Systematically inspects the DataFrame's index to confirm it is a `MultiIndex` with exactly 5 levels named `['timestamp', 'asset', 'expiry_date', 'strike_price', 'option_type']` in that order. It verifies the `dtype` of each level (e.g., `timestamp` is `datetime64`) and the values within categorical levels (e.g., `asset` is in `{'BTC', 'ETH'}`).\n",
        "    2.  **Column Validation:** Checks for the presence and correct `dtype` of all required data columns (`instrument_name`, `best_bid_price`, etc.). It further validates that all numeric columns contain only finite, positive values.\n",
        "    3.  **Economic Constraint Validation:** Performs vectorized checks across the DataFrame to enforce fundamental market logic: `best_ask_price >= best_bid_price` and `expiry_date > timestamp`. It also validates timezone consistency between the temporal index levels.\n",
        "*   **Transformation:** This function is a pure validation gate; it does not transform the data. It either passes the input DataFrame through silently or raises a `ValueError` if any check fails, preventing corrupted or malformed data from entering the pipeline.\n",
        "*   **Outputs:** `None` on success, or raises a `ValueError` on failure.\n",
        "*   **Role in Research Pipeline:** This callable implements the foundational data integrity checks that are a prerequisite for any quantitative analysis. While not tied to a specific equation, it ensures the raw data conforms to the structural assumptions upon which all subsequent mathematical operations are built. It is the first line of defense against \"garbage in, garbage out.\"\n",
        "\n",
        "#### **Task 2: `validate_hyperparameter_dictionary`**\n",
        "\n",
        "*   **Inputs:** A Python dictionary (`fused_input_state`) containing nested `metadata` and `hyperparameters` dictionaries.\n",
        "*   **Processes:**\n",
        "    1.  **Structural Validation:** Confirms the presence of the top-level `metadata` and `hyperparameters` keys.\n",
        "    2.  **Schema-Driven Validation:** Iterates through a predefined internal schema that specifies the required type, and value constraints (e.g., range, exact value, non-empty list) for every single parameter. It checks each parameter in the input dictionary against these rules.\n",
        "    3.  **Unrecognized Parameter Check:** Identifies any keys present in the input dictionary that are not defined in the schema, flagging them as potential typos.\n",
        "*   **Transformation:** This is a pure validation gate. It does not transform the input dictionary.\n",
        "*   **Outputs:** `None` on success, or raises a `ValueError` on failure.\n",
        "*   **Role in Research Pipeline:** This callable ensures the reproducibility and correctness of the entire study by validating the parameters that govern its execution. It operationalizes the specific hyperparameter values mentioned throughout the paper, such as the RANSAC thresholds from Section 4.2. For example, it validates:\n",
        "    *   `ransac_residual_sq_threshold = 0.004`\n",
        "    *   `ransac_slope_tolerance_pct = 0.10`\n",
        "\n",
        "#### **Task 3: `validate_data_completeness`**\n",
        "\n",
        "*   **Inputs:** A structurally valid `pandas.DataFrame` (`df_master`).\n",
        "*   **Processes:**\n",
        "    1.  **Temporal Coverage Analysis:** Calculates the number of unique timestamps and the maximum time gap between any two consecutive timestamps, comparing them against minimum density thresholds (e.g., >= 100 timestamps, max gap <= 7 days).\n",
        "    2.  **Cross-Sectional Coverage Analysis:** For each estimation group (`timestamp`, `asset`, `expiry_date`), it validates that there is a sufficient number of call-put pairs (>= 5), that the available strikes cover a required moneyness range ([0.7, 1.3]), and that a valid futures price is present.\n",
        "    3.  **Market Data Quality Assessment:** Scans the entire dataset for quality red flags like zero prices, inverted spreads, and extreme relative spreads (>100%).\n",
        "*   **Transformation:** This is a pure validation gate. It does not transform the data.\n",
        "*   **Outputs:** `None` on success, or raises a `ValueError` on failure.\n",
        "*   **Role in Research Pipeline:** This callable enforces the implicit assumption of the paper that the data is sufficiently dense and of high enough quality to support the estimation. A sparse or low-quality dataset would lead to statistically unreliable estimates, and this function prevents the pipeline from running on such data.\n",
        "\n",
        "#### **Task 4: `filter_by_maturity`**\n",
        "\n",
        "*   **Inputs:** A `pandas.DataFrame` and an integer `maturity_filter_days`.\n",
        "*   **Processes:**\n",
        "    1.  Calculates the time-to-expiry in days for each option row by subtracting the `timestamp` from the `expiry_date`.\n",
        "    2.  Creates a boolean mask by comparing each option's time-to-expiry against the `maturity_filter_days` threshold.\n",
        "    3.  Applies the mask to the DataFrame, discarding all rows that do not meet the minimum maturity requirement.\n",
        "*   **Transformation:** This function transforms the input DataFrame by filtering out rows, resulting in an output DataFrame with fewer rows.\n",
        "*   **Outputs:** A tuple containing the filtered `pandas.DataFrame` and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable directly implements the first data trimming step described in **Section 4.2**: *\"First, options with maturities of less than one month are excluded.\"*\n",
        "\n",
        "#### **Task 5: `filter_by_liquidity`**\n",
        "\n",
        "*   **Inputs:** A `pandas.DataFrame` and a float `liquidity_filter_spread_pct`.\n",
        "*   **Processes:**\n",
        "    1.  Calculates the mid-price for each option: $Mid_i = \\frac{Ask_i + Bid_i}{2}$.\n",
        "    2.  Calculates the relative bid-ask spread for each option: $Spread_i = \\frac{Ask_i - Bid_i}{Mid_i}$.\n",
        "    3.  Creates a boolean mask by comparing each option's relative spread against the `liquidity_filter_spread_pct` threshold.\n",
        "    4.  Applies the mask to the DataFrame, discarding all rows that exceed the maximum spread threshold.\n",
        "*   **Transformation:** This function transforms the input DataFrame by filtering out rows, resulting in an output DataFrame with fewer rows.\n",
        "*   **Outputs:** A tuple containing the filtered `pandas.DataFrame` and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable directly implements the second data trimming step described in **Section 4.2**: *\"Second, we exclude data points with excessive bid-ask spreads...\"*\n",
        "\n",
        "#### **Task 6: `prepare_features_and_pairs`**\n",
        "\n",
        "*   **Inputs:** A filtered `pandas.DataFrame`.\n",
        "*   **Processes:**\n",
        "    1.  **Feature Engineering:** Computes and adds two new columns to the DataFrame: `mid_price` and `moneyness`.\n",
        "    2.  **Pairing Validation:** Rigorously filters the DataFrame to ensure that for every unique combination of `(timestamp, asset, expiry_date, strike_price)`, there exists exactly one `call` and one `put`. All unpaired options are discarded.\n",
        "*   **Transformation:** This function transforms the DataFrame by adding columns and then filtering out rows, resulting in a DataFrame with a different shape and schema, ready for algorithmic processing.\n",
        "*   **Outputs:** A tuple containing the analysis-ready `pandas.DataFrame` and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable prepares the clean data for the core algorithms. It computes the necessary features for the estimation, most notably the moneyness, $m_{i,t} = \\frac{K_i}{S_t}$, which is a key variable in the final estimation equations.\n",
        "\n",
        "#### **Task 7: `prepare_ransac_inputs`**\n",
        "\n",
        "*   **Inputs:** The paired `pandas.DataFrame` from the previous task.\n",
        "*   **Processes:**\n",
        "    1.  **Pivoting:** Reshapes the data from a long format to a wide format, aligning the `best_bid_price` and `best_ask_price` for the call and put of each strike side-by-side.\n",
        "    2.  **Vector Construction:** Performs vectorized calculations on the pivoted data to construct the `X` and `Y` vectors for the RANSAC regression.\n",
        "    3.  **Grouping & Filtering:** Groups the data by `(timestamp, asset, expiry_date)`, filters out any groups with fewer than a minimum number of strikes, and packages the `X`, `Y`, and `strike` arrays for each valid group into a dictionary.\n",
        "*   **Transformation:** This function transforms a single large DataFrame into a dictionary where keys are group identifiers and values are dictionaries of NumPy arrays. This is a major structural transformation.\n",
        "*   **Outputs:** A tuple containing the dictionary of RANSAC inputs and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable directly prepares the inputs for the RANSAC algorithm as specified in **Equation (4)**. It computes:\n",
        "    *   $X_i = C_{i,t}^{\\text{ask}} - P_{i,t}^{\\text{bid}}$\n",
        "    *   $Y_i = P_{i,t}^{\\text{ask}} - C_{i,t}^{\\text{bid}}$\n",
        "    These vectors form the basis of the linear relationship $Y = \\zeta + \\xi X + \\eta$ that RANSAC will fit.\n",
        "\n",
        "#### **Task 8: `execute_ransac_on_all_groups`**\n",
        "\n",
        "*   **Inputs:** The dictionary of RANSAC inputs from the previous task and RANSAC hyperparameters.\n",
        "*   **Processes:**\n",
        "    1.  Iterates through each estimation group in the input dictionary.\n",
        "    2.  For each group, it executes the full RANSAC algorithm:\n",
        "        *   Repeatedly draws a minimal random sample of points.\n",
        "        *   Fits a linear model to the sample.\n",
        "        *   Calculates the size of the consensus set (inliers) for that model.\n",
        "        *   Keeps track of the model with the largest consensus set.\n",
        "    3.  After all iterations, it performs a final linear regression fit on the best inlier set to get the final robust model parameters.\n",
        "*   **Transformation:** This function transforms the dictionary of input arrays into a dictionary of result objects, where each result contains the fitted model parameters (`slope`, `intercept`) and the boolean mask identifying the inliers.\n",
        "*   **Outputs:** A tuple containing the dictionary of `RansacResult` objects and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable is the implementation of the **Random Sample Consensus (RANSAC)** algorithm, a core component of the paper's robust statistics methodology described in **Section 4**. It is designed to find the parameters $(\\zeta_t, \\xi_t)$ for the model $Y = \\zeta_t + \\xi_t X + \\eta_t$ in a way that is insensitive to outliers.\n",
        "\n",
        "#### **Task 9: `validate_and_filter_ransac_results`**\n",
        "\n",
        "*   **Inputs:** The dictionary of `RansacResult` objects and validation hyperparameters.\n",
        "*   **Processes:**\n",
        "    1.  Iterates through each RANSAC result.\n",
        "    2.  Applies a series of validation checks to each result:\n",
        "        *   **Slope Check:** Verifies that the estimated slope `ξ̂` is close to its theoretical value of -1, i.e., $|\\hat{\\xi} + 1| \\leq \\text{tolerance}$.\n",
        "        *   **Intercept Check:** Verifies that the intercept `ζ̂` is positive.\n",
        "        *   **Inlier Count Check:** Verifies that the final model is supported by a minimum number of inlier points.\n",
        "    3.  Filters out any results that fail one or more checks.\n",
        "*   **Transformation:** This function transforms the input dictionary of results by filtering it, producing a smaller output dictionary containing only the validated results.\n",
        "*   **Outputs:** A tuple containing the dictionary of validated `RansacResult` objects and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable implements the crucial diagnostic check on the RANSAC output, as described in **Section 4.2**: *\"we discard all observations for that pair (t, T) if the estimated slope ξt deviates significantly from −1.\"*\n",
        "\n",
        "#### **Task 10: `construct_feature_vectors`**\n",
        "\n",
        "*   **Inputs:** The original paired DataFrame and the dictionary of validated RANSAC results.\n",
        "*   **Processes:**\n",
        "    1.  Iterates through the validated RANSAC results.\n",
        "    2.  For each valid group, it uses the `inlier_mask` to select only the inlier options from the original DataFrame.\n",
        "    3.  On this inlier-only subset, it computes the final feature vectors for the least-squares estimation: the call-put mid-price difference vector `y` and the moneyness vector `m`.\n",
        "    4.  It also extracts the constant `futures_price` and `spot_price` for the group.\n",
        "*   **Transformation:** This function transforms the validated RANSAC results (which are just model parameters and masks) back into a set of clean, numerical feature vectors, ready for the final estimation.\n",
        "*   **Outputs:** A tuple containing a dictionary of `FeatureVectorGroup` objects and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable prepares the final, clean inputs for the weighted least-squares estimation. It constructs the vectors that directly populate the estimation equations, namely:\n",
        "    *   $y_t^i = C_t(K^i,T) - P_t(K^i,T)$ (using mid-prices)\n",
        "    *   $m_t^i = \\frac{K^i}{S_t}$\n",
        "\n",
        "#### **Task 11: `compute_summations_for_all_groups`**\n",
        "\n",
        "*   **Inputs:** The dictionary of `FeatureVectorGroup` objects and the `estimation_lambda_weight` hyperparameter.\n",
        "*   **Processes:**\n",
        "    1.  Iterates through each feature vector group.\n",
        "    2.  For each group, it computes the five scalar summations and one ratio required for the closed-form solution.\n",
        "*   **Transformation:** This function transforms the dictionary of vector inputs into a dictionary of scalar inputs for the final solver.\n",
        "*   **Outputs:** A tuple containing a dictionary of `SummationGroup` objects and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable computes the scalar components of the closed-form solution presented in the unnumbered equations following **Equation (3)** in **Section 3.3**. It calculates:\n",
        "    *   $S_1 = \\sum_{i=1}^{n_t^T} y_t^i$\n",
        "    *   $S_2 = \\sum_{i=1}^{n_t^T} m_t^i$\n",
        "    *   $S_3 = \\sum_{i=1}^{n_t^T} (m_t^i)^2$\n",
        "    *   $S_4 = \\sum_{i=1}^{n_t^T} m_t^i y_t^i$\n",
        "    *   $\\lambda_{n_t^T}$ (referred to as `lambda_value`)\n",
        "    *   $F_t / S_t$\n",
        "\n",
        "#### **Task 12: `compute_closed_form_solutions`**\n",
        "\n",
        "*   **Inputs:** The dictionary of `SummationGroup` objects.\n",
        "*   **Processes:**\n",
        "    1.  Iterates through each summation group.\n",
        "    2.  For each group, it first calculates the determinant `d` of the 2x2 linear system.\n",
        "    3.  It checks if the system is solvable (i.e., `abs(d)` is greater than a small tolerance).\n",
        "    4.  If solvable, it computes the zero-coupon bond prices `α*` and `β*` using the closed-form formulas.\n",
        "    5.  It performs a final check to ensure the computed prices are positive.\n",
        "*   **Transformation:** This function transforms the dictionary of scalar inputs into a dictionary of solution results (the estimated bond prices).\n",
        "*   **Outputs:** A tuple containing a dictionary of `SolutionResult` objects and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable is the direct implementation of the final closed-form solution for the zero-coupon bond prices, `α*` (crypto ZCB, `ZC_t^C(T)`) and `β*` (reference ZCB, `ZC_t^{ref}(T)`), presented in the unnumbered equations of **Section 3.3**:\n",
        "    *   $d = (\\lambda_{n_t^T} + S_3)(n_t^T + \\lambda_{n_t^T} \\frac{F_t^2}{S_t^2}) - (\\lambda_{n_t^T} \\frac{F_t}{S_t} + S_2)^2$\n",
        "    *   $\\alpha_t^* = \\frac{1}{d}[(\\lambda_{n_t^T} + S_3)S_1 - (\\lambda_{n_t^T} \\frac{F_t}{S_t} + S_2)S_4]$\n",
        "    *   $\\beta_t^* = \\frac{1}{d}[(\\lambda_{n_t^T} \\frac{F_t}{S_t} + S_2)S_1 - (n_t^T + \\lambda_{n_t^T} \\frac{F_t^2}{S_t^2})S_4]$\n",
        "\n",
        "#### **Task 13: `convert_all_solutions_to_rates`**\n",
        "\n",
        "*   **Inputs:** The dictionary of `SolutionResult` objects and the `annualization_factor` hyperparameter.\n",
        "*   **Processes:**\n",
        "    1.  Iterates through each solution group.\n",
        "    2.  For each group, it calculates the precise time-to-expiry in years (`T-t`).\n",
        "    3.  It applies the logarithmic conversion formula to `α*` and `β*` to find the continuously compounded interest rates.\n",
        "*   **Transformation:** This function transforms the estimated bond prices (discount factors) into annualized interest rates.\n",
        "*   **Outputs:** A tuple containing a dictionary of `InterestRateResult` objects and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable implements the standard financial formula for converting a zero-coupon bond price (discount factor) to a continuously compounded rate, as referenced at the end of **Section 3.3**:\n",
        "    *   $\\hat{r}_t^{T-t,\\text{crypto}} = -\\frac{1}{T-t} \\ln(\\alpha_t^*)$\n",
        "    *   $\\hat{r}_t^{T-t,\\text{ref}} = -\\frac{1}{T-t} \\ln(\\beta_t^*)$\n",
        "\n",
        "#### **Task 14: `validate_and_filter_rates`**\n",
        "\n",
        "*   **Inputs:** The dictionary of `InterestRateResult` objects.\n",
        "*   **Processes:**\n",
        "    1.  Iterates through each interest rate result.\n",
        "    2.  Applies a series of economic reasonableness checks based on predefined ranges for the crypto rate, the reference rate, and the spread between them.\n",
        "    3.  Filters out any results that fall outside these plausible bounds.\n",
        "*   **Transformation:** This function transforms the input dictionary of rates by filtering it, producing a smaller output dictionary of validated rates.\n",
        "*   **Outputs:** A tuple containing the dictionary of validated `InterestRateResult` objects and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable serves as a final economic sanity check on the numerical outputs of the model, ensuring the results are plausible before they are used for any further analysis.\n",
        "\n",
        "#### **Task 15: `aggregate_results_to_dataframe` (Remediated Version)**\n",
        "\n",
        "*   **Inputs:** The dictionary of validated `InterestRateResult` objects (which now includes `n_contracts`).\n",
        "*   **Processes:**\n",
        "    1.  Iterates through the input dictionary.\n",
        "    2.  For each item, it creates a \"flattened\" dictionary record containing the group identifiers (`timestamp`, `asset`, `expiry_date`) and all the results (`crypto_rate`, `ref_rate`, `n_contracts`, etc.).\n",
        "    3.  Constructs a single `pandas.DataFrame` from the list of these records.\n",
        "    4.  Enforces a standard column order and correct data types.\n",
        "*   **Transformation:** This function performs a major structural transformation, converting the nested dictionary of results into a single, flat, tabular `pandas.DataFrame`.\n",
        "*   **Outputs:** A `pandas.DataFrame`.\n",
        "*   **Role in Research Pipeline:** This callable is a crucial data structuring step that prepares the granular, point-in-time estimates for the final phase of time-series analysis.\n",
        "\n",
        "#### **Task 16: `aggregate_rates_daily`**\n",
        "\n",
        "*   **Inputs:** The `pandas.DataFrame` of aggregated results from the previous task.\n",
        "*   **Processes:**\n",
        "    1.  Creates a `date` column and a `maturity_bucket` column.\n",
        "    2.  Groups the DataFrame by `(date, asset, maturity_bucket)`.\n",
        "    3.  For each group, it calculates the mean and standard deviation of the rates and counts the number of observations.\n",
        "    4.  Filters out any aggregated points that are based on fewer than a minimum number of observations.\n",
        "*   **Transformation:** This function transforms the high-frequency (e.g., hourly) data into a lower-frequency (daily) panel dataset, smoothing out intraday noise.\n",
        "*   **Outputs:** A tuple containing the daily aggregated `pandas.DataFrame` and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable implements the temporal aggregation step, a standard technique in time-series analysis for reducing noise and creating a dataset with a regular frequency, as discussed in **Section 4.1** regarding the choice to aggregate hourly observations into daily estimates.\n",
        "\n",
        "#### **Task 17: `interpolate_yield_curves`**\n",
        "\n",
        "*   **Inputs:** The daily aggregated DataFrame.\n",
        "*   **Processes:**\n",
        "    1.  For each `(date, asset)` group, it treats the available maturity bucket midpoints and their corresponding mean rates as points on a yield curve.\n",
        "    2.  It uses linear interpolation to estimate the interest rate at specific, standardized target tenors (e.g., 90, 180, 360 days).\n",
        "    3.  It handles cases where a target tenor is outside the available range by using flat extrapolation.\n",
        "    4.  It creates a `status` flag to identify whether each point was interpolated or extrapolated.\n",
        "*   **Transformation:** This function transforms the data from rates at discrete maturity *buckets* to rates at specific maturity *points*, creating a standardized term structure for each day.\n",
        "*   **Outputs:** A tuple containing the final interpolated `pandas.DataFrame` (in wide format) and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This callable implements the maturity interpolation step, as referenced in **Section 4.3.2**: *\"Each point on these curves is computed by applying our methodology..., followed by linear interpolation from the nearest available maturities corresponding to the target tenor.\"*\n",
        "\n",
        "#### **Task 18: `construct_final_time_series`**\n",
        "\n",
        "*   **Inputs:** The daily, maturity-interpolated DataFrame.\n",
        "*   **Processes:**\n",
        "    1.  For each asset, it creates a complete daily date range from the start to the end of the sample period.\n",
        "    2.  It reindexes the data to this complete range, which makes any missing days explicit by filling them with `NaN`.\n",
        "    3.  It uses limited linear interpolation to fill small gaps (<= 7 days) in the time series.\n",
        "    4.  It creates a `source` column to provide an audit trail for each data point (observed, time-interpolated, etc.).\n",
        "*   **Transformation:** This function transforms a potentially sparse daily time series into a dense, continuous daily time series, ready for econometric analysis.\n",
        "*   **Outputs:** A tuple containing the final, dense `pandas.DataFrame` and a diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This is the final data construction step, creating the clean, continuous time series that are plotted in Figures 9 and 10 and used to generate the summary statistics in Tables 3, 4, and 5.\n",
        "\n",
        "#### **Task 19: `orchestrate_yield_curve_estimation`**\n",
        "\n",
        "*   **Inputs:** The raw `df_master` and the `fused_input_state` configuration.\n",
        "*   **Processes:** Sequentially executes the orchestrator functions from Tasks 1 through 18, passing data and parameters between them and collecting diagnostics.\n",
        "*   **Transformation:** Manages the entire end-to-end transformation of raw data into a final, clean yield curve time series.\n",
        "*   **Outputs:** A tuple containing the final `pandas.DataFrame` and a master diagnostic dictionary.\n",
        "*   **Role in Research Pipeline:** This is the master controller for the entire estimation methodology described in the paper.\n",
        "\n",
        "#### **Task 20: `run_parameter_sensitivity_analysis`**\n",
        "\n",
        "*   **Inputs:** The raw `df_master`, a base configuration, and a grid of parameter values to test.\n",
        "*   **Processes:**\n",
        "    1.  Generates all combinations of the specified hyperparameters.\n",
        "    2.  For each combination, it executes the entire core pipeline (`orchestrate_yield_curve_estimation`) in parallel.\n",
        "    3.  Aggregates the results from all runs.\n",
        "    4.  Calculates robustness metrics, such as the standard deviation of the estimated rates across the different parameter runs.\n",
        "*   **Transformation:** Transforms a single set of inputs into a distribution of outputs, which is then summarized to measure model stability.\n",
        "*   **Outputs:** A tuple of summary DataFrames detailing rate stability and coverage stability.\n",
        "*   **Role in Research Pipeline:** This is a meta-analysis function. It provides a framework for rigorously testing the robustness of the paper's results to the specific choices of hyperparameters (e.g., the RANSAC thresholds), which is a critical step in validating any quantitative model.\n",
        "\n",
        "#### **Task 21: `construct_bootstrap_confidence_intervals`**\n",
        "\n",
        "*   **Inputs:** The paired DataFrame, a configuration, and a specific estimation group to analyze.\n",
        "*   **Processes:**\n",
        "    1.  For a single estimation group, it repeatedly resamples the underlying option pairs with replacement.\n",
        "    2.  For each resampled dataset, it re-runs the core estimation logic (RANSAC through rate conversion).\n",
        "    3.  It collects the distribution of the resulting rate estimates.\n",
        "    4.  It calculates percentile-based confidence intervals from this distribution.\n",
        "*   **Transformation:** Transforms a single point estimate into a distribution of estimates, which is then summarized into confidence intervals.\n",
        "*   **Outputs:** A dictionary containing the point estimate and its confidence intervals.\n",
        "*   **Role in Research Pipeline:** This function provides a method for quantifying the statistical uncertainty of any given point estimate produced by the model. This is a standard and rigorous technique for moving beyond a simple point estimate to understanding its precision.\n",
        "\n",
        "#### **Task 22: `run_economic_validation`**\n",
        "\n",
        "*   **Inputs:** The final yield curve DataFrame and an optional external benchmark DataFrame.\n",
        "*   **Processes:**\n",
        "    1.  Performs theoretical consistency checks (term structure shape, cross-asset consistency).\n",
        "    2.  If a benchmark is provided, it calculates the correlation between the estimated rates and the benchmark.\n",
        "    3.  Performs formal econometric tests for stationarity (unit root) and cointegration.\n",
        "*   **Transformation:** This function transforms the final time-series data into a set of summary statistics and test results.\n",
        "*   **Outputs:** A comprehensive dictionary containing the results of all validation tests.\n",
        "*   **Role in Research Pipeline:** This function directly addresses the economic interpretation and validation of the results, similar to the analysis presented in **Section 4.3**, which compares the model's output to DeFi lending rates (Figure 11) and discusses the properties of the resulting curves.\n",
        "\n",
        "<br> <br>\n",
        "\n",
        "### **Usage Example**\n",
        "\n",
        "This guide demonstrates how to execute the complete yield curve estimation and analysis pipeline. The process involves three main stages:\n",
        "1.  **Data Loading and Preparation:** Loading the raw market data and the external benchmark data into the required `pandas.DataFrame` structures.\n",
        "2.  **Configuration Loading:** Loading the study's parameters from the `config.yaml` file.\n",
        "3.  **Pipeline Execution:** Calling the main orchestrator function, `run_full_analysis`, with the prepared data and configuration to generate the final results.\n",
        "\n",
        "#### **1. Data Loading and Preparation**\n",
        "\n",
        "Before running the pipeline, the user must prepare two primary data artifacts.\n",
        "\n",
        "**a) The Master Market Data DataFrame (`df_master`)**\n",
        "\n",
        "This is the most critical input. It must contain the high-frequency, top-of-book quote data for options, along with the corresponding futures and spot prices. The data must be structured precisely as a `pandas.DataFrame` with the specific `MultiIndex` and columns that the pipeline's validation functions expect.\n",
        "\n",
        "*   **Discussion:** A user would typically construct this DataFrame through a dedicated ETL (Extract, Transform, Load) process. This would involve fetching raw data from an API (like CoinMetrics), parsing it, merging the options, futures, and spot feeds, and then setting the specific MultiIndex. For this example, we will simulate a small, correctly formatted `df_master` to demonstrate the pipeline's usage.\n",
        "\n",
        "```python\n",
        "# Python Code Snippet: Simulating df_master\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create the MultiIndex levels\n",
        "timestamps = pd.to_datetime(['2024-11-05 08:00:00', '2024-11-05 09:00:00'])\n",
        "assets = ['BTC']\n",
        "expiries = pd.to_datetime(['2024-12-27', '2025-03-28'])\n",
        "strikes = [68000.0, 70000.0, 72000.0]\n",
        "option_types = ['call', 'put']\n",
        "\n",
        "# Create the full MultiIndex from the product of the levels\n",
        "index = pd.MultiIndex.from_product(\n",
        "    [timestamps, assets, expiries, strikes, option_types],\n",
        "    names=['timestamp', 'asset', 'expiry_date', 'strike_price', 'option_type']\n",
        ")\n",
        "\n",
        "# Create a placeholder DataFrame\n",
        "df_master = pd.DataFrame(index=index)\n",
        "\n",
        "# Populate with realistic, but simulated, data\n",
        "# In a real scenario, this data would be loaded from a file (e.g., Parquet)\n",
        "np.random.seed(42)\n",
        "df_master['best_bid_price'] = np.random.uniform(0.01, 0.1, size=len(df_master))\n",
        "df_master['best_ask_price'] = df_master['best_bid_price'] * np.random.uniform(1.01, 1.05, size=len(df_master))\n",
        "df_master['instrument_name'] = [f\"BTC-{exp.strftime('%d%b%y')}-{int(k)}-{opt[0].upper()}\" for t, a, exp, k, opt in df_master.index]\n",
        "\n",
        "# Add spot and futures prices (constant per timestamp/expiry)\n",
        "df_master['spot_price'] = df_master.index.get_level_values('timestamp').map(\n",
        "    {timestamps[0]: 70000.0, timestamps[1]: 70100.0}\n",
        ")\n",
        "# Create a mapping for futures prices\n",
        "futures_map = {\n",
        "    (timestamps[0], expiries[0]): 70500.0, (timestamps[0], expiries[1]): 71000.0,\n",
        "    (timestamps[1], expiries[0]): 70600.0, (timestamps[1], expiries[1]): 71100.0,\n",
        "}\n",
        "# Map the futures prices based on timestamp and expiry\n",
        "df_master['futures_price'] = [futures_map.get((t, exp)) for t, a, exp, k, opt in df_master.index]\n",
        "\n",
        "print(\"--- Sample df_master created ---\")\n",
        "print(df_master.head())\n",
        "```\n",
        "\n",
        "**b) The External Benchmark DataFrame (`benchmark_df`)**\n",
        "\n",
        "This DataFrame is optional. If provided, it is used in the final economic validation step (Task 22). It should contain one or more columns of benchmark data (e.g., DeFi lending rates) and have a `DatetimeIndex` with daily frequency.\n",
        "\n",
        "*   **Discussion:** This data would typically be sourced from a provider like DeFiLlama. For this example, we will simulate a small benchmark DataFrame.\n",
        "\n",
        "```python\n",
        "# Python Code Snippet: Simulating benchmark_df\n",
        "\n",
        "# Create a date range matching the master data\n",
        "date_range = pd.to_datetime(['2024-11-05', '2024-11-06'])\n",
        "\n",
        "# Create the benchmark DataFrame\n",
        "benchmark_df = pd.DataFrame(\n",
        "    {'aave_usdc_rate': [0.055, 0.056]},\n",
        "    index=pd.DatetimeIndex(date_range, name='date')\n",
        ")\n",
        "\n",
        "print(\"\\n--- Sample benchmark_df created ---\")\n",
        "print(benchmark_df)\n",
        "```\n",
        "\n",
        "#### **2. Configuration Loading**\n",
        "\n",
        "The pipeline's behavior is controlled by the `config.yaml` file. We need to load this file into a Python dictionary. The `PyYAML` library is the standard tool for this.\n",
        "\n",
        "*   **Discussion:** This step cleanly separates the code from the parameters. A researcher can easily create multiple YAML files to test different scenarios without ever modifying the Python source code.\n",
        "\n",
        "```python\n",
        "# Python Code Snippet: Loading the YAML configuration\n",
        "\n",
        "import yaml\n",
        "\n",
        "# Define the path to the configuration file\n",
        "config_path = 'config.yaml'\n",
        "\n",
        "# Open and load the YAML file into a Python dictionary\n",
        "with open(config_path, 'r') as f:\n",
        "    fused_input_state = yaml.safe_load(f)\n",
        "\n",
        "print(\"\\n--- Configuration loaded from config.yaml ---\")\n",
        "pprint(fused_input_state)\n",
        "```\n",
        "\n",
        "#### **3. Pipeline Execution**\n",
        "\n",
        "With the data and configuration prepared, we can now call the main orchestrator function, `run_full_analysis`. This single function call will execute the entire end-to-end process.\n",
        "\n",
        "*   **Discussion:** We will demonstrate a comprehensive call that includes the optional robustness analyses. The `parameter_grid` will test the pipeline's sensitivity to the RANSAC residual threshold. The `bootstrap_group` will be set to one of the specific `(timestamp, asset, expiry)` combinations present in our simulated data to calculate confidence intervals for that point estimate.\n",
        "\n",
        "```python\n",
        "# Python Code Snippet: Executing the full analysis\n",
        "\n",
        "# --- Define Inputs for Optional Analyses ---\n",
        "\n",
        "# Define a parameter grid for the sensitivity analysis (Task 20)\n",
        "# We will test two different values for the RANSAC threshold.\n",
        "param_grid_for_sensitivity = {\n",
        "    'ransac_residual_sq_threshold': [0.004, 0.008]\n",
        "}\n",
        "\n",
        "# Define a specific group for the bootstrap analysis (Task 21)\n",
        "# This must be a valid (timestamp, asset, expiry_date) from our df_master\n",
        "bootstrap_target_group = (\n",
        "    pd.to_datetime('2024-11-05 08:00:00'),\n",
        "    'BTC',\n",
        "    pd.to_datetime('2024-12-27')\n",
        ")\n",
        "\n",
        "# --- Execute the Grand Orchestrator ---\n",
        "\n",
        "# Call the main function with all inputs.\n",
        "# The 'verbose' flag is set to True to print diagnostics from the core pipeline.\n",
        "full_report = run_full_analysis(\n",
        "    df_master=df_master,\n",
        "    fused_input_state=fused_input_state,\n",
        "    parameter_grid=param_grid_for_sensitivity,\n",
        "    bootstrap_group=bootstrap_target_group,\n",
        "    n_bootstrap=100,  # Using a smaller number for a quick example run\n",
        "    benchmark_df=benchmark_df,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --- Inspect the Results ---\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*50)\n",
        "print(\"      FULL ANALYSIS COMPLETE - FINAL REPORT      \")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# Print the final yield curve DataFrame produced by the core pipeline\n",
        "print(\"--- Final Yield Curve Time Series (from core pipeline) ---\")\n",
        "print(full_report['core_pipeline_results']['yield_curves_df'])\n",
        "\n",
        "# Print a summary of the sensitivity analysis\n",
        "print(\"\\n--- Sensitivity Analysis Summary ---\")\n",
        "if 'sensitivity_analysis' in full_report:\n",
        "    print(\"Rate Stability (Mean and Std Dev of rates across parameter runs):\")\n",
        "    print(full_report['sensitivity_analysis']['rate_stability_summary'])\n",
        "\n",
        "# Print the results of the bootstrap analysis\n",
        "print(\"\\n--- Bootstrap Analysis Summary ---\")\n",
        "if 'bootstrap_analysis' in full_report:\n",
        "    pprint(full_report['bootstrap_analysis'])\n",
        "\n",
        "# Print the results of the economic validation\n",
        "print(\"\\n--- Economic Validation Summary ---\")\n",
        "if 'economic_validation' in full_report:\n",
        "    pprint(full_report['economic_validation'])\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wGvxtJI34dFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: DataFrame Structure Validation\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 1, Step 1: MultiIndex Level Validation\n",
        "# ======================================================================================\n",
        "\n",
        "def _validate_multiindex(\n",
        "    df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the structure, names, dtypes, and values of the DataFrame's MultiIndex.\n",
        "\n",
        "    This function performs a series of rigorous checks to ensure the MultiIndex\n",
        "    conforms to the exact schema required by the yield curve estimation pipeline.\n",
        "    It is a critical first step in the input validation process.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame):\n",
        "            The input DataFrame which is expected to have a specific MultiIndex\n",
        "            structure.\n",
        "\n",
        "    Returns:\n",
        "        List[str]:\n",
        "            A list of string error messages. An empty list indicates that all\n",
        "            MultiIndex validation checks have passed successfully.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate potential error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Define the expected schema for the MultiIndex.\n",
        "    expected_levels: int = 5\n",
        "    expected_names: List[str] = [\n",
        "        'timestamp', 'asset', 'expiry_date', 'strike_price', 'option_type'\n",
        "    ]\n",
        "    expected_dtypes: Dict[str, str] = {\n",
        "        'timestamp': 'datetime64',\n",
        "        'asset': 'object',\n",
        "        'expiry_date': 'datetime64',\n",
        "        'strike_price': 'float64',\n",
        "        'option_type': 'object'\n",
        "    }\n",
        "    expected_values: Dict[str, Set[str]] = {\n",
        "        'asset': {'BTC', 'ETH'},\n",
        "        'option_type': {'call', 'put'}\n",
        "    }\n",
        "\n",
        "    # Check 1: Verify that the index is indeed a MultiIndex.\n",
        "    if not isinstance(df.index, pd.MultiIndex):\n",
        "        # If not a MultiIndex, further checks are irrelevant. Return immediately.\n",
        "        return [\"Input is not a pandas MultiIndex.\"]\n",
        "\n",
        "    # Check 2: Validate the number of levels in the MultiIndex.\n",
        "    if df.index.nlevels != expected_levels:\n",
        "        # Add an error message if the number of levels is incorrect.\n",
        "        errors.append(\n",
        "            f\"MultiIndex validation failed: Expected {expected_levels} levels, \"\n",
        "            f\"but found {df.index.nlevels}.\"\n",
        "        )\n",
        "        # If the level count is wrong, further checks by name will fail. Return.\n",
        "        return errors\n",
        "\n",
        "    # Check 3: Validate the names and order of the MultiIndex levels.\n",
        "    if list(df.index.names) != expected_names:\n",
        "        # Add an error message if the names or their order are incorrect.\n",
        "        errors.append(\n",
        "            f\"MultiIndex validation failed: Expected level names \"\n",
        "            f\"{expected_names}, but found {list(df.index.names)}.\"\n",
        "        )\n",
        "\n",
        "    # Check 4: Iterate through each level to validate its dtype and values.\n",
        "    for i, name in enumerate(expected_names):\n",
        "        # Extract the current level's values for inspection.\n",
        "        level_values = df.index.get_level_values(i)\n",
        "\n",
        "        # Sub-check 4a: Validate the data type of the level.\n",
        "        if expected_dtypes[name] == 'datetime64':\n",
        "            # For datetime levels, use the specific pandas API type checker.\n",
        "            if not ptypes.is_datetime64_any_dtype(level_values):\n",
        "                errors.append(\n",
        "                    f\"MultiIndex validation failed: Level '{name}' is not a \"\n",
        "                    f\"datetime64 dtype, found {level_values.dtype}.\"\n",
        "                )\n",
        "        elif expected_dtypes[name] == 'float64':\n",
        "            # For float levels, use the specific pandas API type checker.\n",
        "            if not ptypes.is_float_dtype(level_values):\n",
        "                errors.append(\n",
        "                    f\"MultiIndex validation failed: Level '{name}' is not a \"\n",
        "                    f\"float64 dtype, found {level_values.dtype}.\"\n",
        "                )\n",
        "        elif expected_dtypes[name] == 'object':\n",
        "            # For object (string) levels, use the specific pandas API type checker.\n",
        "            if not ptypes.is_object_dtype(level_values):\n",
        "                errors.append(\n",
        "                    f\"MultiIndex validation failed: Level '{name}' is not an \"\n",
        "                    f\"object dtype, found {level_values.dtype}.\"\n",
        "                )\n",
        "\n",
        "        # Sub-check 4b: Validate the set of unique values for categorical levels.\n",
        "        if name in expected_values:\n",
        "            # Get the unique values present in the data for this level.\n",
        "            unique_vals = set(level_values.unique())\n",
        "            # Check if there are any values in the data that are not in the expected set.\n",
        "            if not unique_vals.issubset(expected_values[name]):\n",
        "                # Identify and report the unexpected values.\n",
        "                unexpected = unique_vals - expected_values[name]\n",
        "                errors.append(\n",
        "                    f\"MultiIndex validation failed: Level '{name}' contains \"\n",
        "                    f\"unexpected values: {unexpected}. Expected a subset of \"\n",
        "                    f\"{expected_values[name]}.\"\n",
        "                )\n",
        "\n",
        "        # Sub-check 4c: For the 'strike_price' level, ensure all values are positive.\n",
        "        if name == 'strike_price' and ptypes.is_float_dtype(level_values):\n",
        "            # Check if any strike price is less than or equal to zero.\n",
        "            if not (level_values > 0).all():\n",
        "                errors.append(\n",
        "                    \"MultiIndex validation failed: Level 'strike_price' \"\n",
        "                    \"contains non-positive values.\"\n",
        "                )\n",
        "\n",
        "    # Return the aggregated list of all found errors.\n",
        "    return errors\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 1, Step 2: Column Schema and Dtype Validation\n",
        "# ======================================================================================\n",
        "\n",
        "def _validate_columns(\n",
        "    df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the DataFrame's columns for presence, dtype, and numeric properties.\n",
        "\n",
        "    This function ensures that all required columns for the analysis exist,\n",
        "    have the correct data types, and contain valid numerical values (i.e.,\n",
        "    positive and finite, where applicable).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame):\n",
        "            The input DataFrame with columns to be validated.\n",
        "\n",
        "    Returns:\n",
        "        List[str]:\n",
        "            A list of string error messages. An empty list indicates successful\n",
        "            validation of all column properties.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate potential error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Define the expected schema for the columns.\n",
        "    expected_columns: Dict[str, np.dtype] = {\n",
        "        'instrument_name': np.dtype('O'),\n",
        "        'best_bid_price': np.dtype('float64'),\n",
        "        'best_ask_price': np.dtype('float64'),\n",
        "        'futures_price': np.dtype('float64'),\n",
        "        'spot_price': np.dtype('float64')\n",
        "    }\n",
        "\n",
        "    # Check 1: Verify that all expected columns are present in the DataFrame.\n",
        "    missing_cols = set(expected_columns.keys()) - set(df.columns)\n",
        "    if missing_cols:\n",
        "        # If any columns are missing, report them and return, as further checks are invalid.\n",
        "        errors.append(f\"Column validation failed: Missing required columns: {missing_cols}.\")\n",
        "        return errors\n",
        "\n",
        "    # Check 2: Iterate through each expected column to validate its properties.\n",
        "    for col_name, expected_dtype in expected_columns.items():\n",
        "        # Extract the column Series for inspection.\n",
        "        column_series = df[col_name]\n",
        "\n",
        "        # Sub-check 2a: Validate the data type of the column.\n",
        "        if column_series.dtype != expected_dtype:\n",
        "            errors.append(\n",
        "                f\"Column validation failed: Column '{col_name}' has incorrect \"\n",
        "                f\"dtype. Expected {expected_dtype}, found {column_series.dtype}.\"\n",
        "            )\n",
        "            # If dtype is wrong, numeric checks below might fail; continue to next column.\n",
        "            continue\n",
        "\n",
        "        # Sub-check 2b: Check for any null (NaN, None) values.\n",
        "        if column_series.isnull().any():\n",
        "            errors.append(\n",
        "                f\"Column validation failed: Column '{col_name}' contains null values.\"\n",
        "            )\n",
        "\n",
        "        # Sub-check 2c: For numeric columns, perform additional checks.\n",
        "        if ptypes.is_numeric_dtype(column_series):\n",
        "            # Ensure all values are finite (not inf, -inf, or NaN).\n",
        "            if not np.isfinite(column_series).all():\n",
        "                errors.append(\n",
        "                    f\"Column validation failed: Column '{col_name}' contains \"\n",
        "                    \"non-finite values (inf, -inf, or NaN).\"\n",
        "                )\n",
        "            # For price columns, ensure all values are strictly positive.\n",
        "            if col_name != 'instrument_name': # All other columns are prices\n",
        "                if not (column_series > 0).all():\n",
        "                    errors.append(\n",
        "                        f\"Column validation failed: Column '{col_name}' contains \"\n",
        "                        \"non-positive values.\"\n",
        "                    )\n",
        "\n",
        "    # Return the aggregated list of all found errors.\n",
        "    return errors\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 1, Step 3: Economic Constraint Validation\n",
        "# ======================================================================================\n",
        "\n",
        "def _validate_economic_constraints(\n",
        "    df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates fundamental economic and logical constraints within the data.\n",
        "\n",
        "    This function checks for internal consistency, such as valid bid-ask spreads\n",
        "    and logical temporal ordering between observation timestamps and contract expiries.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame):\n",
        "            The input DataFrame, assumed to have passed schema validation.\n",
        "\n",
        "    Returns:\n",
        "        List[str]:\n",
        "            A list of string error messages. An empty list indicates all\n",
        "            constraints are satisfied.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate potential error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Constraint 1: The ask price must be greater than or equal to the bid price.\n",
        "    # A violation (ask < bid) indicates a data quality issue or a crossed market.\n",
        "    inverted_spread_mask = df['best_ask_price'] < df['best_bid_price']\n",
        "    if inverted_spread_mask.any():\n",
        "        # If any violations exist, report the count and percentage.\n",
        "        num_violations = inverted_spread_mask.sum()\n",
        "        pct_violations = 100 * num_violations / len(df)\n",
        "        errors.append(\n",
        "            f\"Economic constraint failed: Found {num_violations} rows \"\n",
        "            f\"({pct_violations:.2f}%) where best_ask_price < best_bid_price.\"\n",
        "        )\n",
        "\n",
        "    # Extract timestamp and expiry_date levels for temporal checks.\n",
        "    timestamps = df.index.get_level_values('timestamp')\n",
        "    expiries = df.index.get_level_values('expiry_date')\n",
        "\n",
        "    # Constraint 2: The expiry date of an option must be in the future relative\n",
        "    # to its observation timestamp.\n",
        "    # A violation (expiry <= timestamp) is a logical impossibility.\n",
        "    expired_contract_mask = expiries <= timestamps\n",
        "    if expired_contract_mask.any():\n",
        "        # If any violations exist, report the count and percentage.\n",
        "        num_violations = expired_contract_mask.sum()\n",
        "        pct_violations = 100 * num_violations / len(df)\n",
        "        errors.append(\n",
        "            f\"Economic constraint failed: Found {num_violations} rows \"\n",
        "            f\"({pct_violations:.2f}%) where expiry_date <= timestamp.\"\n",
        "        )\n",
        "\n",
        "    # Constraint 3: Check for timezone consistency. Timestamps and expiries must\n",
        "    # either both be timezone-aware (and have the same timezone) or both be naive.\n",
        "    # Mixing aware and naive datetimes leads to incorrect duration calculations.\n",
        "    ts_tz = timestamps.tz\n",
        "    ex_tz = expiries.tz\n",
        "    if ts_tz != ex_tz:\n",
        "        errors.append(\n",
        "            \"Economic constraint failed: Timezone mismatch between 'timestamp' \"\n",
        "            f\"level (tz='{ts_tz}') and 'expiry_date' level (tz='{ex_tz}').\"\n",
        "        )\n",
        "\n",
        "    # Return the aggregated list of all found errors.\n",
        "    return errors\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 1: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def validate_dataframe_structure(\n",
        "    df_master: pd.DataFrame\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates a comprehensive validation of the input DataFrame's structure.\n",
        "\n",
        "    This master function executes a sequence of validation checks covering the\n",
        "    MultiIndex, column schema, and fundamental economic constraints. It aggregates\n",
        "    all identified issues and raises a single, detailed ValueError if any\n",
        "    validation fails, providing a complete diagnostic report. A successful\n",
        "    execution (i.e., no exception raised) indicates the DataFrame is fit for\n",
        "    the subsequent stages of the yield curve estimation pipeline.\n",
        "\n",
        "    Args:\n",
        "        df_master (pd.DataFrame):\n",
        "            The primary input DataFrame for the yield curve estimation pipeline.\n",
        "            It is expected to conform to a strict, predefined schema.\n",
        "\n",
        "    Raises:\n",
        "        TypeError:\n",
        "            If the input `df_master` is not a pandas DataFrame.\n",
        "        ValueError:\n",
        "            If any of the validation checks fail. The error message contains a\n",
        "            detailed, itemized list of all identified structural, schema, or\n",
        "            constraint violations.\n",
        "\n",
        "    Returns:\n",
        "        None:\n",
        "            This function does not return a value. It either completes silently,\n",
        "            indicating success, or raises an exception, indicating failure.\n",
        "    \"\"\"\n",
        "    # --- Input Type Validation ---\n",
        "    # First, ensure the provided object is a pandas DataFrame.\n",
        "    if not isinstance(df_master, pd.DataFrame):\n",
        "        # Raise a TypeError if the input is of the wrong type.\n",
        "        raise TypeError(\"Input `df_master` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Aggregation of Validation Errors ---\n",
        "    # Initialize a list to hold all error messages from the various checks.\n",
        "    all_errors: List[str] = []\n",
        "\n",
        "    # Execute Step 1: MultiIndex validation.\n",
        "    # This checks the structure, names, dtypes, and values of the index.\n",
        "    multiindex_errors = _validate_multiindex(df=df_master)\n",
        "    all_errors.extend(multiindex_errors)\n",
        "\n",
        "    # Execute Step 2: Column schema validation.\n",
        "    # This checks for column presence, dtypes, and numeric properties.\n",
        "    # This check is only performed if the index validation passed, as some\n",
        "    # constraints depend on a valid index.\n",
        "    if not multiindex_errors:\n",
        "        column_errors = _validate_columns(df=df_master)\n",
        "        all_errors.extend(column_errors)\n",
        "\n",
        "        # Execute Step 3: Economic constraint validation.\n",
        "        # This checks for logical consistency in the data.\n",
        "        # This check is only performed if both index and column schema are valid.\n",
        "        if not column_errors:\n",
        "            constraint_errors = _validate_economic_constraints(df=df_master)\n",
        "            all_errors.extend(constraint_errors)\n",
        "\n",
        "    # --- Final Error Reporting ---\n",
        "    # Check if any errors were collected during the validation process.\n",
        "    if all_errors:\n",
        "        # If errors exist, format them into a single, comprehensive error message.\n",
        "        error_header = \"Input DataFrame validation failed with the following errors:\"\n",
        "        # Join all individual error messages with newlines for readability.\n",
        "        formatted_errors = \"\\n\".join(f\"- {error}\" for error in all_errors)\n",
        "        # Raise a single ValueError containing the complete diagnostic report.\n",
        "        raise ValueError(f\"{error_header}\\n{formatted_errors}\")\n",
        "\n",
        "    # If the function reaches this point, all validations passed.\n",
        "    # No action is needed, and the silent return indicates success.\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "FrHWPqtrXtX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Hyperparameter Dictionary Validation\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 2, Step 1: Metadata Section Validation\n",
        "# ======================================================================================\n",
        "\n",
        "def _validate_metadata(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the 'metadata' section of the configuration dictionary.\n",
        "\n",
        "    This function ensures that the 'metadata' key exists and its value is a\n",
        "    dictionary containing the required keys ('asset', 'data_source') with\n",
        "    appropriate types and values.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]):\n",
        "            The top-level configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]:\n",
        "            A list of error messages. An empty list signifies successful validation.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate potential error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Check 1: Ensure the 'metadata' key exists and is a dictionary.\n",
        "    metadata = config.get('metadata')\n",
        "    if not isinstance(metadata, dict):\n",
        "        # If 'metadata' is missing or not a dict, further checks are impossible.\n",
        "        errors.append(\"Configuration validation failed: Missing or invalid 'metadata' section.\")\n",
        "        return errors\n",
        "\n",
        "    # Check 2: Validate the 'asset' key.\n",
        "    asset = metadata.get('asset')\n",
        "    expected_assets: Set[str] = {'BTC', 'ETH'}\n",
        "    if not isinstance(asset, str) or asset not in expected_assets:\n",
        "        # The asset must be a string and one of the allowed values.\n",
        "        errors.append(\n",
        "            f\"Metadata validation failed: 'asset' must be one of \"\n",
        "            f\"{expected_assets}, but found '{asset}'.\"\n",
        "        )\n",
        "\n",
        "    # Check 3: Validate the 'data_source' key.\n",
        "    data_source = metadata.get('data_source')\n",
        "    if not isinstance(data_source, str) or not data_source:\n",
        "        # The data_source must be a non-empty string.\n",
        "        errors.append(\n",
        "            \"Metadata validation failed: 'data_source' must be a non-empty string.\"\n",
        "        )\n",
        "\n",
        "    # Return the aggregated list of all found errors.\n",
        "    return errors\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 2, Step 2 & 3: Hyperparameter Range, Type, and Special Validation\n",
        "# ======================================================================================\n",
        "\n",
        "def _validate_hyperparameters(\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the 'hyperparameters' section using a declarative schema.\n",
        "\n",
        "    This function provides a rigorous, schema-driven validation of the\n",
        "    'hyperparameters' dictionary. It checks for the presence, type, and value\n",
        "    constraints (e.g., range, exact value, non-empty list) of each expected\n",
        "    hyperparameter. It is a critical gatekeeper that ensures the integrity of\n",
        "    all model parameters before the pipeline execution begins. This revised\n",
        "    version includes an explicit check for non-empty lists.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]):\n",
        "            The top-level configuration dictionary which is expected to contain\n",
        "            a 'hyperparameters' key.\n",
        "\n",
        "    Returns:\n",
        "        List[str]:\n",
        "            A list of specific, actionable error messages. An empty list\n",
        "            signifies that all hyperparameter validation checks have passed\n",
        "            successfully.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate potential error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Define the validation schema for all expected hyperparameters.\n",
        "    # This schema is declarative, robust, and easily extensible.\n",
        "    # New Rule: 'non_empty' has been added for list validation.\n",
        "    schema: Dict[str, Dict[str, Any]] = {\n",
        "        'maturity_filter_days': {'type': int, 'range': (1, 365)},\n",
        "        'liquidity_filter_spread_pct': {'type': float, 'range': (0.001, 1.0)},\n",
        "        'ransac_residual_sq_threshold': {'type': float, 'range': (0.0001, 0.1)},\n",
        "        'ransac_slope_tolerance_pct': {'type': float, 'range': (0.01, 0.5)},\n",
        "        'ransac_min_sample_size': {'type': int, 'exact_value': 2},\n",
        "        'ransac_num_iterations': {'type': int, 'range': (10, 10000)},\n",
        "        'estimation_lambda_weight': {'type': float, 'range': (0.1, 10.0)},\n",
        "        'annualization_factor': {'type': float, 'exact_value': 365.25},\n",
        "        'target_tenors_days': {\n",
        "            'type': list,\n",
        "            'non_empty': True, # New rule to enforce non-empty list.\n",
        "            'element_type': int,\n",
        "            'element_positive': True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Check 1: Ensure the 'hyperparameters' key exists and is a dictionary.\n",
        "    hyperparams = config.get('hyperparameters')\n",
        "    if not isinstance(hyperparams, dict):\n",
        "        # If the main section is missing or invalid, no further checks are possible.\n",
        "        errors.append(\"Configuration validation failed: Missing or invalid 'hyperparameters' section.\")\n",
        "        return errors\n",
        "\n",
        "    # Check 2: Iterate through the schema to validate each expected parameter.\n",
        "    for param, rules in schema.items():\n",
        "        # Sub-check 2a: Check for the presence of the parameter.\n",
        "        if param not in hyperparams:\n",
        "            errors.append(f\"Hyperparameter validation failed: Missing required parameter '{param}'.\")\n",
        "            # Skip further checks for this missing parameter.\n",
        "            continue\n",
        "\n",
        "        # Retrieve the value for the current parameter.\n",
        "        value = hyperparams[param]\n",
        "        # Retrieve the expected primary type from the schema.\n",
        "        expected_type = rules['type']\n",
        "\n",
        "        # Sub-check 2b: Validate the primary type of the parameter.\n",
        "        if not isinstance(value, expected_type):\n",
        "            errors.append(\n",
        "                f\"Hyperparameter validation failed: '{param}' must be of type \"\n",
        "                f\"{expected_type.__name__}, but found {type(value).__name__}.\"\n",
        "            )\n",
        "            # If the primary type is wrong, subsequent checks are invalid.\n",
        "            continue\n",
        "\n",
        "        # Sub-check 2c: Perform detailed validation based on schema rules.\n",
        "        if expected_type is list:\n",
        "            # --- Enhanced List Validation ---\n",
        "            # First, check if the list is required to be non-empty.\n",
        "            if rules.get('non_empty', False) and not value:\n",
        "                errors.append(f\"Hyperparameter validation failed: '{param}' must be a non-empty list.\")\n",
        "            # Only proceed to check elements if the list is not empty.\n",
        "            elif value:\n",
        "                # Check that all elements in the list have the correct type.\n",
        "                element_type = rules.get('element_type')\n",
        "                if not all(isinstance(el, element_type) for el in value):\n",
        "                    errors.append(f\"Hyperparameter validation failed: All elements in '{param}' must be of type {element_type.__name__}.\")\n",
        "                # Check if all elements in the list must be positive.\n",
        "                if rules.get('element_positive') and not all(el > 0 for el in value):\n",
        "                    errors.append(f\"Hyperparameter validation failed: All elements in '{param}' must be positive.\")\n",
        "\n",
        "        elif 'range' in rules:\n",
        "            # For numeric types with a range constraint.\n",
        "            min_val, max_val = rules['range']\n",
        "            if not (min_val <= value <= max_val):\n",
        "                errors.append(\n",
        "                    f\"Hyperparameter validation failed: '{param}' must be within the range \"\n",
        "                    f\"[{min_val}, {max_val}], but found {value}.\"\n",
        "                )\n",
        "\n",
        "        elif 'exact_value' in rules:\n",
        "            # For parameters that must have an exact value.\n",
        "            exact_val = rules['exact_value']\n",
        "            # Use math.isclose for robust floating-point comparison.\n",
        "            if isinstance(exact_val, float):\n",
        "                if not math.isclose(value, exact_val, rel_tol=1e-9, abs_tol=1e-9):\n",
        "                    errors.append(f\"Hyperparameter validation failed: '{param}' must be exactly {exact_val}, but found {value}.\")\n",
        "            # Use direct comparison for other types (e.g., int).\n",
        "            elif value != exact_val:\n",
        "                errors.append(f\"Hyperparameter validation failed: '{param}' must be exactly {exact_val}, but found {value}.\")\n",
        "\n",
        "    # Check 3: Identify any unrecognized parameters to catch potential typos.\n",
        "    unrecognized_params = set(hyperparams.keys()) - set(schema.keys())\n",
        "    if unrecognized_params:\n",
        "        errors.append(f\"Hyperparameter validation failed: Unrecognized parameters found: {unrecognized_params}.\")\n",
        "\n",
        "    # Return the aggregated list of all found errors.\n",
        "    return errors\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 2: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def validate_hyperparameter_dictionary(\n",
        "    fused_input_state: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates a comprehensive validation of the hyperparameter dictionary.\n",
        "\n",
        "    This master function validates the entire configuration structure, including\n",
        "    the 'metadata' and 'hyperparameters' sections. It uses a declarative,\n",
        "    schema-driven approach to ensure all parameters are present, correctly typed,\n",
        "    and within their valid operational ranges. A successful execution indicates\n",
        "    the configuration is safe to use for the pipeline.\n",
        "\n",
        "    Args:\n",
        "        fused_input_state (Dict[str, Any]):\n",
        "            The primary configuration dictionary for the study, containing\n",
        "            metadata and algorithm hyperparameters.\n",
        "\n",
        "    Raises:\n",
        "        TypeError:\n",
        "            If the input `fused_input_state` is not a dictionary.\n",
        "        ValueError:\n",
        "            If any validation checks fail. The error message provides a detailed,\n",
        "            itemized list of all identified configuration issues.\n",
        "\n",
        "    Returns:\n",
        "        None:\n",
        "            This function returns nothing upon successful validation, otherwise\n",
        "            it raises an exception.\n",
        "    \"\"\"\n",
        "    # --- Input Type Validation ---\n",
        "    # Ensure the top-level input is a dictionary.\n",
        "    if not isinstance(fused_input_state, dict):\n",
        "        raise TypeError(\"Input `fused_input_state` must be a dictionary.\")\n",
        "\n",
        "    # --- Aggregation of Validation Errors ---\n",
        "    # Initialize a list to hold all error messages.\n",
        "    all_errors: List[str] = []\n",
        "\n",
        "    # Execute Step 1: Validate the 'metadata' section.\n",
        "    metadata_errors = _validate_metadata(config=fused_input_state)\n",
        "    all_errors.extend(metadata_errors)\n",
        "\n",
        "    # Execute Step 2 & 3: Validate the 'hyperparameters' section.\n",
        "    hyperparam_errors = _validate_hyperparameters(config=fused_input_state)\n",
        "    all_errors.extend(hyperparam_errors)\n",
        "\n",
        "    # --- Final Error Reporting ---\n",
        "    # Check if the aggregated list of errors is non-empty.\n",
        "    if all_errors:\n",
        "        # If errors were found, construct a comprehensive error message.\n",
        "        error_header = \"Input hyperparameter dictionary validation failed with the following errors:\"\n",
        "        # Format each error for clarity.\n",
        "        formatted_errors = \"\\n\".join(f\"- {error}\" for error in all_errors)\n",
        "        # Raise a single, informative ValueError.\n",
        "        raise ValueError(f\"{error_header}\\n{formatted_errors}\")\n",
        "\n",
        "    # If no errors were found, the function completes silently.\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "IkX3sCa3Zd7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Data Completeness and Consistency Validation\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 3, Step 1: Temporal Coverage Analysis\n",
        "# ======================================================================================\n",
        "\n",
        "def _validate_temporal_coverage(\n",
        "    df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the temporal density and continuity of the dataset.\n",
        "\n",
        "    This function checks if the dataset meets minimum requirements for the number\n",
        "    of observations over time and ensures there are no excessively large gaps\n",
        "    between consecutive data points.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame):\n",
        "            The input DataFrame, assumed to have a valid MultiIndex structure.\n",
        "\n",
        "    Returns:\n",
        "        List[str]:\n",
        "            A list of error messages. An empty list indicates sufficient temporal coverage.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate potential error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Define temporal coverage thresholds.\n",
        "    MIN_UNIQUE_TIMESTAMPS = 100\n",
        "    MAX_GAP_DAYS = pd.Timedelta(days=7)\n",
        "\n",
        "    # Extract the unique timestamps from the index for analysis.\n",
        "    unique_timestamps = df.index.get_level_values('timestamp').unique()\n",
        "\n",
        "    # Check 1: Ensure a minimum number of unique timestamps exist.\n",
        "    if len(unique_timestamps) < MIN_UNIQUE_TIMESTAMPS:\n",
        "        errors.append(\n",
        "            f\"Temporal coverage failed: Insufficient unique timestamps. \"\n",
        "            f\"Found {len(unique_timestamps)}, but require at least {MIN_UNIQUE_TIMESTAMPS}.\"\n",
        "        )\n",
        "\n",
        "    # Check 2: Calculate and validate the maximum gap between consecutive timestamps.\n",
        "    if len(unique_timestamps) > 1:\n",
        "        # Sort timestamps to ensure correct diff calculation.\n",
        "        sorted_timestamps = unique_timestamps.sort_values()\n",
        "        # Calculate the difference between each consecutive timestamp.\n",
        "        gaps = pd.Series(sorted_timestamps).diff()\n",
        "        # Find the largest gap.\n",
        "        max_gap = gaps.max()\n",
        "        # Check if the largest gap exceeds the allowed threshold.\n",
        "        if max_gap > MAX_GAP_DAYS:\n",
        "            errors.append(\n",
        "                f\"Temporal coverage failed: Maximum gap between consecutive timestamps \"\n",
        "                f\"is {max_gap}, which exceeds the threshold of {MAX_GAP_DAYS}.\"\n",
        "            )\n",
        "\n",
        "    # Return the aggregated list of all found errors.\n",
        "    return errors\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 3, Step 2: Cross-Sectional Coverage Validation\n",
        "# ======================================================================================\n",
        "\n",
        "def _validate_cross_sectional_coverage(\n",
        "    df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates the cross-sectional density of option contracts.\n",
        "\n",
        "    For each (timestamp, expiry_date) group, this function checks for a\n",
        "    sufficient number of option pairs and an adequate range of moneyness to\n",
        "    ensure the stability of the estimation procedure.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame):\n",
        "            The input DataFrame, assumed to have a valid MultiIndex structure.\n",
        "\n",
        "    Returns:\n",
        "        List[str]:\n",
        "            A list of error messages. An empty list indicates sufficient cross-sectional coverage.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate potential error messages.\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Define cross-sectional coverage thresholds.\n",
        "    MIN_CALL_PUT_PAIRS = 5\n",
        "    MIN_MONEYNESS = 0.7\n",
        "    MAX_MONEYNESS = 1.3\n",
        "\n",
        "    # Group the DataFrame by the estimation unit: (timestamp, asset, expiry_date).\n",
        "    grouped = df.groupby(['timestamp', 'asset', 'expiry_date'])\n",
        "    num_groups = len(grouped)\n",
        "\n",
        "    # --- Check 1: Minimum number of call-put pairs ---\n",
        "    # To accurately check for pairs, we pivot the data.\n",
        "    try:\n",
        "        # Create a pivot table to align calls and puts for each strike.\n",
        "        paired_data = df.reset_index().pivot_table(\n",
        "            index=['timestamp', 'asset', 'expiry_date', 'strike_price'],\n",
        "            columns='option_type',\n",
        "            values='best_bid_price' # Value column is arbitrary, just for structure\n",
        "        )\n",
        "        # A valid pair exists where neither 'call' nor 'put' is NaN.\n",
        "        valid_pairs = paired_data.dropna()\n",
        "        # Count the number of valid pairs per estimation group.\n",
        "        pair_counts = valid_pairs.groupby(['timestamp', 'asset', 'expiry_date']).size()\n",
        "\n",
        "        # Identify groups that fail to meet the minimum pair requirement.\n",
        "        insufficient_pair_groups = pair_counts[pair_counts < MIN_CALL_PUT_PAIRS]\n",
        "        if not insufficient_pair_groups.empty:\n",
        "            pct_failing = 100 * len(insufficient_pair_groups) / num_groups\n",
        "            errors.append(\n",
        "                f\"Cross-sectional coverage failed: {len(insufficient_pair_groups)} groups \"\n",
        "                f\"({pct_failing:.2f}%) have fewer than {MIN_CALL_PUT_PAIRS} call-put pairs.\"\n",
        "            )\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Cross-sectional coverage failed during pairing analysis: {e}\")\n",
        "\n",
        "    # --- Check 2: Moneyness range ---\n",
        "    # Compute moneyness on the fly for the entire DataFrame.\n",
        "    df_with_moneyness = df.copy()\n",
        "    df_with_moneyness['moneyness'] = (\n",
        "        df_with_moneyness.index.get_level_values('strike_price') / df_with_moneyness['spot_price']\n",
        "    )\n",
        "\n",
        "    # Calculate the min and max moneyness for each group.\n",
        "    moneyness_ranges = df_with_moneyness.groupby(['timestamp', 'asset', 'expiry_date'])['moneyness'].agg(['min', 'max'])\n",
        "\n",
        "    # Identify groups with an insufficient moneyness range.\n",
        "    inadequate_range_mask = (moneyness_ranges['min'] > MIN_MONEYNESS) | (moneyness_ranges['max'] < MAX_MONEYNESS)\n",
        "    num_inadequate_range = inadequate_range_mask.sum()\n",
        "    if num_inadequate_range > 0:\n",
        "        pct_failing = 100 * num_inadequate_range / num_groups\n",
        "        errors.append(\n",
        "            f\"Cross-sectional coverage failed: {num_inadequate_range} groups \"\n",
        "            f\"({pct_failing:.2f}%) do not have strikes spanning the required moneyness \"\n",
        "            f\"range of [{MIN_MONEYNESS}, {MAX_MONEYNESS}].\"\n",
        "        )\n",
        "\n",
        "    # --- Check 3: Futures price availability ---\n",
        "    # Check if a single, unique, non-null futures price exists for each group.\n",
        "    futures_check = grouped['futures_price'].agg(lambda x: x.nunique() == 1 and not x.isnull().any())\n",
        "    num_missing_futures = (~futures_check).sum()\n",
        "    if num_missing_futures > 0:\n",
        "        pct_failing = 100 * num_missing_futures / num_groups\n",
        "        errors.append(\n",
        "            f\"Cross-sectional coverage failed: {num_missing_futures} groups \"\n",
        "            f\"({pct_failing:.2f}%) are missing a unique, valid futures price.\"\n",
        "        )\n",
        "\n",
        "    # Return the aggregated list of all found errors.\n",
        "    return errors\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 3, Step 3: Market Data Quality Assessment\n",
        "# ======================================================================================\n",
        "\n",
        "def _validate_market_data_quality(\n",
        "    df: pd.DataFrame\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Performs a high-level assessment of the market data quality.\n",
        "\n",
        "    This function screens for common data quality issues like zero prices,\n",
        "    inverted spreads, and extremely wide spreads, which can indicate illiquid\n",
        "    markets or data feed errors.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame):\n",
        "            The input DataFrame, assumed to have a valid schema.\n",
        "\n",
        "    Returns:\n",
        "        List[str]:\n",
        "            A list of error messages detailing data quality issues.\n",
        "    \"\"\"\n",
        "    # Initialize a list to aggregate potential error messages.\n",
        "    errors: List[str] = []\n",
        "    total_rows = len(df)\n",
        "\n",
        "    # Check 1: Identify rows with zero bid or ask prices.\n",
        "    zero_price_mask = (df['best_bid_price'] == 0) | (df['best_ask_price'] == 0)\n",
        "    num_zero_prices = zero_price_mask.sum()\n",
        "    if num_zero_prices > 0:\n",
        "        pct_zero = 100 * num_zero_prices / total_rows\n",
        "        errors.append(\n",
        "            f\"Market data quality warning: Found {num_zero_prices} rows \"\n",
        "            f\"({pct_zero:.2f}%) with a zero bid or ask price.\"\n",
        "        )\n",
        "\n",
        "    # Check 2: Identify rows with inverted bid-ask spreads (re-checked here for logging).\n",
        "    inverted_spread_mask = df['best_ask_price'] < df['best_bid_price']\n",
        "    num_inverted = inverted_spread_mask.sum()\n",
        "    if num_inverted > 0:\n",
        "        pct_inverted = 100 * num_inverted / total_rows\n",
        "        errors.append(\n",
        "            f\"Market data quality warning: Found {num_inverted} rows \"\n",
        "            f\"({pct_inverted:.2f}%) with inverted bid-ask spreads (ask < bid).\"\n",
        "        )\n",
        "\n",
        "    # Check 3: Identify rows with extreme relative spreads (> 100%).\n",
        "    # Calculate mid-price, handling potential division by zero.\n",
        "    mid_price = (df['best_ask_price'] + df['best_bid_price']) / 2\n",
        "    # Calculate relative spread safely.\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        relative_spread = (df['best_ask_price'] - df['best_bid_price']) / mid_price\n",
        "\n",
        "    # An extreme spread is defined as > 100%.\n",
        "    extreme_spread_mask = relative_spread > 1.0\n",
        "    num_extreme = extreme_spread_mask.sum()\n",
        "    if num_extreme > 0:\n",
        "        pct_extreme = 100 * num_extreme / total_rows\n",
        "        errors.append(\n",
        "            f\"Market data quality warning: Found {num_extreme} rows \"\n",
        "            f\"({pct_extreme:.2f}%) with an extreme relative spread (>100%).\"\n",
        "        )\n",
        "\n",
        "    # Return the aggregated list of all found errors.\n",
        "    return errors\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 3: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def validate_data_completeness(\n",
        "    df_master: pd.DataFrame\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates a validation of the dataset's completeness and consistency.\n",
        "\n",
        "    This master function executes a sequence of checks to ensure the data is\n",
        "    sufficiently dense and of high enough quality for a robust analysis. It\n",
        "    validates temporal coverage, cross-sectional (instrument) density, and\n",
        "    market data quality. A failure indicates the dataset is not suitable for\n",
        "    the yield curve estimation pipeline in its current state.\n",
        "\n",
        "    Args:\n",
        "        df_master (pd.DataFrame):\n",
        "            The primary input DataFrame, which is assumed to have already passed\n",
        "            the structural validation from `validate_dataframe_structure`.\n",
        "\n",
        "    Raises:\n",
        "        TypeError:\n",
        "            If the input `df_master` is not a pandas DataFrame.\n",
        "        ValueError:\n",
        "            If any of the completeness or consistency checks fail. The error\n",
        "            message contains a detailed, itemized list of all identified issues.\n",
        "\n",
        "    Returns:\n",
        "        None:\n",
        "            This function does not return a value. It either completes silently,\n",
        "            indicating success, or raises an exception, indicating failure.\n",
        "    \"\"\"\n",
        "    # --- Input Type Validation ---\n",
        "    if not isinstance(df_master, pd.DataFrame):\n",
        "        raise TypeError(\"Input `df_master` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Aggregation of Validation Errors ---\n",
        "    all_errors: List[str] = []\n",
        "\n",
        "    # Execute Step 1: Temporal coverage validation.\n",
        "    temporal_errors = _validate_temporal_coverage(df=df_master)\n",
        "    all_errors.extend(temporal_errors)\n",
        "\n",
        "    # Execute Step 2: Cross-sectional coverage validation.\n",
        "    cross_sectional_errors = _validate_cross_sectional_coverage(df=df_master)\n",
        "    all_errors.extend(cross_sectional_errors)\n",
        "\n",
        "    # Execute Step 3: Market data quality assessment.\n",
        "    # Note: These are often treated as warnings but are raised as errors here\n",
        "    # for maximum rigor, forcing the user to acknowledge them.\n",
        "    quality_errors = _validate_market_data_quality(df=df_master)\n",
        "    all_errors.extend(quality_errors)\n",
        "\n",
        "    # --- Final Error Reporting ---\n",
        "    if all_errors:\n",
        "        error_header = \"Input DataFrame failed data completeness and consistency validation:\"\n",
        "        formatted_errors = \"\\n\".join(f\"- {error}\" for error in all_errors)\n",
        "        raise ValueError(f\"{error_header}\\n{formatted_errors}\")\n",
        "\n",
        "    # If no errors were found, the function completes silently.\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "uMMfD-WxaZYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Maturity-Based Filtering.\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 4: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def filter_by_maturity(\n",
        "    df_master: pd.DataFrame,\n",
        "    maturity_filter_days: int\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Filters option contracts based on a minimum time-to-expiry threshold.\n",
        "\n",
        "    This function implements the first data cleansing step as per Section 4.2 of\n",
        "    the reference paper. It calculates the time-to-expiry for each option contract\n",
        "    and excludes those with a maturity less than the specified threshold. The\n",
        "    function is designed to be a pure transformation, returning a new, filtered\n",
        "    DataFrame and a detailed diagnostic report of the operation.\n",
        "\n",
        "    Args:\n",
        "        df_master (pd.DataFrame):\n",
        "            The input DataFrame containing option market data. It is assumed to\n",
        "            have passed prior validation steps, ensuring a valid MultiIndex with\n",
        "            'timestamp' and 'expiry_date' levels.\n",
        "        maturity_filter_days (int):\n",
        "            The minimum number of days to expiry for an option to be included.\n",
        "            Contracts with maturities strictly less than this value will be\n",
        "            excluded.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - pd.DataFrame: A new DataFrame containing only the option contracts\n",
        "              that meet the minimum maturity requirement.\n",
        "            - Dict[str, Any]: A diagnostic dictionary detailing the filtering\n",
        "              process, including initial and final row counts, and the number\n",
        "              and percentage of rows that were excluded.\n",
        "\n",
        "    Raises:\n",
        "        TypeError:\n",
        "            If `df_master` is not a pandas DataFrame or if `maturity_filter_days`\n",
        "            is not an integer.\n",
        "        ValueError:\n",
        "            If `maturity_filter_days` is not a positive integer.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify the type of the main DataFrame input.\n",
        "    if not isinstance(df_master, pd.DataFrame):\n",
        "        raise TypeError(\"Input `df_master` must be a pandas DataFrame.\")\n",
        "    # Verify the type of the threshold parameter.\n",
        "    if not isinstance(maturity_filter_days, int):\n",
        "        raise TypeError(\"Input `maturity_filter_days` must be an integer.\")\n",
        "    # Verify the value of the threshold parameter.\n",
        "    if maturity_filter_days <= 0:\n",
        "        raise ValueError(\"Input `maturity_filter_days` must be a positive integer.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Record the initial number of rows for the diagnostic report.\n",
        "    initial_row_count = len(df_master)\n",
        "\n",
        "    # Create a copy to ensure the function is pure and avoids side effects.\n",
        "    df_filtered = df_master.copy()\n",
        "\n",
        "    # --- Step 1: Time-to-Expiry Calculation ---\n",
        "    # Extract the timestamp and expiry date levels from the MultiIndex.\n",
        "    timestamps = df_filtered.index.get_level_values('timestamp')\n",
        "    expiries = df_filtered.index.get_level_values('expiry_date')\n",
        "\n",
        "    # Calculate the time difference as a TimedeltaIndex. This is a robust,\n",
        "    # timezone-aware calculation provided the inputs are consistent.\n",
        "    time_to_expiry_delta = expiries - timestamps\n",
        "\n",
        "    # Convert the TimedeltaIndex to a floating-point number of days.\n",
        "    # This is the most precise way to represent the duration for comparison.\n",
        "    # Equation Ref: Corresponds to calculating (T_i - t) for the filtering rule.\n",
        "    time_to_expiry_days = time_to_expiry_delta / pd.Timedelta(days=1)\n",
        "\n",
        "    # --- Step 2: Maturity Threshold Application ---\n",
        "    # Create a boolean mask to identify rows that meet the inclusion criteria.\n",
        "    # The condition is inclusive of the boundary (>=).\n",
        "    # Filtering Rule: Inclusion Condition: τ_{i,t} >= maturity_filter_days\n",
        "    inclusion_mask = time_to_expiry_days >= maturity_filter_days\n",
        "\n",
        "    # Apply the mask to the DataFrame to select the desired rows.\n",
        "    # Using .loc with a boolean mask is the canonical and most efficient method.\n",
        "    df_filtered = df_filtered.loc[inclusion_mask]\n",
        "\n",
        "    # --- Step 3: Filtering Validation and Documentation ---\n",
        "    # Record the final number of rows after filtering.\n",
        "    final_row_count = len(df_filtered)\n",
        "\n",
        "    # Calculate the number of rows that were excluded by the filter.\n",
        "    rows_excluded = initial_row_count - final_row_count\n",
        "\n",
        "    # Calculate the percentage of rows excluded, handling the case of zero initial rows.\n",
        "    percentage_excluded = (\n",
        "        (rows_excluded / initial_row_count * 100)\n",
        "        if initial_row_count > 0\n",
        "        else 0.0\n",
        "    )\n",
        "\n",
        "    # Assemble the diagnostic report dictionary.\n",
        "    diagnostics = {\n",
        "        \"filter_type\": \"Maturity-Based Filtering\",\n",
        "        \"maturity_threshold_days\": maturity_filter_days,\n",
        "        \"initial_row_count\": initial_row_count,\n",
        "        \"final_row_count\": final_row_count,\n",
        "        \"rows_excluded\": rows_excluded,\n",
        "        \"percentage_excluded\": round(percentage_excluded, 4)\n",
        "    }\n",
        "\n",
        "    # Return the filtered DataFrame and the detailed diagnostic report.\n",
        "    return df_filtered, diagnostics\n",
        "\n"
      ],
      "metadata": {
        "id": "_x9gIJWObCGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Liquidity-Based Filtering\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 5: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def filter_by_liquidity(\n",
        "    df_master: pd.DataFrame,\n",
        "    liquidity_filter_spread_pct: float\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Filters option contracts based on a maximum relative bid-ask spread.\n",
        "\n",
        "    This function implements the second data cleansing step as per Section 4.2 of\n",
        "    the reference paper. It calculates the relative bid-ask spread for each\n",
        "    option and excludes those with a spread exceeding the specified percentage\n",
        "    threshold. This is a crucial step to remove illiquid or stale quotes.\n",
        "\n",
        "    The relative spread is calculated as:\n",
        "    Spread_i = (Ask_i - Bid_i) / Mid_i\n",
        "    where Mid_i = (Ask_i + Bid_i) / 2\n",
        "\n",
        "    Args:\n",
        "        df_master (pd.DataFrame):\n",
        "            The input DataFrame, potentially already filtered by maturity. It is\n",
        "            assumed to have passed prior validation, ensuring the presence and\n",
        "            validity of 'best_bid_price' and 'best_ask_price' columns.\n",
        "        liquidity_filter_spread_pct (float):\n",
        "            The maximum allowable relative bid-ask spread, expressed as a decimal\n",
        "            (e.g., 0.15 for 15%). Contracts with a spread greater than this\n",
        "            value will be excluded.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - pd.DataFrame: A new DataFrame containing only the option contracts\n",
        "              that meet the liquidity requirement.\n",
        "            - Dict[str, Any]: A diagnostic dictionary detailing the filtering\n",
        "              process, including counts and statistics on excluded rows.\n",
        "\n",
        "    Raises:\n",
        "        TypeError:\n",
        "            If `df_master` is not a pandas DataFrame or if\n",
        "            `liquidity_filter_spread_pct` is not a float.\n",
        "        ValueError:\n",
        "            If `liquidity_filter_spread_pct` is not within a sensible range\n",
        "            (e.g., between 0.0 and 2.0).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify the type of the main DataFrame input.\n",
        "    if not isinstance(df_master, pd.DataFrame):\n",
        "        raise TypeError(\"Input `df_master` must be a pandas DataFrame.\")\n",
        "    # Verify the type of the threshold parameter.\n",
        "    if not isinstance(liquidity_filter_spread_pct, float):\n",
        "        raise TypeError(\"Input `liquidity_filter_spread_pct` must be a float.\")\n",
        "    # Verify the value of the threshold parameter is within a reasonable range.\n",
        "    if not (0.0 < liquidity_filter_spread_pct <= 2.0):\n",
        "        raise ValueError(\n",
        "            \"Input `liquidity_filter_spread_pct` must be a positive float, \"\n",
        "            \"typically between 0.0 and 2.0 (200%).\"\n",
        "        )\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Record the initial number of rows for the diagnostic report.\n",
        "    initial_row_count = len(df_master)\n",
        "\n",
        "    # Create a copy to ensure the function is pure and avoids side effects.\n",
        "    df_with_spread = df_master.copy()\n",
        "\n",
        "    # --- Step 1: Bid-Ask Spread Calculation ---\n",
        "    # Extract bid and ask price Series for calculation.\n",
        "    bid_prices = df_with_spread['best_bid_price']\n",
        "    ask_prices = df_with_spread['best_ask_price']\n",
        "\n",
        "    # Calculate the absolute spread. Assumes ask >= bid from prior validation.\n",
        "    # Equation Ref: Numerator of Spread_i = Ask_i - Bid_i\n",
        "    absolute_spread = ask_prices - bid_prices\n",
        "\n",
        "    # Calculate the mid-price.\n",
        "    # Equation Ref: Denominator's component Mid_i = (Ask_i + Bid_i) / 2\n",
        "    mid_price = (ask_prices + bid_prices) / 2\n",
        "\n",
        "    # Calculate the relative spread with robust handling of division by zero.\n",
        "    # If mid_price is 0, the spread is effectively infinite and should be filtered.\n",
        "    # We assign np.inf in such cases, ensuring they are caught by the filter.\n",
        "    # Equation Ref: Spread_i = (Ask_i - Bid_i) / Mid_i\n",
        "    relative_spread = np.divide(\n",
        "        absolute_spread,\n",
        "        mid_price,\n",
        "        out=np.full_like(mid_price, np.inf, dtype=np.float64),\n",
        "        where=mid_price != 0\n",
        "    )\n",
        "\n",
        "    # Add the calculated spread to the DataFrame for filtering and diagnostics.\n",
        "    df_with_spread['relative_spread'] = relative_spread\n",
        "\n",
        "    # --- Step 2: Liquidity Threshold Application ---\n",
        "    # Create a boolean mask to identify rows that meet the liquidity criteria.\n",
        "    # The condition is inclusive of the boundary (<=).\n",
        "    # Filtering Rule: Inclusion Condition: Spread_i <= liquidity_filter_spread_pct\n",
        "    inclusion_mask = df_with_spread['relative_spread'] <= liquidity_filter_spread_pct\n",
        "\n",
        "    # Apply the mask to the DataFrame.\n",
        "    df_filtered = df_with_spread.loc[inclusion_mask].drop(columns=['relative_spread'])\n",
        "\n",
        "    # Identify the rows that were excluded for diagnostic purposes.\n",
        "    excluded_rows = df_with_spread.loc[~inclusion_mask]\n",
        "\n",
        "    # --- Step 3: Filtering Validation and Documentation ---\n",
        "    # Record the final number of rows after filtering.\n",
        "    final_row_count = len(df_filtered)\n",
        "\n",
        "    # Calculate the number of rows that were excluded.\n",
        "    rows_excluded = initial_row_count - final_row_count\n",
        "\n",
        "    # Calculate the percentage of rows excluded.\n",
        "    percentage_excluded = (\n",
        "        (rows_excluded / initial_row_count * 100)\n",
        "        if initial_row_count > 0\n",
        "        else 0.0\n",
        "    )\n",
        "\n",
        "    # Assemble the diagnostic report.\n",
        "    diagnostics = {\n",
        "        \"filter_type\": \"Liquidity-Based Filtering\",\n",
        "        \"liquidity_threshold_pct\": liquidity_filter_spread_pct,\n",
        "        \"input_row_count\": initial_row_count,\n",
        "        \"final_row_count\": final_row_count,\n",
        "        \"rows_excluded\": rows_excluded,\n",
        "        \"percentage_excluded\": round(percentage_excluded, 4),\n",
        "        \"excluded_spread_stats\": {\n",
        "            \"min\": excluded_rows['relative_spread'].min(),\n",
        "            \"mean\": excluded_rows['relative_spread'].mean(),\n",
        "            \"median\": excluded_rows['relative_spread'].median(),\n",
        "            \"max\": excluded_rows['relative_spread'].max(),\n",
        "            \"std\": excluded_rows['relative_spread'].std()\n",
        "        } if rows_excluded > 0 else {}\n",
        "    }\n",
        "\n",
        "    # Return the filtered DataFrame and the detailed diagnostic report.\n",
        "    return df_filtered, diagnostics\n"
      ],
      "metadata": {
        "id": "vuAGe3URbn21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Mid-Price and Moneyness Computation\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 6: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def prepare_features_and_pairs(\n",
        "    df_master: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Enriches the DataFrame with derived features and filters for complete call-put pairs.\n",
        "\n",
        "    This function performs the final data preparation steps before the core\n",
        "    RANSAC algorithm. It executes three critical operations:\n",
        "    1.  Computes the mid-price for each option contract.\n",
        "    2.  Computes the moneyness (K/S) for each option contract.\n",
        "    3.  Filters the dataset to retain only options that form a complete and\n",
        "        unique call-put pair for a given strike, expiry, and timestamp.\n",
        "\n",
        "    This ensures the output DataFrame is perfectly structured and contains all\n",
        "    necessary features for the subsequent analysis phases.\n",
        "\n",
        "    Args:\n",
        "        df_master (pd.DataFrame):\n",
        "            The input DataFrame, assumed to have been cleansed by prior filtering\n",
        "            steps (maturity, liquidity). It must have a valid schema.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - pd.DataFrame: A new, analysis-ready DataFrame with 'mid_price' and\n",
        "              'moneyness' columns, containing only complete call-put pairs.\n",
        "            - Dict[str, Any]: A diagnostic dictionary detailing the pairing\n",
        "              process, including the number of rows and unique strikes removed.\n",
        "\n",
        "    Raises:\n",
        "        TypeError:\n",
        "            If `df_master` is not a pandas DataFrame.\n",
        "        AssertionError:\n",
        "            If critical columns like 'spot_price' contain non-positive values,\n",
        "            indicating an issue that should have been caught by prior validation.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify the type of the main DataFrame input.\n",
        "    if not isinstance(df_master, pd.DataFrame):\n",
        "        raise TypeError(\"Input `df_master` must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Record the initial number of rows for the diagnostic report.\n",
        "    initial_row_count = len(df_master)\n",
        "\n",
        "    # Create a copy to ensure the function is pure and avoids side effects.\n",
        "    df_processed = df_master.copy()\n",
        "\n",
        "    # --- Step 1: Mid-Price Computation and Validation ---\n",
        "    # This step computes the mid-price, which will be used in the final estimation.\n",
        "    # Equation Ref: Mid_{i,t} = (Bid_{i,t} + Ask_{i,t}) / 2\n",
        "    df_processed['mid_price'] = (\n",
        "        df_processed['best_bid_price'] + df_processed['best_ask_price']\n",
        "    ) / 2\n",
        "\n",
        "    # --- Step 2: Moneyness Calculation Implementation ---\n",
        "    # This step computes moneyness, a key regressor in the estimation model.\n",
        "    # Defensively assert that spot prices are positive, as this is a critical assumption.\n",
        "    assert (df_processed['spot_price'] > 0).all(), \"Spot prices must be positive for moneyness calculation.\"\n",
        "\n",
        "    # Equation Ref: m_{i,t} = K_i / S_t\n",
        "    df_processed['moneyness'] = (\n",
        "        df_processed.index.get_level_values('strike_price') / df_processed['spot_price']\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Call-Put Pairing Validation and Filtering ---\n",
        "    # This is the most critical step to ensure data is correctly structured for RANSAC.\n",
        "    # Define the index levels that uniquely identify an option series for pairing.\n",
        "    pairing_levels = ['timestamp', 'asset', 'expiry_date', 'strike_price']\n",
        "\n",
        "    # First, perform a highly efficient filter to keep only groups that have\n",
        "    # exactly two entries (a potential call-put pair).\n",
        "    # The use of .transform('size') is significantly faster than .filter().\n",
        "    group_sizes = df_processed.groupby(pairing_levels).transform('size')\n",
        "    df_potential_pairs = df_processed[group_sizes == 2].copy()\n",
        "\n",
        "    # Now, on this much smaller DataFrame, perform the rigorous check to ensure\n",
        "    # that the two entries are indeed one 'call' and one 'put'.\n",
        "    def check_valid_pair(group: pd.DataFrame) -> bool:\n",
        "        # A valid pair must contain exactly one 'call' and one 'put'.\n",
        "        return set(group.index.get_level_values('option_type')) == {'call', 'put'}\n",
        "\n",
        "    # Use .groupby().filter() to apply the rigorous check.\n",
        "    df_final_pairs = df_potential_pairs.groupby(pairing_levels).filter(check_valid_pair)\n",
        "\n",
        "    # --- Diagnostic Reporting ---\n",
        "    # Record the final number of rows after pairing.\n",
        "    final_row_count = len(df_final_pairs)\n",
        "\n",
        "    # Calculate the number of rows that were excluded.\n",
        "    rows_excluded = initial_row_count - final_row_count\n",
        "\n",
        "    # Calculate the percentage of rows excluded.\n",
        "    percentage_excluded = (\n",
        "        (rows_excluded / initial_row_count * 100)\n",
        "        if initial_row_count > 0\n",
        "        else 0.0\n",
        "    )\n",
        "\n",
        "    # Calculate the number of unique strikes before and after.\n",
        "    initial_unique_strikes = df_master.index.droplevel('option_type').nunique()\n",
        "    final_unique_strikes = df_final_pairs.index.droplevel('option_type').nunique()\n",
        "    strikes_excluded = initial_unique_strikes - final_unique_strikes\n",
        "\n",
        "    # Assemble the diagnostic report.\n",
        "    diagnostics = {\n",
        "        \"process_name\": \"Feature Engineering and Call-Put Pairing\",\n",
        "        \"initial_row_count\": initial_row_count,\n",
        "        \"final_row_count\": final_row_count,\n",
        "        \"rows_excluded_unpaired\": rows_excluded,\n",
        "        \"percentage_rows_excluded\": round(percentage_excluded, 4),\n",
        "        \"initial_unique_strikes\": initial_unique_strikes,\n",
        "        \"final_unique_strikes\": final_unique_strikes,\n",
        "        \"unique_strikes_excluded\": strikes_excluded\n",
        "    }\n",
        "\n",
        "    # Return the final, analysis-ready DataFrame and the diagnostic report.\n",
        "    return df_final_pairs, diagnostics\n"
      ],
      "metadata": {
        "id": "qNzazdtzcGIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: RANSAC Data Preparation\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 7: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def prepare_ransac_inputs(\n",
        "    df_paired: pd.DataFrame,\n",
        "    min_samples_per_group: int = 5\n",
        ") -> Tuple[Dict[Tuple, Dict[str, np.ndarray]], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Prepares and validates the input data for the RANSAC algorithm.\n",
        "\n",
        "    This function transforms the analysis-ready DataFrame of paired options into\n",
        "    the specific numerical inputs required for the RANSAC procedure. It performs\n",
        "    three main steps:\n",
        "    1.  Pivots the data to align call and put prices for each strike.\n",
        "    2.  Constructs the X and Y vectors for the RANSAC regression based on the\n",
        "        parity relationship defined in the reference paper (Equation 4).\n",
        "    3.  Filters out estimation groups (timestamp-expiry pairs) that do not have\n",
        "        a sufficient number of samples (strikes) and packages the valid groups\n",
        "        into a dictionary of NumPy arrays.\n",
        "\n",
        "    Args:\n",
        "        df_paired (pd.DataFrame):\n",
        "            The input DataFrame, assumed to contain clean, paired call-put data\n",
        "            with 'best_bid_price' and 'best_ask_price' columns.\n",
        "        min_samples_per_group (int, optional):\n",
        "            The minimum number of valid call-put pairs (strikes) required for\n",
        "            an estimation group to be considered for RANSAC. Defaults to 5.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[Tuple, Dict[str, np.ndarray]], Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - Dict[Tuple, Dict[str, np.ndarray]]: The primary output. A dictionary\n",
        "              where each key is a tuple `(timestamp, asset, expiry_date)`\n",
        "              representing an estimation group, and each value is another\n",
        "              dictionary containing the NumPy arrays 'X', 'Y', and 'strikes'\n",
        "              for that group.\n",
        "            - Dict[str, Any]: A diagnostic dictionary detailing the preparation\n",
        "              and filtering process.\n",
        "\n",
        "    Raises:\n",
        "        TypeError:\n",
        "            If `df_paired` is not a pandas DataFrame or `min_samples_per_group`\n",
        "            is not an integer.\n",
        "        ValueError:\n",
        "            If `min_samples_per_group` is not a positive integer.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify the type of the main DataFrame input.\n",
        "    if not isinstance(df_paired, pd.DataFrame):\n",
        "        raise TypeError(\"Input `df_paired` must be a pandas DataFrame.\")\n",
        "    # Verify the type and value of the min_samples parameter.\n",
        "    if not isinstance(min_samples_per_group, int):\n",
        "        raise TypeError(\"Input `min_samples_per_group` must be an integer.\")\n",
        "    if min_samples_per_group <= 0:\n",
        "        raise ValueError(\"Input `min_samples_per_group` must be a positive integer.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Record the initial number of unique estimation groups.\n",
        "    grouping_levels = ['timestamp', 'asset', 'expiry_date']\n",
        "    initial_group_count = df_paired.groupby(grouping_levels).ngroups\n",
        "\n",
        "    # --- Step 1: Call-Put Pair Extraction via Pivoting ---\n",
        "    # Reset the index to make 'strike_price' and 'option_type' regular columns.\n",
        "    df_long = df_paired.reset_index()\n",
        "\n",
        "    # Pivot the table to place call and put prices side-by-side for each strike.\n",
        "    # This is the most efficient structure for the subsequent vector calculations.\n",
        "    df_pivoted = df_long.pivot_table(\n",
        "        index=['timestamp', 'asset', 'expiry_date', 'strike_price'],\n",
        "        columns='option_type',\n",
        "        values=['best_bid_price', 'best_ask_price']\n",
        "    )\n",
        "\n",
        "    # --- Step 2: RANSAC Input Array Construction ---\n",
        "    # Construct the X and Y vectors for the RANSAC regression.\n",
        "    # These equations are derived from the parity relationship in Equation (4).\n",
        "    # Equation Ref: X_i = C_{i,t}^{ask} - P_{i,t}^{bid}\n",
        "    X = (\n",
        "        df_pivoted[('best_ask_price', 'call')] -\n",
        "        df_pivoted[('best_bid_price', 'put')]\n",
        "    )\n",
        "    # Equation Ref: Y_i = P_{i,t}^{ask} - C_{i,t}^{bid}\n",
        "    Y = (\n",
        "        df_pivoted[('best_ask_price', 'put')] -\n",
        "        df_pivoted[('best_bid_price', 'call')]\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Minimum Sample Size Validation and Final Structuring ---\n",
        "    # Count the number of samples (strikes) in each estimation group.\n",
        "    group_sizes = df_pivoted.groupby(grouping_levels).transform('size')\n",
        "\n",
        "    # Filter the pivoted DataFrame to keep only groups with enough samples.\n",
        "    df_valid_groups = df_pivoted[group_sizes >= min_samples_per_group]\n",
        "\n",
        "    # Initialize the dictionary to hold the final NumPy array inputs.\n",
        "    ransac_inputs: Dict[Tuple, Dict[str, np.ndarray]] = {}\n",
        "\n",
        "    # Iterate over the valid groups to package the data.\n",
        "    for group_name, group_df in df_valid_groups.groupby(grouping_levels):\n",
        "        # Extract the strike prices from the group's index.\n",
        "        strikes = group_df.index.get_level_values('strike_price').to_numpy()\n",
        "\n",
        "        # Extract the corresponding X and Y values for the group.\n",
        "        # We use the group's index to select from the original X and Y Series.\n",
        "        x_values = X.loc[group_df.index].to_numpy()\n",
        "        y_values = Y.loc[group_df.index].to_numpy()\n",
        "\n",
        "        # Store the prepared NumPy arrays in the output dictionary.\n",
        "        ransac_inputs[group_name] = {\n",
        "            'X': x_values,\n",
        "            'Y': y_values,\n",
        "            'strikes': strikes\n",
        "        }\n",
        "\n",
        "    # --- Diagnostic Reporting ---\n",
        "    # Record the final number of groups that are valid for RANSAC.\n",
        "    final_group_count = len(ransac_inputs)\n",
        "\n",
        "    # Calculate the number and percentage of groups excluded.\n",
        "    groups_excluded = initial_group_count - final_group_count\n",
        "    percentage_excluded = (\n",
        "        (groups_excluded / initial_group_count * 100)\n",
        "        if initial_group_count > 0\n",
        "        else 0.0\n",
        "    )\n",
        "\n",
        "    # Assemble the diagnostic report.\n",
        "    diagnostics = {\n",
        "        \"process_name\": \"RANSAC Input Preparation\",\n",
        "        \"min_samples_per_group\": min_samples_per_group,\n",
        "        \"initial_group_count\": initial_group_count,\n",
        "        \"final_group_count\": final_group_count,\n",
        "        \"groups_excluded_insufficient_samples\": groups_excluded,\n",
        "        \"percentage_groups_excluded\": round(percentage_excluded, 4)\n",
        "    }\n",
        "\n",
        "    # Return the prepared inputs and the diagnostic report.\n",
        "    return ransac_inputs, diagnostics\n"
      ],
      "metadata": {
        "id": "mXQ7ssRKclsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: RANSAC Algorithm Execution\n",
        "\n",
        "# ======================================================================================\n",
        "# Custom Data Types\n",
        "# ======================================================================================\n",
        "\n",
        "# A structured type hint for the output of a single RANSAC execution.\n",
        "RansacResult = Dict[str, Any]\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 8, Steps 1-3: Core RANSAC Helper Function\n",
        "# ======================================================================================\n",
        "\n",
        "def _execute_ransac_single_group(\n",
        "    X: np.ndarray,\n",
        "    Y: np.ndarray,\n",
        "    residual_sq_threshold: float,\n",
        "    max_iterations: int,\n",
        "    min_samples: int,\n",
        "    seed: Optional[int] = None\n",
        ") -> Optional[RansacResult]:\n",
        "    \"\"\"\n",
        "    Executes the RANSAC algorithm for a single group of data points.\n",
        "\n",
        "    This function implements the core RANSAC logic from first principles to\n",
        "    find the largest subset of data (inliers) that fits a linear model\n",
        "    Y = intercept + slope * X.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray):\n",
        "            The independent variable data (1D array).\n",
        "        Y (np.ndarray):\n",
        "            The dependent variable data (1D array).\n",
        "        residual_sq_threshold (float):\n",
        "            The squared residual threshold. A point is considered an inlier if its\n",
        "            squared distance to the model is less than this value.\n",
        "        max_iterations (int):\n",
        "            The maximum number of iterations to perform.\n",
        "        min_samples (int):\n",
        "            The minimum number of data points required to fit the model. For a\n",
        "            line, this is 2.\n",
        "        seed (Optional[int]):\n",
        "            An optional seed for the random number generator to ensure\n",
        "            reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Optional[RansacResult]:\n",
        "            A dictionary containing the results ('slope', 'intercept',\n",
        "            'inlier_mask', 'inlier_count') if a valid model with a consensus\n",
        "            set is found. Returns None if no consensus set larger than the\n",
        "            initial sample size can be established.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Initialize a random number generator for reproducible sampling.\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Get the total number of data points.\n",
        "    n_total_samples = X.shape[0]\n",
        "\n",
        "    # Initialize variables to track the best model found so far.\n",
        "    best_inlier_count = 0\n",
        "    best_inlier_mask = np.zeros(n_total_samples, dtype=bool)\n",
        "\n",
        "    # --- Core RANSAC Iteration Loop ---\n",
        "    for _ in range(max_iterations):\n",
        "        # Step 1: Randomly sample a minimal subset of the data.\n",
        "        sample_indices = rng.choice(\n",
        "            n_total_samples, size=min_samples, replace=False\n",
        "        )\n",
        "        X_sample, Y_sample = X[sample_indices], Y[sample_indices]\n",
        "\n",
        "        # Step 2: Fit a linear model to the minimal sample.\n",
        "        # Using np.linalg.lstsq is numerically more stable than direct inversion.\n",
        "        # We fit Y = A * p, where A = [X, 1] and p = [slope, intercept].\n",
        "        A_sample = np.vstack([X_sample, np.ones(min_samples)]).T\n",
        "\n",
        "        # Defensively check for degenerate samples (e.g., two points with the same X).\n",
        "        if np.linalg.matrix_rank(A_sample) < min_samples:\n",
        "            continue # Skip this iteration if the sample is degenerate.\n",
        "\n",
        "        try:\n",
        "            params, _, _, _ = np.linalg.lstsq(A_sample, Y_sample, rcond=None)\n",
        "            slope, intercept = params\n",
        "        except np.linalg.LinAlgError:\n",
        "            continue # Skip if the linear algebra solver fails.\n",
        "\n",
        "        # Step 3: Calculate squared residuals for all data points.\n",
        "        # Equation Ref: residual_i^2 = (Y_i - (intercept + slope * X_i))^2\n",
        "        Y_pred = intercept + slope * X\n",
        "        squared_residuals = (Y - Y_pred)**2\n",
        "\n",
        "        # Step 4: Identify inliers based on the residual threshold.\n",
        "        current_inlier_mask = squared_residuals < residual_sq_threshold\n",
        "        current_inlier_count = np.sum(current_inlier_mask)\n",
        "\n",
        "        # Step 5: Update the best model if a better consensus set is found.\n",
        "        if current_inlier_count > best_inlier_count:\n",
        "            best_inlier_count = current_inlier_count\n",
        "            best_inlier_mask = current_inlier_mask\n",
        "\n",
        "    # --- Final Model Fitting ---\n",
        "    # If a consensus set was found (more inliers than the initial sample size),\n",
        "    # refit the model using all identified inliers for a more robust estimate.\n",
        "    if best_inlier_count > min_samples:\n",
        "        X_inliers = X[best_inlier_mask]\n",
        "        Y_inliers = Y[best_inlier_mask]\n",
        "\n",
        "        A_inliers = np.vstack([X_inliers, np.ones(best_inlier_count)]).T\n",
        "\n",
        "        try:\n",
        "            final_params, _, _, _ = np.linalg.lstsq(A_inliers, Y_inliers, rcond=None)\n",
        "            final_slope, final_intercept = final_params\n",
        "        except np.linalg.LinAlgError:\n",
        "            return None # Final fit failed, return None.\n",
        "\n",
        "        # Package and return the final results.\n",
        "        return {\n",
        "            \"slope\": final_slope,\n",
        "            \"intercept\": final_intercept,\n",
        "            \"inlier_mask\": best_inlier_mask,\n",
        "            \"inlier_count\": best_inlier_count\n",
        "        }\n",
        "\n",
        "    # If no sufficient consensus set was ever found, return None.\n",
        "    return None\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 8: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def execute_ransac_on_all_groups(\n",
        "    ransac_inputs: Dict[Tuple, Dict[str, np.ndarray]],\n",
        "    hyperparameters: Dict[str, Any],\n",
        "    seed: Optional[int] = None\n",
        ") -> Tuple[Dict[Tuple, RansacResult], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of the RANSAC algorithm on all prepared groups.\n",
        "\n",
        "    This function iterates through the dictionary of prepared RANSAC inputs,\n",
        "    executes the core RANSAC algorithm for each group, and compiles the results.\n",
        "    It provides a diagnostic report on the success rate of the procedure.\n",
        "\n",
        "    Args:\n",
        "        ransac_inputs (Dict[Tuple, Dict[str, np.ndarray]]):\n",
        "            The dictionary of prepared inputs from `prepare_ransac_inputs`.\n",
        "            Keys are group identifiers, values are dicts of NumPy arrays.\n",
        "        hyperparameters (Dict[str, Any]):\n",
        "            A dictionary containing the necessary hyperparameters for RANSAC:\n",
        "            'ransac_residual_sq_threshold', 'ransac_num_iterations',\n",
        "            'ransac_min_sample_size'.\n",
        "        seed (Optional[int]):\n",
        "            An optional seed for the random number generator to ensure\n",
        "            reproducibility across all groups.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[Tuple, RansacResult], Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - Dict[Tuple, RansacResult]: A dictionary where keys are the group\n",
        "              identifiers and values are the structured RansacResult dictionaries\n",
        "              for successfully processed groups.\n",
        "            - Dict[str, Any]: A diagnostic dictionary summarizing the execution.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Initialize the dictionary to store results.\n",
        "    ransac_results: Dict[Tuple, RansacResult] = {}\n",
        "\n",
        "    # Initialize counters for the diagnostic report.\n",
        "    groups_processed = 0\n",
        "    groups_succeeded = 0\n",
        "\n",
        "    # Extract hyperparameters for clarity.\n",
        "    residual_sq_threshold = hyperparameters['ransac_residual_sq_threshold']\n",
        "    max_iterations = hyperparameters['ransac_num_iterations']\n",
        "    min_samples = hyperparameters['ransac_min_sample_size']\n",
        "\n",
        "    # --- Group Iteration ---\n",
        "    # Iterate through each estimation group and its prepared data.\n",
        "    for group_name, data_arrays in ransac_inputs.items():\n",
        "        groups_processed += 1\n",
        "\n",
        "        # Execute the core RANSAC algorithm for the current group.\n",
        "        result = _execute_ransac_single_group(\n",
        "            X=data_arrays['X'],\n",
        "            Y=data_arrays['Y'],\n",
        "            residual_sq_threshold=residual_sq_threshold,\n",
        "            max_iterations=max_iterations,\n",
        "            min_samples=min_samples,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        # If the algorithm returned a valid result, store it.\n",
        "        if result is not None:\n",
        "            ransac_results[group_name] = result\n",
        "            groups_succeeded += 1\n",
        "\n",
        "    # --- Diagnostic Reporting ---\n",
        "    # Calculate the success rate.\n",
        "    success_rate = (\n",
        "        (groups_succeeded / groups_processed * 100)\n",
        "        if groups_processed > 0\n",
        "        else 0.0\n",
        "    )\n",
        "\n",
        "    # Assemble the diagnostic report.\n",
        "    diagnostics = {\n",
        "        \"process_name\": \"RANSAC Algorithm Execution\",\n",
        "        \"groups_processed\": groups_processed,\n",
        "        \"groups_succeeded\": groups_succeeded,\n",
        "        \"groups_failed\": groups_processed - groups_succeeded,\n",
        "        \"success_rate_pct\": round(success_rate, 4)\n",
        "    }\n",
        "\n",
        "    # Return the compiled results and the diagnostic report.\n",
        "    return ransac_results, diagnostics\n"
      ],
      "metadata": {
        "id": "7TZ8IECkdKKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: RANSAC Validation and Filtering\n",
        "\n",
        "# ======================================================================================\n",
        "# Custom Data Types (re-defined for clarity in this standalone context)\n",
        "# ======================================================================================\n",
        "RansacResult = Dict[str, Any]\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 9, Steps 1-2: Core Validation Helper Function\n",
        "# ======================================================================================\n",
        "\n",
        "def _validate_ransac_result(\n",
        "    result: RansacResult,\n",
        "    slope_tolerance: float,\n",
        "    min_inliers: int\n",
        ") -> Tuple[bool, str]:\n",
        "    \"\"\"\n",
        "    Validates a single RANSAC result against specified criteria.\n",
        "\n",
        "    This function applies a series of checks based on economic theory and\n",
        "    statistical robustness to a single RansacResult object.\n",
        "\n",
        "    Args:\n",
        "        result (RansacResult):\n",
        "            The dictionary containing the output of a single RANSAC execution.\n",
        "        slope_tolerance (float):\n",
        "            The maximum allowable percentage deviation of the slope from -1.0.\n",
        "        min_inliers (int):\n",
        "            The minimum number of inliers required for the result to be valid.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[bool, str]:\n",
        "            A tuple containing:\n",
        "            - bool: True if the result is valid, False otherwise.\n",
        "            - str: A string indicating the status ('VALID') or the specific\n",
        "              reason for rejection.\n",
        "    \"\"\"\n",
        "    # --- Validation Check 1: Slope Condition ---\n",
        "    # The slope (ξ) should be theoretically close to -1.\n",
        "    # Equation Ref: |ξ̂ + 1| <= ransac_slope_tolerance_pct\n",
        "    slope = result['slope']\n",
        "    if not (abs(slope + 1.0) <= slope_tolerance):\n",
        "        return False, \"Invalid Slope\"\n",
        "\n",
        "    # --- Validation Check 2: Intercept Positivity ---\n",
        "    # The intercept (ζ) represents market friction and should be positive.\n",
        "    intercept = result['intercept']\n",
        "    if not (intercept > 0):\n",
        "        return False, \"Negative Intercept\"\n",
        "\n",
        "    # --- Validation Check 3: Minimum Inlier Count ---\n",
        "    # The model must be supported by a minimum number of data points.\n",
        "    inlier_count = result['inlier_count']\n",
        "    if not (inlier_count >= min_inliers):\n",
        "        return False, \"Insufficient Inliers\"\n",
        "\n",
        "    # If all checks pass, the result is considered valid.\n",
        "    return True, \"VALID\"\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 9, Step 3: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def validate_and_filter_ransac_results(\n",
        "    ransac_results: Dict[Tuple, RansacResult],\n",
        "    hyperparameters: Dict[str, Any],\n",
        "    min_inliers_for_validation: int = 5\n",
        ") -> Tuple[Dict[Tuple, RansacResult], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Filters RANSAC results based on economic and statistical criteria.\n",
        "\n",
        "    This function iterates through a dictionary of RANSAC results, applying a\n",
        "    series of validation checks to each. It filters out any results that fail\n",
        "    these checks and returns a new dictionary containing only the valid results,\n",
        "    along with a detailed diagnostic report summarizing the filtering process\n",
        "    and the reasons for rejection.\n",
        "\n",
        "    Args:\n",
        "        ransac_results (Dict[Tuple, RansacResult]):\n",
        "            A dictionary of RANSAC results, keyed by group identifier tuples.\n",
        "        hyperparameters (Dict[str, Any]):\n",
        "            A dictionary containing the necessary hyperparameters, specifically\n",
        "            'ransac_slope_tolerance_pct'.\n",
        "        min_inliers_for_validation (int, optional):\n",
        "            The minimum number of inliers a RANSAC result must have to be\n",
        "            considered valid. Defaults to 5.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[Tuple, RansacResult], Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - Dict[Tuple, RansacResult]: A new dictionary containing only the\n",
        "              RANSAC results that passed all validation checks.\n",
        "            - Dict[str, Any]: A diagnostic dictionary detailing the number of\n",
        "              results processed, passed, and rejected, including a breakdown\n",
        "              of rejection reasons.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify the type of the main results input.\n",
        "    if not isinstance(ransac_results, dict):\n",
        "        raise TypeError(\"Input `ransac_results` must be a dictionary.\")\n",
        "    # Verify the type of the hyperparameters input.\n",
        "    if not isinstance(hyperparameters, dict):\n",
        "        raise TypeError(\"Input `hyperparameters` must be a dictionary.\")\n",
        "    # Check for the required hyperparameter key.\n",
        "    if 'ransac_slope_tolerance_pct' not in hyperparameters:\n",
        "        raise ValueError(\"Missing 'ransac_slope_tolerance_pct' in hyperparameters.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Extract the slope tolerance for use in the validation helper.\n",
        "    slope_tolerance = hyperparameters['ransac_slope_tolerance_pct']\n",
        "\n",
        "    # Initialize a dictionary to store the filtered, valid results.\n",
        "    valid_results: Dict[Tuple, RansacResult] = {}\n",
        "\n",
        "    # Initialize a Counter to tally the reasons for rejection.\n",
        "    rejection_reasons = Counter()\n",
        "\n",
        "    # --- Iteration and Validation ---\n",
        "    # Iterate through each group's RANSAC result.\n",
        "    for group_name, result in ransac_results.items():\n",
        "        # Call the helper function to perform the validation checks.\n",
        "        is_valid, reason = _validate_ransac_result(\n",
        "            result=result,\n",
        "            slope_tolerance=slope_tolerance,\n",
        "            min_inliers=min_inliers_for_validation\n",
        "        )\n",
        "\n",
        "        # If the result is valid, add it to the output dictionary.\n",
        "        if is_valid:\n",
        "            valid_results[group_name] = result\n",
        "        # Otherwise, increment the counter for the specific rejection reason.\n",
        "        else:\n",
        "            rejection_reasons[reason] += 1\n",
        "\n",
        "    # --- Diagnostic Reporting ---\n",
        "    # Get the total number of results processed.\n",
        "    total_processed = len(ransac_results)\n",
        "    # Get the number of results that passed validation.\n",
        "    total_valid = len(valid_results)\n",
        "    # Calculate the total number of rejected results.\n",
        "    total_rejected = total_processed - total_valid\n",
        "\n",
        "    # Calculate the validation pass rate.\n",
        "    pass_rate = (total_valid / total_processed * 100) if total_processed > 0 else 0.0\n",
        "\n",
        "    # Assemble the final diagnostic report.\n",
        "    diagnostics = {\n",
        "        \"process_name\": \"RANSAC Result Validation and Filtering\",\n",
        "        \"total_results_processed\": total_processed,\n",
        "        \"results_passed_validation\": total_valid,\n",
        "        \"results_rejected\": total_rejected,\n",
        "        \"validation_pass_rate_pct\": round(pass_rate, 4),\n",
        "        \"rejection_reason_counts\": dict(rejection_reasons)\n",
        "    }\n",
        "\n",
        "    # Return the dictionary of valid results and the diagnostic report.\n",
        "    return valid_results, diagnostics\n"
      ],
      "metadata": {
        "id": "UGKUxaYSd0S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Feature Vector Construction\n",
        "\n",
        "# ======================================================================================\n",
        "# Custom Data Types (re-defined for clarity in this standalone context)\n",
        "# ======================================================================================\n",
        "RansacResult = Dict[str, Any]\n",
        "# The structured output for a single estimation group's prepared vectors.\n",
        "FeatureVectorGroup = Dict[str, Any]\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 10: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def construct_feature_vectors(\n",
        "    df_analysis_ready: pd.DataFrame,\n",
        "    valid_ransac_results: Dict[Tuple, RansacResult]\n",
        ") -> Tuple[Dict[Tuple, FeatureVectorGroup], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Constructs the final feature vectors for the least-squares estimation.\n",
        "\n",
        "    This function takes the analysis-ready DataFrame and the validated RANSAC\n",
        "    results to produce the precise inputs for the closed-form solution. For each\n",
        "    valid estimation group, it performs the following:\n",
        "    1.  Extracts the inlier options identified by the RANSAC procedure.\n",
        "    2.  Constructs the call-put difference vector 'y' using mid-prices.\n",
        "    3.  Constructs the moneyness vector 'm'.\n",
        "    4.  Integrates the corresponding futures and spot prices.\n",
        "    5.  Packages these components into a structured dictionary.\n",
        "\n",
        "    Args:\n",
        "        df_analysis_ready (pd.DataFrame):\n",
        "            The fully prepared DataFrame from Task 6, containing mid-price,\n",
        "            moneyness, and complete call-put pairs.\n",
        "        valid_ransac_results (Dict[Tuple, RansacResult]):\n",
        "            The dictionary of validated RANSAC results from Task 9, containing\n",
        "            the inlier masks for each successful group.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[Tuple, FeatureVectorGroup], Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - Dict[Tuple, FeatureVectorGroup]: A dictionary where keys are the\n",
        "              group identifiers and values are `FeatureVectorGroup` dictionaries\n",
        "              containing the final NumPy arrays and scalar values for estimation.\n",
        "            - Dict[str, Any]: A diagnostic dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df_analysis_ready, pd.DataFrame):\n",
        "        raise TypeError(\"Input `df_analysis_ready` must be a pandas DataFrame.\")\n",
        "    if not isinstance(valid_ransac_results, dict):\n",
        "        raise TypeError(\"Input `valid_ransac_results` must be a dictionary.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Initialize the dictionary to hold the final feature vectors.\n",
        "    feature_vectors: Dict[Tuple, FeatureVectorGroup] = {}\n",
        "\n",
        "    # --- Group Iteration and Vector Construction ---\n",
        "    # Iterate through each group that passed RANSAC validation.\n",
        "    for group_name, ransac_data in valid_ransac_results.items():\n",
        "        try:\n",
        "            # --- Step 1: Inlier Data Extraction ---\n",
        "            # Select all data for the current estimation group.\n",
        "            group_df = df_analysis_ready.loc[group_name]\n",
        "\n",
        "            # Retrieve the boolean mask identifying the inliers for this group.\n",
        "            inlier_mask = ransac_data['inlier_mask']\n",
        "\n",
        "            # Apply the mask to get the DataFrame of inliers only.\n",
        "            # Note: The mask is a NumPy array, which works correctly with .iloc\n",
        "            # because the order is preserved from the initial .groupby().\n",
        "            inlier_df = group_df.iloc[inlier_mask]\n",
        "\n",
        "            # --- Step 2: Call-Put Difference and Moneyness Vector Construction ---\n",
        "            # Pivot the inlier data to align call and put information.\n",
        "            inlier_pivoted = inlier_df.reset_index().pivot_table(\n",
        "                index='strike_price',\n",
        "                columns='option_type',\n",
        "                values=['mid_price', 'moneyness']\n",
        "            )\n",
        "\n",
        "            # Construct the 'y' vector from the difference in mid-prices.\n",
        "            # Equation Ref: y_t^i = C_t(K^i,T) - P_t(K^i,T)\n",
        "            y_vector = (\n",
        "                inlier_pivoted[('mid_price', 'call')] -\n",
        "                inlier_pivoted[('mid_price', 'put')]\n",
        "            ).to_numpy()\n",
        "\n",
        "            # Construct the 'm' vector from the moneyness values.\n",
        "            # Moneyness is identical for the call and put of a given strike.\n",
        "            # Equation Ref: m_t^i = K^i / S_t\n",
        "            m_vector = inlier_pivoted[('moneyness', 'call')].to_numpy()\n",
        "\n",
        "            # --- Step 3: Futures and Spot Price Integration ---\n",
        "            # Extract the futures and spot prices. They are constant for the group.\n",
        "            # We can safely take the value from the first inlier row.\n",
        "            F_value = inlier_df['futures_price'].iloc[0]\n",
        "            S_value = inlier_df['spot_price'].iloc[0]\n",
        "\n",
        "            # The number of contracts is the number of inlier pairs.\n",
        "            n_contracts = len(inlier_pivoted)\n",
        "\n",
        "            # --- Final Packaging ---\n",
        "            # Store the constructed vectors and scalars in the output dictionary.\n",
        "            feature_vectors[group_name] = {\n",
        "                'y_vector': y_vector,\n",
        "                'm_vector': m_vector,\n",
        "                'F_value': F_value,\n",
        "                'S_value': S_value,\n",
        "                'n_contracts': n_contracts\n",
        "            }\n",
        "\n",
        "        except (KeyError, IndexError) as e:\n",
        "            # This block catches potential alignment errors, which would indicate\n",
        "            # a flaw in the upstream data processing pipeline.\n",
        "            print(f\"Warning: Could not process group {group_name}. Error: {e}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "    # --- Diagnostic Reporting ---\n",
        "    diagnostics = {\n",
        "        \"process_name\": \"Feature Vector Construction\",\n",
        "        \"num_input_groups\": len(valid_ransac_results),\n",
        "        \"num_output_groups\": len(feature_vectors),\n",
        "        \"num_failed_groups\": len(valid_ransac_results) - len(feature_vectors)\n",
        "    }\n",
        "\n",
        "    # Return the dictionary of feature vectors and the diagnostic report.\n",
        "    return feature_vectors, diagnostics\n"
      ],
      "metadata": {
        "id": "P2Tu-t1gebCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Summation Computations\n",
        "\n",
        "# ======================================================================================\n",
        "# Custom Data Types (re-defined for clarity in this standalone context)\n",
        "# ======================================================================================\n",
        "FeatureVectorGroup = Dict[str, Any]\n",
        "# The structured output for a single group's computed summations.\n",
        "SummationGroup = Dict[str, float]\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 11, Steps 1-3: Core Computation and Validation Helper\n",
        "# ======================================================================================\n",
        "\n",
        "def _calculate_summations(\n",
        "    feature_vectors: FeatureVectorGroup,\n",
        "    estimation_lambda_weight: float\n",
        ") -> SummationGroup:\n",
        "    \"\"\"\n",
        "    Calculates all scalar summations required for the closed-form solution.\n",
        "\n",
        "    This function takes the prepared feature vectors for a single estimation\n",
        "    group and computes the necessary scalar aggregates (S1, S2, S3, S4), the\n",
        "    lambda weight, and the futures-spot price ratio. It includes validation\n",
        "    to ensure all computed values are numerically sound.\n",
        "\n",
        "    Args:\n",
        "        feature_vectors (FeatureVectorGroup):\n",
        "            A dictionary containing the NumPy arrays 'y_vector', 'm_vector' and\n",
        "            scalar values 'F_value', 'S_value', 'n_contracts'.\n",
        "        estimation_lambda_weight (float):\n",
        "            The hyperparameter that scales the regularization term.\n",
        "\n",
        "    Returns:\n",
        "        SummationGroup:\n",
        "            A dictionary containing all the computed scalar values required for\n",
        "            the final estimation in Task 12.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If any computed value is not finite.\n",
        "    \"\"\"\n",
        "    # --- Extract Inputs ---\n",
        "    # Unpack the necessary arrays and scalars from the input dictionary.\n",
        "    y_vec = feature_vectors['y_vector']\n",
        "    m_vec = feature_vectors['m_vector']\n",
        "    n_contracts = feature_vectors['n_contracts']\n",
        "    F_value = feature_vectors['F_value']\n",
        "    S_value = feature_vectors['S_value']\n",
        "\n",
        "    # --- Step 1: Basic Summation Implementation ---\n",
        "    # These calculations directly implement the summation terms required for the\n",
        "    # analytical solution of the weighted least-squares problem.\n",
        "\n",
        "    # Equation Ref: S_1 = Σ y_t^i\n",
        "    S1 = np.sum(y_vec)\n",
        "\n",
        "    # Equation Ref: S_2 = Σ m_t^i\n",
        "    S2 = np.sum(m_vec)\n",
        "\n",
        "    # Equation Ref: S_3 = Σ (m_t^i)^2\n",
        "    S3 = np.sum(np.square(m_vec))\n",
        "\n",
        "    # Equation Ref: S_4 = Σ m_t^i * y_t^i\n",
        "    S4 = np.dot(m_vec, y_vec)\n",
        "\n",
        "    # --- Step 2: Lambda Weight and Auxiliary Computation ---\n",
        "    # Defensive assertion to prevent division by zero.\n",
        "    assert S_value > 0, \"Spot price must be positive.\"\n",
        "\n",
        "    # Calculate the lambda weighting term for the regularization component.\n",
        "    # Equation Ref: λ_{n_t^T} in the paper, scaled by n_contracts.\n",
        "    lambda_value = estimation_lambda_weight * n_contracts\n",
        "\n",
        "    # Calculate the futures-to-spot price ratio.\n",
        "    # Equation Ref: F_t / S_t\n",
        "    F_over_S = F_value / S_value\n",
        "\n",
        "    # --- Assemble and Validate Output ---\n",
        "    # Package all computed scalars into a result dictionary.\n",
        "    summations = {\n",
        "        'S1': S1, 'S2': S2, 'S3': S3, 'S4': S4,\n",
        "        'lambda_value': lambda_value,\n",
        "        'F_over_S': F_over_S,\n",
        "        'n_contracts': float(n_contracts) # Cast to float for consistency\n",
        "    }\n",
        "\n",
        "    # --- Step 3: Computation Validation and Quality Control ---\n",
        "    # Ensure all computed values are finite (not NaN or infinity). This is a\n",
        "    # critical sanity check against numerical issues or invalid inputs.\n",
        "    for key, value in summations.items():\n",
        "        assert np.isfinite(value), f\"Computed summation '{key}' is not finite: {value}\"\n",
        "\n",
        "    return summations\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 11: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def compute_summations_for_all_groups(\n",
        "    feature_vectors: Dict[Tuple, FeatureVectorGroup],\n",
        "    hyperparameters: Dict[str, Any]\n",
        ") -> Tuple[Dict[Tuple, SummationGroup], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of summations for all valid estimation groups.\n",
        "\n",
        "    This function iterates through the dictionary of prepared feature vectors,\n",
        "    calculates the required scalar summations for each group, and compiles the\n",
        "    results into a new dictionary, ready for the final solution step.\n",
        "\n",
        "    Args:\n",
        "        feature_vectors (Dict[Tuple, FeatureVectorGroup]):\n",
        "            The dictionary of prepared inputs from `construct_feature_vectors`.\n",
        "        hyperparameters (Dict[str, Any]):\n",
        "            A dictionary containing the 'estimation_lambda_weight' hyperparameter.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[Tuple, SummationGroup], Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - Dict[Tuple, SummationGroup]: A dictionary where keys are the group\n",
        "              identifiers and values are `SummationGroup` dictionaries containing\n",
        "              the scalar inputs for the closed-form solution.\n",
        "            - Dict[str, Any]: A diagnostic dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(feature_vectors, dict):\n",
        "        raise TypeError(\"Input `feature_vectors` must be a dictionary.\")\n",
        "    if not isinstance(hyperparameters, dict):\n",
        "        raise TypeError(\"Input `hyperparameters` must be a dictionary.\")\n",
        "    if 'estimation_lambda_weight' not in hyperparameters:\n",
        "        raise ValueError(\"Missing 'estimation_lambda_weight' in hyperparameters.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Extract the lambda weight hyperparameter.\n",
        "    lambda_weight = hyperparameters['estimation_lambda_weight']\n",
        "\n",
        "    # Initialize the dictionary to store the results.\n",
        "    all_summations: Dict[Tuple, SummationGroup] = {}\n",
        "\n",
        "    # Initialize counters for diagnostics.\n",
        "    groups_processed = 0\n",
        "    groups_failed = 0\n",
        "\n",
        "    # --- Group Iteration and Computation ---\n",
        "    # Iterate through each group and its feature vectors.\n",
        "    for group_name, fv_group in feature_vectors.items():\n",
        "        groups_processed += 1\n",
        "        try:\n",
        "            # Call the helper function to compute the summations for the group.\n",
        "            summation_group = _calculate_summations(\n",
        "                feature_vectors=fv_group,\n",
        "                estimation_lambda_weight=lambda_weight\n",
        "            )\n",
        "            # Store the successful result.\n",
        "            all_summations[group_name] = summation_group\n",
        "        except AssertionError as e:\n",
        "            # Catch validation errors from the helper and log them.\n",
        "            print(f\"Warning: Summation calculation failed for group {group_name}. Error: {e}. Skipping.\")\n",
        "            groups_failed += 1\n",
        "            continue\n",
        "\n",
        "    # --- Diagnostic Reporting ---\n",
        "    diagnostics = {\n",
        "        \"process_name\": \"Summation Computation\",\n",
        "        \"num_input_groups\": len(feature_vectors),\n",
        "        \"num_groups_succeeded\": len(all_summations),\n",
        "        \"num_groups_failed\": groups_failed\n",
        "    }\n",
        "\n",
        "    # Return the dictionary of computed summations and the diagnostic report.\n",
        "    return all_summations, diagnostics\n"
      ],
      "metadata": {
        "id": "73Ve5U_je7Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Closed-Form Solution Computation\n",
        "\n",
        "# ======================================================================================\n",
        "# Custom Data Types (re-defined for clarity in this standalone context)\n",
        "# ======================================================================================\n",
        "SummationGroup = Dict[str, float]\n",
        "# The structured output for a single group's estimated bond prices.\n",
        "SolutionResult = Dict[str, float]\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 12, Steps 1-3: Core Solver and Validation Helper\n",
        "# ======================================================================================\n",
        "\n",
        "def _solve_closed_form(\n",
        "    summations: SummationGroup,\n",
        "    determinant_tolerance: float = 1e-12\n",
        ") -> Optional[SolutionResult]:\n",
        "    \"\"\"\n",
        "    Computes the closed-form solution for zero-coupon bond prices (α*, β*).\n",
        "\n",
        "    This function implements the analytical solution derived from the weighted\n",
        "    least-squares problem. It first calculates the system's determinant to\n",
        "    ensure solvability and then computes the bond prices, followed by a final\n",
        "    economic validation check. This revised version also passes through the\n",
        "    'n_contracts' value for the final audit trail.\n",
        "\n",
        "    Args:\n",
        "        summations (SummationGroup):\n",
        "            A dictionary containing all pre-computed scalar summations and\n",
        "            auxiliary values for a single estimation group.\n",
        "        determinant_tolerance (float):\n",
        "            The numerical tolerance below which the system's determinant is\n",
        "            considered zero, indicating an unsolvable (singular) system.\n",
        "\n",
        "    Returns:\n",
        "        Optional[SolutionResult]:\n",
        "            A dictionary containing the estimated bond prices 'alpha_star',\n",
        "            'beta_star', and the supporting 'n_contracts' if the solution is\n",
        "            valid. Returns None if the system is unsolvable or if the results\n",
        "            are economically invalid.\n",
        "    \"\"\"\n",
        "    # --- Unpack Inputs ---\n",
        "    # Extract all necessary scalar values from the input dictionary.\n",
        "    S1, S2, S3, S4 = summations['S1'], summations['S2'], summations['S3'], summations['S4']\n",
        "    lambda_val = summations['lambda_value']\n",
        "    F_over_S = summations['F_over_S']\n",
        "    n = summations['n_contracts']\n",
        "\n",
        "    # --- Step 1: Determinant Calculation and Solvability Verification ---\n",
        "    # These terms correspond to the elements of the 2x2 matrix of the linear system.\n",
        "    term_A = lambda_val + S3\n",
        "    term_B = lambda_val * F_over_S + S2\n",
        "    term_C = n + lambda_val * (F_over_S**2)\n",
        "\n",
        "    # Equation Ref: d = (λ + S3)(n + λ(F/S)²) - (λ(F/S) + S2)²\n",
        "    determinant = term_A * term_C - term_B**2\n",
        "\n",
        "    # Check if the system is singular or ill-conditioned.\n",
        "    if abs(determinant) < determinant_tolerance:\n",
        "        return None # System is unsolvable.\n",
        "\n",
        "    # --- Step 2: Zero-Coupon Bond Price Calculation ---\n",
        "    # This is the direct implementation of the closed-form solution.\n",
        "    # Equation Ref: α* = (1/d) * [(λ + S3)S1 - (λ(F/S) + S2)S4]\n",
        "    alpha_star = (1 / determinant) * (term_A * S1 - term_B * S4)\n",
        "\n",
        "    # Equation Ref: β* = (1/d) * [(λ(F/S) + S2)S1 - (n + λ(F/S)²)S4]\n",
        "    beta_star = (1 / determinant) * (term_B * S1 - term_C * S4)\n",
        "\n",
        "    # --- Step 3: Solution Validation and Quality Assessment ---\n",
        "    # A critical economic sanity check: zero-coupon bond prices must be positive.\n",
        "    if not (alpha_star > 0 and beta_star > 0):\n",
        "        return None # Result is economically nonsensical.\n",
        "\n",
        "    # If all checks pass, return the valid solution, now including n_contracts.\n",
        "    return {\n",
        "        \"alpha_star\": alpha_star,\n",
        "        \"beta_star\": beta_star,\n",
        "        \"n_contracts\": n\n",
        "    }\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 12: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def compute_closed_form_solutions(\n",
        "    all_summations: Dict[Tuple, SummationGroup]\n",
        ") -> Tuple[Dict[Tuple, SolutionResult], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of closed-form solutions for all valid groups.\n",
        "\n",
        "    This function iterates through the dictionary of pre-computed summations,\n",
        "    calls the core solver for each group, and compiles the valid solutions.\n",
        "    It provides a detailed diagnostic report on the success and failure rates\n",
        "    of the estimation process.\n",
        "\n",
        "    Args:\n",
        "        all_summations (Dict[Tuple, SummationGroup]):\n",
        "            The dictionary of summation groups from `compute_summations_for_all_groups`.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[Tuple, SolutionResult], Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - Dict[Tuple, SolutionResult]: A dictionary where keys are the group\n",
        "              identifiers and values are `SolutionResult` dictionaries containing\n",
        "              the valid estimated bond prices.\n",
        "            - Dict[str, Any]: A diagnostic dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(all_summations, dict):\n",
        "        raise TypeError(\"Input `all_summations` must be a dictionary.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Initialize the dictionary to store the final valid solutions.\n",
        "    all_solutions: Dict[Tuple, SolutionResult] = {}\n",
        "\n",
        "    # Initialize a Counter to tally the reasons for failure.\n",
        "    failure_reasons = Counter()\n",
        "\n",
        "    # --- Group Iteration and Computation ---\n",
        "    # Iterate through each group and its computed summations.\n",
        "    for group_name, summation_group in all_summations.items():\n",
        "        # Call the helper function to solve the system for the current group.\n",
        "        solution = _solve_closed_form(summations=summation_group)\n",
        "\n",
        "        # If the solver returned a valid solution, store it.\n",
        "        if solution is not None:\n",
        "            all_solutions[group_name] = solution\n",
        "        else:\n",
        "            # If the solver returned None, it's due to a singular system or an\n",
        "            # economically invalid result. For diagnostics, we can distinguish,\n",
        "            # but for now, we'll group them as \"Estimation Failure\".\n",
        "            # A more granular check could be added inside the helper if needed.\n",
        "            failure_reasons[\"Estimation Failure (Singular or Invalid Result)\"] += 1\n",
        "\n",
        "    # --- Diagnostic Reporting ---\n",
        "    total_processed = len(all_summations)\n",
        "    total_succeeded = len(all_solutions)\n",
        "    total_failed = total_processed - total_succeeded\n",
        "    success_rate = (total_succeeded / total_processed * 100) if total_processed > 0 else 0.0\n",
        "\n",
        "    diagnostics = {\n",
        "        \"process_name\": \"Closed-Form Solution Computation\",\n",
        "        \"num_input_groups\": total_processed,\n",
        "        \"num_solutions_succeeded\": total_succeeded,\n",
        "        \"num_solutions_failed\": total_failed,\n",
        "        \"success_rate_pct\": round(success_rate, 4),\n",
        "        \"failure_reason_counts\": dict(failure_reasons)\n",
        "    }\n",
        "\n",
        "    # Return the dictionary of valid solutions and the diagnostic report.\n",
        "    return all_solutions, diagnostics\n"
      ],
      "metadata": {
        "id": "64vFvY6MfdfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Logarithmic Interest Rate Conversion\n",
        "\n",
        "# ======================================================================================\n",
        "# Custom Data Types (re-defined for clarity in this standalone context)\n",
        "# ======================================================================================\n",
        "SolutionResult = Dict[str, float]\n",
        "# The structured output for a single group's converted interest rates.\n",
        "InterestRateResult = Dict[str, Any]\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 13, Steps 1-3: Core Conversion and Validation Helper\n",
        "# ======================================================================================\n",
        "\n",
        "def _convert_prices_to_rates(\n",
        "    solution: SolutionResult,\n",
        "    group_name: Tuple,\n",
        "    annualization_factor: float\n",
        ") -> Optional[InterestRateResult]:\n",
        "    \"\"\"\n",
        "    Converts zero-coupon bond prices to continuously compounded interest rates.\n",
        "\n",
        "    This function takes the estimated bond prices for a single group and\n",
        "    calculates the corresponding annualized interest rates. This revised version\n",
        "    also passes through the 'n_contracts' value from the solution dictionary.\n",
        "\n",
        "    Args:\n",
        "        solution (SolutionResult):\n",
        "            A dictionary containing the estimated bond prices 'alpha_star',\n",
        "            'beta_star', and the supporting 'n_contracts'.\n",
        "        group_name (Tuple):\n",
        "            The identifier tuple for the group, containing (timestamp, asset,\n",
        "            expiry_date).\n",
        "        annualization_factor (float):\n",
        "            The factor to convert days to years (e.g., 365.25).\n",
        "\n",
        "    Returns:\n",
        "        Optional[InterestRateResult]:\n",
        "            A dictionary containing the computed rates and metadata if the\n",
        "            conversion is successful. Returns None if any numerical error occurs.\n",
        "    \"\"\"\n",
        "    # --- Unpack Inputs ---\n",
        "    # Extract bond prices and the number of contracts from the solution.\n",
        "    alpha_star = solution['alpha_star']\n",
        "    beta_star = solution['beta_star']\n",
        "    n_contracts = solution['n_contracts']\n",
        "    # Extract temporal information from the group name.\n",
        "    timestamp, _, expiry_date = group_name\n",
        "\n",
        "    # --- Step 1: Time-to-Expiry Calculation ---\n",
        "    # Calculate the time to expiry in days.\n",
        "    time_to_expiry_days = (expiry_date - timestamp) / pd.Timedelta(days=1)\n",
        "\n",
        "    # Defensively check that time-to-expiry is positive.\n",
        "    if time_to_expiry_days <= 0:\n",
        "        return None\n",
        "\n",
        "    # Convert the duration to years using the specified annualization factor.\n",
        "    time_to_expiry_years = time_to_expiry_days / annualization_factor\n",
        "\n",
        "    # --- Step 2: Logarithmic Rate Computation ---\n",
        "    # Equation Ref: r_crypto = - (1 / (T-t)) * ln(α*)\n",
        "    crypto_rate = -np.log(alpha_star) / time_to_expiry_years\n",
        "\n",
        "    # Equation Ref: r_ref = - (1 / (T-t)) * ln(β*)\n",
        "    ref_rate = -np.log(beta_star) / time_to_expiry_years\n",
        "\n",
        "    # --- Step 3: Rate Validation ---\n",
        "    # A final sanity check to ensure the computed rates are finite numbers.\n",
        "    if not (np.isfinite(crypto_rate) and np.isfinite(ref_rate)):\n",
        "        return None\n",
        "\n",
        "    # --- Final Packaging ---\n",
        "    # Assemble the complete result, now including n_contracts.\n",
        "    return {\n",
        "        \"crypto_rate\": crypto_rate,\n",
        "        \"ref_rate\": ref_rate,\n",
        "        \"time_to_expiry_days\": time_to_expiry_days,\n",
        "        \"alpha_star\": alpha_star,\n",
        "        \"beta_star\": beta_star,\n",
        "        \"n_contracts\": n_contracts\n",
        "    }\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 13: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def convert_all_solutions_to_rates(\n",
        "    all_solutions: Dict[Tuple, SolutionResult],\n",
        "    hyperparameters: Dict[str, Any]\n",
        ") -> Tuple[Dict[Tuple, InterestRateResult], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the conversion of all bond price solutions to interest rates.\n",
        "\n",
        "    This function iterates through the dictionary of valid closed-form solutions,\n",
        "    converts each pair of bond prices into their corresponding continuously\n",
        "    compounded interest rates, and compiles the results.\n",
        "\n",
        "    Args:\n",
        "        all_solutions (Dict[Tuple, SolutionResult]):\n",
        "            The dictionary of valid solutions from `compute_closed_form_solutions`.\n",
        "        hyperparameters (Dict[str, Any]):\n",
        "            A dictionary containing the 'annualization_factor' hyperparameter.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[Tuple, InterestRateResult], Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - Dict[Tuple, InterestRateResult]: A dictionary where keys are group\n",
        "              identifiers and values are `InterestRateResult` dictionaries.\n",
        "            - Dict[str, Any]: A diagnostic dictionary summarizing the conversion.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(all_solutions, dict):\n",
        "        raise TypeError(\"Input `all_solutions` must be a dictionary.\")\n",
        "    if not isinstance(hyperparameters, dict):\n",
        "        raise TypeError(\"Input `hyperparameters` must be a dictionary.\")\n",
        "    if 'annualization_factor' not in hyperparameters:\n",
        "        raise ValueError(\"Missing 'annualization_factor' in hyperparameters.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Extract the annualization factor.\n",
        "    annualization_factor = hyperparameters['annualization_factor']\n",
        "\n",
        "    # Initialize the dictionary to store the final interest rate results.\n",
        "    all_rates: Dict[Tuple, InterestRateResult] = {}\n",
        "\n",
        "    # Initialize counters for diagnostics.\n",
        "    groups_processed = 0\n",
        "    groups_failed = 0\n",
        "\n",
        "    # --- Group Iteration and Conversion ---\n",
        "    # Iterate through each group and its solved bond prices.\n",
        "    for group_name, solution in all_solutions.items():\n",
        "        groups_processed += 1\n",
        "\n",
        "        # Call the helper function to perform the conversion and validation.\n",
        "        rate_result = _convert_prices_to_rates(\n",
        "            solution=solution,\n",
        "            group_name=group_name,\n",
        "            annualization_factor=annualization_factor\n",
        "        )\n",
        "\n",
        "        # If the conversion was successful, store the result.\n",
        "        if rate_result is not None:\n",
        "            all_rates[group_name] = rate_result\n",
        "        else:\n",
        "            groups_failed += 1\n",
        "\n",
        "    # --- Diagnostic Reporting ---\n",
        "    success_rate = (\n",
        "        ((groups_processed - groups_failed) / groups_processed * 100)\n",
        "        if groups_processed > 0\n",
        "        else 0.0\n",
        "    )\n",
        "\n",
        "    diagnostics = {\n",
        "        \"process_name\": \"Interest Rate Conversion\",\n",
        "        \"num_input_solutions\": groups_processed,\n",
        "        \"num_conversions_succeeded\": len(all_rates),\n",
        "        \"num_conversions_failed\": groups_failed,\n",
        "        \"success_rate_pct\": round(success_rate, 4)\n",
        "    }\n",
        "\n",
        "    # Return the dictionary of computed rates and the diagnostic report.\n",
        "    return all_rates, diagnostics\n"
      ],
      "metadata": {
        "id": "jHCHodB9gFCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Rate Validation and Quality Control\n",
        "\n",
        "# ======================================================================================\n",
        "# Custom Data Types (re-defined for clarity in this standalone context)\n",
        "# ======================================================================================\n",
        "InterestRateResult = Dict[str, Any]\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 14, Steps 1-3: Core Validation Helper Function\n",
        "# ======================================================================================\n",
        "\n",
        "def _validate_single_rate_result(\n",
        "    rate_result: InterestRateResult,\n",
        "    asset: str,\n",
        "    validation_bounds: Dict[str, Tuple[float, float]]\n",
        ") -> Tuple[bool, str]:\n",
        "    \"\"\"\n",
        "    Validates a single interest rate result against economic bounds.\n",
        "\n",
        "    This function applies a series of asset-specific and cross-validation checks\n",
        "    to ensure the computed interest rates are economically plausible.\n",
        "\n",
        "    Args:\n",
        "        rate_result (InterestRateResult):\n",
        "            The dictionary containing the computed rates for a single group.\n",
        "        asset (str):\n",
        "            The crypto asset for the group (e.g., 'BTC', 'ETH').\n",
        "        validation_bounds (Dict[str, Tuple[float, float]]):\n",
        "            A dictionary defining the plausible [min, max] ranges for each\n",
        "            asset and the reference currency, plus the max spread.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[bool, str]:\n",
        "            A tuple containing:\n",
        "            - bool: True if the result is valid, False otherwise.\n",
        "            - str: A string indicating 'VALID' or the reason for rejection.\n",
        "    \"\"\"\n",
        "    # --- Unpack Inputs ---\n",
        "    crypto_rate = rate_result['crypto_rate']\n",
        "    ref_rate = rate_result['ref_rate']\n",
        "\n",
        "    # --- Step 1: Crypto Rate Validation ---\n",
        "    # Retrieve the valid range for the specific crypto asset.\n",
        "    crypto_min, crypto_max = validation_bounds.get(asset, (-float('inf'), float('inf')))\n",
        "    # Check if the crypto rate is within its defined plausible range.\n",
        "    if not (crypto_min <= crypto_rate <= crypto_max):\n",
        "        return False, f\"Crypto Rate Out of Bounds ({asset})\"\n",
        "\n",
        "    # --- Step 2: Reference Rate Validation ---\n",
        "    # Retrieve the valid range for the reference currency (assumed USD).\n",
        "    ref_min, ref_max = validation_bounds.get('USD', (-float('inf'), float('inf')))\n",
        "    # Check if the reference rate is within its defined plausible range.\n",
        "    if not (ref_min <= ref_rate <= ref_max):\n",
        "        return False, \"Reference Rate Out of Bounds (USD)\"\n",
        "\n",
        "    # --- Step 3: Cross-Validation Checks ---\n",
        "    # Check if the spread between the two rates is excessively large.\n",
        "    max_spread = validation_bounds.get('MAX_SPREAD', float('inf'))\n",
        "    rate_spread = abs(crypto_rate - ref_rate)\n",
        "    if rate_spread > max_spread:\n",
        "        return False, \"Extreme Rate Spread\"\n",
        "\n",
        "    # If all checks pass, the result is considered valid.\n",
        "    return True, \"VALID\"\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 14: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def validate_and_filter_rates(\n",
        "    all_rates: Dict[Tuple, InterestRateResult]\n",
        ") -> Tuple[Dict[Tuple, InterestRateResult], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation and filtering of computed interest rates.\n",
        "\n",
        "    This function applies a final layer of quality control by filtering the\n",
        "    computed interest rates based on predefined, economically reasonable bounds.\n",
        "    It checks asset-specific rates, reference currency rates, and the spread\n",
        "    between them.\n",
        "\n",
        "    Args:\n",
        "        all_rates (Dict[Tuple, InterestRateResult]):\n",
        "            The dictionary of computed interest rates from the previous task.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[Tuple, InterestRateResult], Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - Dict[Tuple, InterestRateResult]: A new dictionary containing only\n",
        "              the interest rate results that passed all validation checks.\n",
        "            - Dict[str, Any]: A diagnostic dictionary summarizing the filtering\n",
        "              process and the reasons for rejection.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(all_rates, dict):\n",
        "        raise TypeError(\"Input `all_rates` must be a dictionary.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Define the validation schema with plausible economic bounds.\n",
        "    validation_bounds: Dict[str, Any] = {\n",
        "        'BTC': (-0.5, 0.5),   # Annual rate range for BTC\n",
        "        'ETH': (-0.5, 0.5),   # Annual rate range for ETH\n",
        "        'USD': (-0.2, 1.0),   # Annual rate range for the reference currency\n",
        "        'MAX_SPREAD': 2.0     # Maximum absolute spread between crypto and ref rates\n",
        "    }\n",
        "\n",
        "    # Initialize a dictionary to store the filtered, valid results.\n",
        "    valid_rates: Dict[Tuple, InterestRateResult] = {}\n",
        "\n",
        "    # Initialize a Counter to tally the reasons for rejection.\n",
        "    rejection_reasons = Counter()\n",
        "\n",
        "    # --- Iteration and Validation ---\n",
        "    # Iterate through each group's computed interest rate result.\n",
        "    for group_name, rate_result in all_rates.items():\n",
        "        # The asset is the second element in the group name tuple.\n",
        "        asset = group_name[1]\n",
        "\n",
        "        # Call the helper function to perform the validation checks.\n",
        "        is_valid, reason = _validate_single_rate_result(\n",
        "            rate_result=rate_result,\n",
        "            asset=asset,\n",
        "            validation_bounds=validation_bounds\n",
        "        )\n",
        "\n",
        "        # If the result is valid, add it to the output dictionary.\n",
        "        if is_valid:\n",
        "            valid_rates[group_name] = rate_result\n",
        "        # Otherwise, increment the counter for the specific rejection reason.\n",
        "        else:\n",
        "            rejection_reasons[reason] += 1\n",
        "\n",
        "    # --- Diagnostic Reporting ---\n",
        "    total_processed = len(all_rates)\n",
        "    total_valid = len(valid_rates)\n",
        "    total_rejected = total_processed - total_valid\n",
        "    pass_rate = (total_valid / total_processed * 100) if total_processed > 0 else 0.0\n",
        "\n",
        "    diagnostics = {\n",
        "        \"process_name\": \"Economic Validation of Interest Rates\",\n",
        "        \"validation_bounds\": validation_bounds,\n",
        "        \"total_rates_processed\": total_processed,\n",
        "        \"rates_passed_validation\": total_valid,\n",
        "        \"rates_rejected\": total_rejected,\n",
        "        \"validation_pass_rate_pct\": round(pass_rate, 4),\n",
        "        \"rejection_reason_counts\": dict(rejection_reasons)\n",
        "    }\n",
        "\n",
        "    # Return the dictionary of valid rates and the diagnostic report.\n",
        "    return valid_rates, diagnostics\n"
      ],
      "metadata": {
        "id": "IXlsVInCgzde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Results Aggregation and Storage\n",
        "\n",
        "# ======================================================================================\n",
        "# Custom Data Types (re-defined for clarity in this standalone context)\n",
        "# ======================================================================================\n",
        "InterestRateResult = Dict[str, Any]\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 15: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def aggregate_results_to_dataframe(\n",
        "    all_rates: Dict[Tuple, InterestRateResult]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates the final interest rate results into a structured DataFrame.\n",
        "\n",
        "    This function transforms the nested dictionary of estimation results into a\n",
        "    single, flat pandas DataFrame. This revised version is more complete, as it\n",
        "    now includes the 'n_contracts' column, which represents the number of\n",
        "    inlier option pairs that supported each individual estimate, providing a\n",
        "    crucial measure of reliability.\n",
        "\n",
        "    Args:\n",
        "        all_rates (Dict[Tuple, InterestRateResult]):\n",
        "            The dictionary of validated interest rate results. It is now expected\n",
        "            that each `InterestRateResult` dictionary contains the 'n_contracts' key.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A DataFrame where each row corresponds to a successful estimation\n",
        "            for a given timestamp and maturity. The DataFrame contains all key\n",
        "            information from the estimation process in a clean, tabular format.\n",
        "\n",
        "    Raises:\n",
        "        TypeError:\n",
        "            If `all_rates` is not a dictionary.\n",
        "        ValueError:\n",
        "            If the input dictionary is empty, or if the inner result dictionaries\n",
        "            are missing the required 'n_contracts' key.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check the main input type.\n",
        "    if not isinstance(all_rates, dict):\n",
        "        raise TypeError(\"Input `all_rates` must be a dictionary.\")\n",
        "    # Check if the input is empty.\n",
        "    if not all_rates:\n",
        "        raise ValueError(\"Input dictionary `all_rates` is empty. No results to aggregate.\")\n",
        "    # Check the structure of the first inner dictionary as a representative sample.\n",
        "    first_result = next(iter(all_rates.values()))\n",
        "    if 'n_contracts' not in first_result:\n",
        "        raise ValueError(\"Incomplete data propagation: 'n_contracts' key is missing from result dictionaries.\")\n",
        "\n",
        "    # --- Step 1: Results DataFrame Construction ---\n",
        "    # Initialize a list to hold the \"flattened\" records for the DataFrame.\n",
        "    records: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Iterate through the input dictionary to create a list of records.\n",
        "    for group_name, rate_result in all_rates.items():\n",
        "        # Unpack the group name tuple into its components.\n",
        "        timestamp, asset, expiry_date = group_name\n",
        "\n",
        "        # Create a new dictionary for the current record, combining the group\n",
        "        # identifier with the result data (which now includes n_contracts).\n",
        "        record = {\n",
        "            'timestamp': timestamp,\n",
        "            'asset': asset,\n",
        "            'expiry_date': expiry_date,\n",
        "            **rate_result\n",
        "        }\n",
        "        # Append the fully constructed record to the list.\n",
        "        records.append(record)\n",
        "\n",
        "    # Create the DataFrame from the list of records.\n",
        "    results_df = pd.DataFrame.from_records(records)\n",
        "\n",
        "    # --- Schema Enforcement and Finalization ---\n",
        "    # Define the exact column order for the final, more complete DataFrame.\n",
        "    final_column_order = [\n",
        "        'timestamp',\n",
        "        'asset',\n",
        "        'expiry_date',\n",
        "        'time_to_expiry_days',\n",
        "        'n_contracts', # Added for completeness\n",
        "        'crypto_rate',\n",
        "        'ref_rate',\n",
        "        'alpha_star',\n",
        "        'beta_star'\n",
        "    ]\n",
        "\n",
        "    # Reorder the columns to match the specified schema.\n",
        "    results_df = results_df[final_column_order]\n",
        "\n",
        "    # Explicitly cast data types for correctness and memory efficiency.\n",
        "    dtype_mapping = {\n",
        "        'timestamp': 'datetime64[ns]',\n",
        "        'asset': 'category',\n",
        "        'expiry_date': 'datetime64[ns]',\n",
        "        'time_to_expiry_days': 'float64',\n",
        "        'n_contracts': 'int32', # Use an efficient integer type\n",
        "        'crypto_rate': 'float64',\n",
        "        'ref_rate': 'float64',\n",
        "        'alpha_star': 'float64',\n",
        "        'beta_star': 'float64'\n",
        "    }\n",
        "    results_df = results_df.astype(dtype_mapping)\n",
        "\n",
        "    # Sort the final DataFrame for clean, predictable ordering.\n",
        "    results_df = results_df.sort_values(by=['timestamp', 'asset', 'expiry_date']).reset_index(drop=True)\n",
        "\n",
        "    # Return the final, polished, and more complete DataFrame.\n",
        "    return results_df\n",
        "\n"
      ],
      "metadata": {
        "id": "r-mNBB1ChgVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Daily Aggregation from Hourly Data\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 16: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def aggregate_rates_daily(\n",
        "    results_df: pd.DataFrame,\n",
        "    min_hourly_obs: int = 2\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Aggregates high-frequency interest rate estimates to a daily frequency.\n",
        "\n",
        "    This function transforms the granular, point-in-time interest rate estimates\n",
        "    into a daily panel dataset. It performs three main steps:\n",
        "    1.  Assigns each estimate to a calendar date.\n",
        "    2.  Buckets each estimate into a predefined maturity range.\n",
        "    3.  Groups the data by date, asset, and maturity bucket, then computes\n",
        "        statistical aggregates (mean, std, count) for the rates.\n",
        "    4.  Filters out any daily aggregates that are based on an insufficient\n",
        "        number of hourly observations.\n",
        "\n",
        "    Args:\n",
        "        results_df (pd.DataFrame):\n",
        "            The DataFrame of successful estimation results from Task 15.\n",
        "        min_hourly_obs (int, optional):\n",
        "            The minimum number of hourly observations required within a day for\n",
        "            an aggregated data point to be considered valid. Defaults to 2.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - pd.DataFrame: A new DataFrame containing the daily aggregated\n",
        "              yield curve data, indexed by date, asset, and maturity bucket.\n",
        "            - Dict[str, Any]: A diagnostic dictionary summarizing the aggregation.\n",
        "\n",
        "    Raises:\n",
        "        TypeError:\n",
        "            If `results_df` is not a pandas DataFrame or `min_hourly_obs` is not an int.\n",
        "        ValueError:\n",
        "            If `results_df` is empty or `min_hourly_obs` is not positive.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(results_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input `results_df` must be a pandas DataFrame.\")\n",
        "    if not isinstance(min_hourly_obs, int):\n",
        "        raise TypeError(\"Input `min_hourly_obs` must be an integer.\")\n",
        "    if results_df.empty:\n",
        "        raise ValueError(\"Input `results_df` is empty. Cannot perform aggregation.\")\n",
        "    if min_hourly_obs <= 0:\n",
        "        raise ValueError(\"Input `min_hourly_obs` must be a positive integer.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    df_agg = results_df.copy()\n",
        "    initial_point_count = len(df_agg)\n",
        "\n",
        "    # --- Step 1: Temporal Grouping and Maturity Bucketing ---\n",
        "    # Extract the calendar date from the timestamp for daily grouping.\n",
        "    df_agg['date'] = df_agg['timestamp'].dt.date\n",
        "\n",
        "    # Define the maturity buckets (in days).\n",
        "    maturity_bins = [30, 60, 120, 240, 400]\n",
        "    maturity_labels = ['30-60d', '60-120d', '120-240d', '240-400d']\n",
        "\n",
        "    # Use pd.cut to assign each observation to a maturity bucket.\n",
        "    df_agg['maturity_bucket'] = pd.cut(\n",
        "        df_agg['time_to_expiry_days'],\n",
        "        bins=maturity_bins,\n",
        "        labels=maturity_labels,\n",
        "        right=True, # Bins are (30, 60], (60, 120], etc.\n",
        "        include_lowest=True # Ensures the lowest bound (30) is included.\n",
        "    )\n",
        "\n",
        "    # Drop rows that fall outside the defined maturity buckets.\n",
        "    df_agg.dropna(subset=['maturity_bucket'], inplace=True)\n",
        "    rows_after_bucketing = len(df_agg)\n",
        "\n",
        "    # --- Step 2: Statistical Aggregation and Quality Control ---\n",
        "    # Define the aggregation operations to be performed on each group.\n",
        "    agg_functions = {\n",
        "        'crypto_rate': ['mean', 'std'],\n",
        "        'ref_rate': ['mean', 'std'],\n",
        "        'timestamp': 'count' # Use a non-null column to count observations\n",
        "    }\n",
        "\n",
        "    # Group by date, asset, and bucket, then apply the aggregations.\n",
        "    daily_aggregated = df_agg.groupby(\n",
        "        ['date', 'asset', 'maturity_bucket']\n",
        "    ).agg(agg_functions)\n",
        "\n",
        "    # Clean up the hierarchical column names created by .agg().\n",
        "    daily_aggregated.columns = [\n",
        "        '_'.join(col).strip() for col in daily_aggregated.columns.values\n",
        "    ]\n",
        "    daily_aggregated.rename(columns={'timestamp_count': 'n_observations'}, inplace=True)\n",
        "\n",
        "    # Filter out aggregated points that are based on too few observations.\n",
        "    initial_agg_points = len(daily_aggregated)\n",
        "    daily_aggregated_filtered = daily_aggregated[\n",
        "        daily_aggregated['n_observations'] >= min_hourly_obs\n",
        "    ]\n",
        "    final_agg_points = len(daily_aggregated_filtered)\n",
        "\n",
        "    # --- Diagnostic Reporting ---\n",
        "    diagnostics = {\n",
        "        \"process_name\": \"Daily Aggregation of Rates\",\n",
        "        \"min_hourly_obs_per_day\": min_hourly_obs,\n",
        "        \"initial_point_count\": initial_point_count,\n",
        "        \"points_after_bucketing\": rows_after_bucketing,\n",
        "        \"initial_aggregated_points\": initial_agg_points,\n",
        "        \"final_aggregated_points\": final_agg_points,\n",
        "        \"points_rejected_insufficient_obs\": initial_agg_points - final_agg_points\n",
        "    }\n",
        "\n",
        "    # Return the final aggregated DataFrame and the diagnostic report.\n",
        "    return daily_aggregated_filtered, diagnostics\n"
      ],
      "metadata": {
        "id": "1ZZv1oy-iD1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Maturity Interpolation to Target Tenors\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 17, Steps 1-3: Core Interpolation Helper Function\n",
        "# ======================================================================================\n",
        "\n",
        "def _interpolate_single_curve(\n",
        "    daily_slice: pd.DataFrame,\n",
        "    target_tenors: List[int]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs linear interpolation on a single day's yield curve data.\n",
        "\n",
        "    This function takes the aggregated rate data for a single date and asset,\n",
        "    and interpolates/extrapolates to find the rates at the specified target tenors.\n",
        "\n",
        "    Args:\n",
        "        daily_slice (pd.DataFrame):\n",
        "            A DataFrame slice containing the aggregated rates for one date/asset.\n",
        "            Must contain 'maturity_midpoint', 'crypto_rate_mean', 'ref_rate_mean'.\n",
        "        target_tenors (List[int]):\n",
        "            A list of integer tenors (in days) to interpolate to.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame:\n",
        "            A DataFrame with an index of the target tenors and columns for the\n",
        "            interpolated crypto and reference rates, along with a status flag.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Feasibility Analysis ---\n",
        "    # Drop any potential duplicates in maturity points and sort by maturity.\n",
        "    curve_data = daily_slice.drop_duplicates(\n",
        "        subset=['maturity_midpoint']\n",
        "    ).sort_values('maturity_midpoint')\n",
        "\n",
        "    # At least two points are required for linear interpolation.\n",
        "    if len(curve_data) < 2:\n",
        "        return pd.DataFrame() # Return empty DataFrame if not feasible.\n",
        "\n",
        "    # Extract the known x (maturities) and y (rates) values.\n",
        "    x_known = curve_data['maturity_midpoint'].to_numpy()\n",
        "    y_known_crypto = curve_data['crypto_rate_mean'].to_numpy()\n",
        "    y_known_ref = curve_data['ref_rate_mean'].to_numpy()\n",
        "\n",
        "    # --- Step 2: Linear Interpolation ---\n",
        "    # Use numpy.interp for efficient and robust 1D linear interpolation.\n",
        "    # This single function handles both interpolation and flat extrapolation.\n",
        "    # Equation Ref: r(τ_target) = r(τ₁) + [ (r(τ₂) - r(τ₁))/(τ₂ - τ₁) ] * (τ_target - τ₁)\n",
        "    interpolated_crypto_rates = np.interp(target_tenors, x_known, y_known_crypto)\n",
        "    interpolated_ref_rates = np.interp(target_tenors, x_known, y_known_ref)\n",
        "\n",
        "    # --- Step 3: Extrapolation Handling and Quality Assessment ---\n",
        "    # Determine the status (Interpolated vs. Extrapolated) for each target tenor.\n",
        "    min_known_maturity = x_known[0]\n",
        "    max_known_maturity = x_known[-1]\n",
        "\n",
        "    status = [\n",
        "        'INTERPOLATED' if min_known_maturity <= tenor <= max_known_maturity else 'EXTRAPOLATED'\n",
        "        for tenor in target_tenors\n",
        "    ]\n",
        "\n",
        "    # --- Final Packaging ---\n",
        "    # Create a results DataFrame for this single curve.\n",
        "    result_df = pd.DataFrame({\n",
        "        'tenor_days': target_tenors,\n",
        "        'crypto_rate': interpolated_crypto_rates,\n",
        "        'ref_rate': interpolated_ref_rates,\n",
        "        'status': status\n",
        "    })\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 17: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def interpolate_yield_curves(\n",
        "    daily_aggregated_df: pd.DataFrame,\n",
        "    target_tenors_days: List[int]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Interpolates daily yield curves to a set of standardized target tenors.\n",
        "\n",
        "    This function transforms the daily aggregated data from maturity buckets\n",
        "    into a time series of rates at fixed maturity points (e.g., 90, 180, 360 days).\n",
        "    It uses linear interpolation where possible and flat extrapolation for points\n",
        "    outside the available maturity range, flagging each point accordingly.\n",
        "\n",
        "    Args:\n",
        "        daily_aggregated_df (pd.DataFrame):\n",
        "            The DataFrame of daily aggregated rates from Task 16.\n",
        "        target_tenors_days (List[int]):\n",
        "            A list of integer tenors (in days) to which the curves will be\n",
        "            standardized.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - pd.DataFrame: A new DataFrame indexed by date and asset, with\n",
        "              columns for each target tenor's rates and status.\n",
        "            - Dict[str, Any]: A diagnostic dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(daily_aggregated_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input `daily_aggregated_df` must be a pandas DataFrame.\")\n",
        "    if not isinstance(target_tenors_days, list) or not all(isinstance(i, int) for i in target_tenors_days):\n",
        "        raise TypeError(\"Input `target_tenors_days` must be a list of integers.\")\n",
        "    if daily_aggregated_df.empty:\n",
        "        raise ValueError(\"Input `daily_aggregated_df` is empty.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    df_interp = daily_aggregated_df.reset_index().copy()\n",
        "\n",
        "    # --- Step 1 (Prep): Calculate Maturity Midpoints ---\n",
        "    # Define the midpoints for each maturity bucket.\n",
        "    bucket_midpoints = {\n",
        "        '30-60d': (30 + 60) / 2,\n",
        "        '60-120d': (60 + 120) / 2,\n",
        "        '120-240d': (120 + 240) / 2,\n",
        "        '240-400d': (240 + 400) / 2\n",
        "    }\n",
        "    df_interp['maturity_midpoint'] = df_interp['maturity_bucket'].map(bucket_midpoints)\n",
        "\n",
        "    # --- Main Interpolation Step ---\n",
        "    # Group by date and asset, then apply the interpolation helper to each group.\n",
        "    interpolated_results = df_interp.groupby(['date', 'asset']).apply(\n",
        "        _interpolate_single_curve,\n",
        "        target_tenors=target_tenors_days\n",
        "    ).reset_index()\n",
        "\n",
        "    # --- Reshape Final DataFrame ---\n",
        "    # Pivot the results to create the final time-series format.\n",
        "    if not interpolated_results.empty:\n",
        "        final_df = interpolated_results.pivot_table(\n",
        "            index=['date', 'asset'],\n",
        "            columns='tenor_days',\n",
        "            values=['crypto_rate', 'ref_rate', 'status']\n",
        "        )\n",
        "        # Clean up the hierarchical column names.\n",
        "        final_df.columns = [f\"{val}_{tenor}d\" for val, tenor in final_df.columns]\n",
        "    else:\n",
        "        final_df = pd.DataFrame() # Handle case where no interpolation was possible.\n",
        "\n",
        "    # --- Diagnostic Reporting ---\n",
        "    total_curves = len(df_interp.groupby(['date', 'asset']))\n",
        "    successful_curves = len(final_df) if not final_df.empty else 0\n",
        "\n",
        "    diagnostics = {\n",
        "        \"process_name\": \"Maturity Interpolation\",\n",
        "        \"target_tenors_days\": target_tenors_days,\n",
        "        \"total_curves_processed\": total_curves,\n",
        "        \"curves_successfully_interpolated\": successful_curves,\n",
        "        \"curves_failed_interpolation\": total_curves - successful_curves\n",
        "    }\n",
        "\n",
        "    return final_df, diagnostics\n"
      ],
      "metadata": {
        "id": "iTtZKU5Qip6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18: Time Series Construction and Gap Handling\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 18: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def construct_final_time_series(\n",
        "    interpolated_df: pd.DataFrame,\n",
        "    max_interpolation_gap: int = 7\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Constructs a complete, daily time series by handling gaps in the data.\n",
        "\n",
        "    This function takes the daily, maturity-interpolated yield curve data and\n",
        "    ensures its temporal continuity. It performs three main steps:\n",
        "    1.  Reindexes the data for each asset to a full daily calendar range, making\n",
        "        any missing days explicit with NaNs.\n",
        "    2.  Applies a tiered gap-filling strategy: linear interpolation for small\n",
        "        gaps (up to `max_interpolation_gap` days).\n",
        "    3.  Creates a quality flag for each data point, indicating whether it was\n",
        "        originally observed, interpolated over time, or extrapolated over maturity.\n",
        "\n",
        "    Args:\n",
        "        interpolated_df (pd.DataFrame):\n",
        "            The DataFrame of daily, maturity-interpolated rates from Task 17.\n",
        "            Must have a MultiIndex of (date, asset).\n",
        "        max_interpolation_gap (int, optional):\n",
        "            The maximum number of consecutive missing days to fill using linear\n",
        "            interpolation. Gaps larger than this will be left as NaN. Defaults to 7.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - pd.DataFrame: The final, dense time-series DataFrame with gaps\n",
        "              filled and quality flags.\n",
        "            - Dict[str, Any]: A diagnostic dictionary summarizing the process.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(interpolated_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input `interpolated_df` must be a pandas DataFrame.\")\n",
        "    if not isinstance(max_interpolation_gap, int) or max_interpolation_gap < 0:\n",
        "        raise ValueError(\"`max_interpolation_gap` must be a non-negative integer.\")\n",
        "    if interpolated_df.empty:\n",
        "        return pd.DataFrame(), {\"process_name\": \"Time Series Construction\", \"status\": \"Input empty, no action taken.\"}\n",
        "\n",
        "    # --- Initialization ---\n",
        "    df_final = interpolated_df.copy()\n",
        "    # Identify the rate columns that need to be interpolated.\n",
        "    rate_cols = [col for col in df_final.columns if 'rate' in col]\n",
        "    # Identify the status columns that need to be forward-filled.\n",
        "    status_cols = [col for col in df_final.columns if 'status' in col]\n",
        "\n",
        "    all_filled_dfs = []\n",
        "    initial_rows = len(df_final)\n",
        "\n",
        "    # --- Main Loop: Process each asset's time series independently ---\n",
        "    for asset, asset_df in df_final.groupby('asset'):\n",
        "        # --- Step 1: Temporal Reindexing ---\n",
        "        # Prepare the asset-specific DataFrame for reindexing.\n",
        "        asset_df = asset_df.droplevel('asset').reset_index().set_index('date')\n",
        "\n",
        "        # Create a complete daily date range for this asset.\n",
        "        start_date, end_date = asset_df.index.min(), asset_df.index.max()\n",
        "        full_date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "        # Reindex the DataFrame to the full date range, introducing NaNs for missing days.\n",
        "        asset_reindexed = asset_df.reindex(full_date_range)\n",
        "\n",
        "        # --- Step 2: Gap Classification and Systematic Filling ---\n",
        "        # Interpolate the numeric rate columns.\n",
        "        # The 'limit' parameter elegantly handles the tiered gap-filling rule.\n",
        "        asset_reindexed[rate_cols] = asset_reindexed[rate_cols].interpolate(\n",
        "            method='linear',\n",
        "            limit=max_interpolation_gap,\n",
        "            limit_direction='forward' # Fill forward up to the limit\n",
        "        )\n",
        "\n",
        "        # Forward-fill the categorical status columns to propagate status.\n",
        "        asset_reindexed[status_cols] = asset_reindexed[status_cols].ffill(\n",
        "            limit=max_interpolation_gap\n",
        "        )\n",
        "\n",
        "        # --- Step 3: Time Series Quality Assessment and Flagging ---\n",
        "        # Create a new 'source' column to track interpolated points.\n",
        "        # We identify rows that were originally NaN but are now filled.\n",
        "        original_nan_mask = asset_df.reindex(full_date_range)[rate_cols[0]].isna()\n",
        "        filled_mask = asset_reindexed[rate_cols[0]].notna()\n",
        "\n",
        "        # Assign the source based on the masks.\n",
        "        asset_reindexed['source'] = 'OBSERVED'\n",
        "        asset_reindexed.loc[original_nan_mask & filled_mask, 'source'] = 'INTERPOLATED_TIME'\n",
        "\n",
        "        # Add the asset back as a column and append to our list.\n",
        "        asset_reindexed['asset'] = asset\n",
        "        all_filled_dfs.append(asset_reindexed.reset_index().rename(columns={'index': 'date'}))\n",
        "\n",
        "    # --- Final Assembly and Cleanup ---\n",
        "    # Concatenate the processed DataFrames for all assets.\n",
        "    final_df_filled = pd.concat(all_filled_dfs, ignore_index=True)\n",
        "\n",
        "    # Drop any rows that still have NaNs (from gaps > max_interpolation_gap).\n",
        "    final_df_cleaned = final_df_filled.dropna(subset=rate_cols).reset_index(drop=True)\n",
        "\n",
        "    # Set a clean final index.\n",
        "    final_df_cleaned = final_df_cleaned.set_index(['date', 'asset']).sort_index()\n",
        "\n",
        "    # --- Diagnostic Reporting ---\n",
        "    final_rows = len(final_df_cleaned)\n",
        "    rows_added_by_interp = final_df_cleaned['source'].eq('INTERPOLATED_TIME').sum()\n",
        "\n",
        "    diagnostics = {\n",
        "        \"process_name\": \"Time Series Construction and Gap Handling\",\n",
        "        \"max_interpolation_gap_days\": max_interpolation_gap,\n",
        "        \"initial_observed_points\": initial_rows,\n",
        "        \"final_total_points\": final_rows,\n",
        "        \"points_added_by_interpolation\": int(rows_added_by_interp),\n",
        "        \"points_dropped_large_gaps\": (initial_rows + int(rows_added_by_interp)) - final_rows\n",
        "    }\n",
        "\n",
        "    return final_df_cleaned, diagnostics\n"
      ],
      "metadata": {
        "id": "11cyGo5_jR3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19: Pipeline Orchestrator Function Design\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 19: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def orchestrate_yield_curve_estimation(\n",
        "    df_master: pd.DataFrame,\n",
        "    fused_input_state: Dict[str, Any],\n",
        "    verbose: bool = False\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end pipeline for yield curve estimation.\n",
        "\n",
        "    This master orchestrator function manages the entire workflow, from initial\n",
        "    data validation to the final construction of a continuous, daily time series\n",
        "    of yield curves. It calls a sequence of modular, single-responsibility\n",
        "    functions, handling the flow of data, extraction of parameters, and\n",
        "    aggregation of diagnostic reports at each stage.\n",
        "\n",
        "    The pipeline consists of the following major phases:\n",
        "    1.  Input Validation: Rigorous checks on the input DataFrame and hyperparameters.\n",
        "    2.  Data Filtering: Removal of data based on maturity and liquidity.\n",
        "    3.  Feature Engineering: Computation of derived features and pairing of options.\n",
        "    4.  RANSAC Outlier Detection: Identification and removal of outliers.\n",
        "    5.  Closed-Form Estimation: Calculation of zero-coupon bond prices.\n",
        "    6.  Rate Conversion: Transformation of bond prices to interest rates.\n",
        "    7.  Aggregation & Interpolation: Creation of a continuous daily time series.\n",
        "\n",
        "    Args:\n",
        "        df_master (pd.DataFrame):\n",
        "            The primary input DataFrame containing raw, high-frequency option\n",
        "            market data with the specified MultiIndex structure.\n",
        "        fused_input_state (Dict[str, Any]):\n",
        "            The configuration dictionary containing all metadata and hyperparameters\n",
        "            required for the pipeline execution.\n",
        "        verbose (bool, optional):\n",
        "            If True, the diagnostic report from each major step will be printed\n",
        "            to the console. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - pd.DataFrame: The final, analysis-ready DataFrame of daily,\n",
        "              standardized yield curve time series.\n",
        "            - Dict[str, Any]: A comprehensive, nested dictionary containing the\n",
        "              diagnostic reports from every stage of the pipeline, providing a\n",
        "              full audit trail of the process.\n",
        "\n",
        "    Raises:\n",
        "        ValueError/TypeError: If initial input validation fails.\n",
        "        Exception: For any unexpected errors during the pipeline execution.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Initialize a master dictionary to store all diagnostic reports.\n",
        "    master_diagnostics: Dict[str, Any] = {}\n",
        "\n",
        "    def log_step(step_name: str, diagnostics: Dict[str, Any]):\n",
        "        \"\"\"Helper to store and optionally print diagnostics.\"\"\"\n",
        "        master_diagnostics[step_name] = diagnostics\n",
        "        if verbose:\n",
        "            print(f\"\\n--- {step_name} ---\")\n",
        "            pprint(diagnostics)\n",
        "\n",
        "    try:\n",
        "        # --- PHASE 1: INPUT VALIDATION ---\n",
        "        # Step 1: Validate the structure and schema of the input DataFrame.\n",
        "        validate_dataframe_structure(df_master=df_master)\n",
        "        # Step 2: Validate the hyperparameter dictionary.\n",
        "        validate_hyperparameter_dictionary(fused_input_state=fused_input_state)\n",
        "        # Step 3: Validate the completeness and consistency of the data.\n",
        "        validate_data_completeness(df_master=df_master)\n",
        "        log_step(\"Input Validation\", {\"status\": \"All checks passed.\"})\n",
        "\n",
        "        # Extract hyperparameters for easier access.\n",
        "        hyperparams = fused_input_state['hyperparameters']\n",
        "\n",
        "        # --- PHASE 2: DATA QUALITY FILTERING AND CLEANSING ---\n",
        "        # Step 4: Filter out options with short maturities.\n",
        "        df_filtered_maturity, diag_maturity = filter_by_maturity(\n",
        "            df_master=df_master,\n",
        "            maturity_filter_days=hyperparams['maturity_filter_days']\n",
        "        )\n",
        "        log_step(diag_maturity['filter_type'], diag_maturity)\n",
        "        if df_filtered_maturity.empty:\n",
        "            print(\"Warning: All data removed by maturity filter. Exiting.\")\n",
        "            return pd.DataFrame(), master_diagnostics\n",
        "\n",
        "        # Step 5: Filter out illiquid options with wide bid-ask spreads.\n",
        "        df_filtered_liquidity, diag_liquidity = filter_by_liquidity(\n",
        "            df_master=df_filtered_maturity,\n",
        "            liquidity_filter_spread_pct=hyperparams['liquidity_filter_spread_pct']\n",
        "        )\n",
        "        log_step(diag_liquidity['filter_type'], diag_liquidity)\n",
        "        if df_filtered_liquidity.empty:\n",
        "            print(\"Warning: All data removed by liquidity filter. Exiting.\")\n",
        "            return pd.DataFrame(), master_diagnostics\n",
        "\n",
        "        # --- PHASE 3: RANSAC OUTLIER DETECTION IMPLEMENTATION ---\n",
        "        # Step 6: Compute derived features and filter for complete call-put pairs.\n",
        "        df_paired, diag_pairing = prepare_features_and_pairs(df_master=df_filtered_liquidity)\n",
        "        log_step(diag_pairing['process_name'], diag_pairing)\n",
        "        if df_paired.empty:\n",
        "            print(\"Warning: No complete call-put pairs found. Exiting.\")\n",
        "            return pd.DataFrame(), master_diagnostics\n",
        "\n",
        "        # Step 7: Prepare the numerical inputs (X, Y arrays) for the RANSAC algorithm.\n",
        "        ransac_inputs, diag_ransac_prep = prepare_ransac_inputs(df_paired=df_paired)\n",
        "        log_step(diag_ransac_prep['process_name'], diag_ransac_prep)\n",
        "\n",
        "        # Step 8: Execute the RANSAC algorithm on all prepared groups.\n",
        "        ransac_results, diag_ransac_exec = execute_ransac_on_all_groups(\n",
        "            ransac_inputs=ransac_inputs, hyperparameters=hyperparams\n",
        "        )\n",
        "        log_step(diag_ransac_exec['process_name'], diag_ransac_exec)\n",
        "\n",
        "        # Step 9: Validate the RANSAC results based on economic/statistical criteria.\n",
        "        valid_ransac_results, diag_ransac_val = validate_and_filter_ransac_results(\n",
        "            ransac_results=ransac_results, hyperparameters=hyperparams\n",
        "        )\n",
        "        log_step(diag_ransac_val['process_name'], diag_ransac_val)\n",
        "        if not valid_ransac_results:\n",
        "            print(\"Warning: No valid RANSAC results after validation. Exiting.\")\n",
        "            return pd.DataFrame(), master_diagnostics\n",
        "\n",
        "        # --- PHASE 4 & 5: JOINT OPTIMIZATION AND RATE CONVERSION ---\n",
        "        # Step 10: Construct the final feature vectors (y, m) using only inlier data.\n",
        "        feature_vectors, diag_fv = construct_feature_vectors(\n",
        "            df_analysis_ready=df_paired, valid_ransac_results=valid_ransac_results\n",
        "        )\n",
        "        log_step(diag_fv['process_name'], diag_fv)\n",
        "\n",
        "        # Step 11: Compute the scalar summations required for the closed-form solution.\n",
        "        summations, diag_sums = compute_summations_for_all_groups(\n",
        "            feature_vectors=feature_vectors, hyperparameters=hyperparams\n",
        "        )\n",
        "        log_step(diag_sums['process_name'], diag_sums)\n",
        "\n",
        "        # Step 12: Compute the closed-form solution for the zero-coupon bond prices.\n",
        "        solutions, diag_solve = compute_closed_form_solutions(all_summations=summations)\n",
        "        log_step(diag_solve['process_name'], diag_solve)\n",
        "\n",
        "        # Step 13: Convert the bond prices to continuously compounded interest rates.\n",
        "        rates, diag_rates = convert_all_solutions_to_rates(\n",
        "            all_solutions=solutions, hyperparameters=hyperparams\n",
        "        )\n",
        "        log_step(diag_rates['process_name'], diag_rates)\n",
        "\n",
        "        # Step 14: Apply final economic validation checks to the computed rates.\n",
        "        valid_rates, diag_rate_val = validate_and_filter_rates(all_rates=rates)\n",
        "        log_step(diag_rate_val['process_name'], diag_rate_val)\n",
        "        if not valid_rates:\n",
        "            print(\"Warning: No valid rates after economic validation. Exiting.\")\n",
        "            return pd.DataFrame(), master_diagnostics\n",
        "\n",
        "        # --- PHASE 6: TEMPORAL AGGREGATION AND INTERPOLATION ---\n",
        "        # Step 15: Aggregate the high-frequency results into a structured DataFrame.\n",
        "        df_aggregated_results = aggregate_results_to_dataframe(all_rates=valid_rates)\n",
        "        log_step(\"Results Aggregation\", {\"status\": f\"Aggregated {len(df_aggregated_results)} valid points into DataFrame.\"})\n",
        "\n",
        "        # Step 16: Aggregate the hourly estimates to a daily frequency by maturity bucket.\n",
        "        df_daily_agg, diag_daily_agg = aggregate_rates_daily(results_df=df_aggregated_results)\n",
        "        log_step(diag_daily_agg['process_name'], diag_daily_agg)\n",
        "\n",
        "        # Step 17: Interpolate the bucketed daily rates to standardized target tenors.\n",
        "        df_interpolated, diag_interp = interpolate_yield_curves(\n",
        "            daily_aggregated_df=df_daily_agg,\n",
        "            target_tenors_days=hyperparams['target_tenors_days']\n",
        "        )\n",
        "        log_step(diag_interp['process_name'], diag_interp)\n",
        "\n",
        "        # Step 18: Construct the final, continuous time series by handling gaps.\n",
        "        df_final, diag_final_ts = construct_final_time_series(interpolated_df=df_interpolated)\n",
        "        log_step(diag_final_ts['process_name'], diag_final_ts)\n",
        "\n",
        "        # --- Final Return ---\n",
        "        # Return the final time series and the master diagnostic report.\n",
        "        return df_final, master_diagnostics\n",
        "\n",
        "    except (ValueError, TypeError) as e:\n",
        "        # Catch validation errors and other explicit exceptions.\n",
        "        print(f\"Pipeline execution failed. Error: {e}\")\n",
        "        # Return empty results but include diagnostics up to the point of failure.\n",
        "        return pd.DataFrame(), master_diagnostics\n",
        "    except Exception as e:\n",
        "        # Catch any other unexpected errors during execution.\n",
        "        print(f\"An unexpected error occurred during pipeline execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return pd.DataFrame(), master_diagnostics\n"
      ],
      "metadata": {
        "id": "9K4Wm7y9j8d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20: Parameter Sensitivity Analysis\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 20, Step 1: Helper function for a single pipeline run\n",
        "# ======================================================================================\n",
        "# ======================================================================================\n",
        "# NOTE: We assume the master orchestrator from Task 19,\n",
        "# `orchestrate_yield_curve_estimation`, is available in the execution scope.\n",
        "# ======================================================================================\n",
        "\n",
        "def _run_single_pipeline_instance(\n",
        "    params: Tuple,\n",
        "    param_names: List[str],\n",
        "    df_master: pd.DataFrame,\n",
        "    base_config: Dict[str, Any]\n",
        ") -> Tuple[Tuple, pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes a single, complete pipeline run with a specific parameter set.\n",
        "\n",
        "    This function serves as a self-contained wrapper around the main pipeline\n",
        "    orchestrator (`orchestrate_yield_curve_estimation`). It is specifically\n",
        "    designed to be the target function for a parallel processing pool (like\n",
        "    `multiprocessing.Pool`). It takes a tuple of parameter values, updates a\n",
        "    copy of the base configuration, runs the entire pipeline, and returns the\n",
        "    results along with the parameters that generated them. This structure is\n",
        "    essential for conducting the parameter sensitivity analysis in parallel.\n",
        "\n",
        "    Args:\n",
        "        params (Tuple):\n",
        "            A tuple containing the specific hyperparameter values for this\n",
        "            single run (e.g., (0.004, 0.10)). The order must correspond to\n",
        "            `param_names`.\n",
        "        param_names (List[str]):\n",
        "            A list of the hyperparameter names being varied in this run\n",
        "            (e.g., ['ransac_residual_sq_threshold', 'ransac_slope_tolerance_pct']).\n",
        "        df_master (pd.DataFrame):\n",
        "            The primary, raw input DataFrame containing the market data. This is\n",
        "            passed to each worker process.\n",
        "        base_config (Dict[str, Any]):\n",
        "            The base configuration dictionary, which serves as a template for\n",
        "            the run-specific configuration.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Tuple, pd.DataFrame, Dict[str, Any]]:\n",
        "            A tuple containing:\n",
        "            - Tuple: The input `params` tuple, used to identify which parameter\n",
        "              combination produced these results.\n",
        "            - pd.DataFrame: The final yield curve DataFrame produced by the\n",
        "              pipeline for this parameter set.\n",
        "            - Dict[str, Any]: The comprehensive diagnostic dictionary from the\n",
        "              pipeline run.\n",
        "    \"\"\"\n",
        "    # --- Configuration Setup ---\n",
        "    # Create a deep copy of the base configuration. This is a critical step in\n",
        "    # parallel processing to ensure that each worker operates on an independent\n",
        "    # copy of the configuration object, preventing race conditions or unintended\n",
        "    # modifications of the shared base configuration.\n",
        "    config = copy.deepcopy(base_config)\n",
        "\n",
        "    # --- Parameter Update ---\n",
        "    # Iterate through the provided parameter names and their corresponding values.\n",
        "    for name, value in zip(param_names, params):\n",
        "        # Update the 'hyperparameters' section of the copied configuration\n",
        "        # with the specific values for this experimental run.\n",
        "        config['hyperparameters'][name] = value\n",
        "\n",
        "    # --- Pipeline Execution ---\n",
        "    # Execute the main end-to-end pipeline orchestrator using the newly\n",
        "    # configured state. The `verbose` flag is explicitly set to False to\n",
        "    # prevent interleaved printing from multiple parallel worker processes,\n",
        "    # which would corrupt the console output.\n",
        "    yield_curve_df, diagnostics = orchestrate_yield_curve_estimation(\n",
        "        df_master=df_master,\n",
        "        fused_input_state=config,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # --- Result Packaging ---\n",
        "    # Return the results as a tuple. Including the input `params` in the\n",
        "    # output is essential for linking the results back to the specific\n",
        "    # hyperparameter combination that generated them during the aggregation phase.\n",
        "    return params, yield_curve_df, diagnostics\n",
        "\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 20: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def run_parameter_sensitivity_analysis(\n",
        "    df_master: pd.DataFrame,\n",
        "    base_fused_input_state: Dict[str, Any],\n",
        "    parameter_grid: Dict[str, List[Any]]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Performs a sensitivity analysis by running the pipeline across a grid of hyperparameters.\n",
        "\n",
        "    This function systematically evaluates the robustness of the yield curve\n",
        "    estimation pipeline by executing it for every combination of parameters\n",
        "    defined in the input grid. It leverages multiprocessing to accelerate the\n",
        "    computation. The results are then aggregated to compute key robustness\n",
        "    metrics, such as the stability of the estimated rates.\n",
        "\n",
        "    Args:\n",
        "        df_master (pd.DataFrame):\n",
        "            The primary input DataFrame containing raw market data.\n",
        "        base_fused_input_state (Dict[str, Any]):\n",
        "            The base configuration dictionary. This will be used as a template,\n",
        "            with hyperparameter values being updated for each run.\n",
        "        parameter_grid (Dict[str, List[Any]]):\n",
        "            A dictionary where keys are hyperparameter names (e.g.,\n",
        "            'ransac_residual_sq_threshold') and values are lists of the\n",
        "            values to test for that parameter.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "            A tuple containing:\n",
        "            - pd.DataFrame: A DataFrame summarizing the rate stability. It is\n",
        "              indexed by date and asset, with columns for the mean and standard\n",
        "              deviation of each tenor's rate across all parameter runs.\n",
        "            - pd.DataFrame: A DataFrame summarizing the coverage stability,\n",
        "              showing the mean and standard deviation of key diagnostic\n",
        "              metrics (e.g., success rates) across all parameter runs.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Parameter Grid and Testing Framework ---\n",
        "    # Extract the names and value lists from the parameter grid.\n",
        "    param_names = list(parameter_grid.keys())\n",
        "    param_values = list(parameter_grid.values())\n",
        "\n",
        "    # Create the Cartesian product of all parameter values.\n",
        "    param_combinations = list(itertools.product(*param_values))\n",
        "\n",
        "    print(f\"Starting sensitivity analysis for {len(param_combinations)} parameter combinations...\")\n",
        "\n",
        "    # Prepare arguments for the parallel execution.\n",
        "    args_for_pool = [\n",
        "        (params, param_names, df_master, base_fused_input_state)\n",
        "        for params in param_combinations\n",
        "    ]\n",
        "\n",
        "    # Use a multiprocessing pool to run the pipeline in parallel.\n",
        "    with Pool(processes=max(1, cpu_count() - 1)) as pool:\n",
        "        all_results = pool.starmap(_run_single_pipeline_instance, args_for_pool)\n",
        "\n",
        "    # --- Step 2: Robustness Metrics Computation and Analysis ---\n",
        "    # --- Rate Stability Analysis ---\n",
        "    all_rate_dfs = []\n",
        "    for params, yield_curve_df, _ in all_results:\n",
        "        if not yield_curve_df.empty:\n",
        "            # Add parameter identifiers to each result DataFrame.\n",
        "            for i, name in enumerate(param_names):\n",
        "                yield_curve_df[name] = params[i]\n",
        "            all_rate_dfs.append(yield_curve_df.reset_index())\n",
        "\n",
        "    if not all_rate_dfs:\n",
        "        print(\"Warning: No successful pipeline runs in sensitivity analysis.\")\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # Concatenate all results into a single DataFrame.\n",
        "    combined_rates_df = pd.concat(all_rate_dfs, ignore_index=True)\n",
        "\n",
        "    # Identify the rate columns to analyze.\n",
        "    rate_cols = [col for col in combined_rates_df.columns if 'rate' in col and 'status' not in col]\n",
        "\n",
        "    # Group by the original index and compute mean and std dev for each rate.\n",
        "    rate_stability_summary = combined_rates_df.groupby(['date', 'asset'])[rate_cols].agg(['mean', 'std'])\n",
        "\n",
        "    # --- Coverage Stability Analysis ---\n",
        "    all_diagnostics = [diag for _, _, diag in all_results]\n",
        "\n",
        "    # Extract key success metrics from each diagnostic report.\n",
        "    coverage_data = []\n",
        "    for params, diag in zip(param_combinations, all_diagnostics):\n",
        "        record = dict(zip(param_names, params))\n",
        "        # Example: Extracting the RANSAC validation pass rate.\n",
        "        pass_rate = diag.get('RANSAC Result Validation and Filtering', {}).get('validation_pass_rate_pct', np.nan)\n",
        "        record['ransac_pass_rate_pct'] = pass_rate\n",
        "        coverage_data.append(record)\n",
        "\n",
        "    coverage_df = pd.DataFrame.from_records(coverage_data)\n",
        "\n",
        "    # Summarize the stability of the coverage metrics.\n",
        "    coverage_stability_summary = coverage_df.describe().transpose()\n",
        "\n",
        "    # --- Step 3: Reporting and Recommendations (Return Summaries) ---\n",
        "    print(\"Sensitivity analysis complete.\")\n",
        "\n",
        "    return rate_stability_summary, coverage_stability_summary\n"
      ],
      "metadata": {
        "id": "D5FvjIvjj2Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21: Bootstrap Confidence Interval Construction\n",
        "\n",
        "# ======================================================================================\n",
        "# NOTE: We assume the full suite of pipeline functions from Tasks 7-13 are\n",
        "# available in the execution scope.\n",
        "# ======================================================================================\n",
        "\n",
        "def _run_condensed_pipeline_for_bootstrap(\n",
        "    df_sample: pd.DataFrame,\n",
        "    hyperparameters: Dict[str, Any],\n",
        "    group_name: Tuple\n",
        ") -> Optional[Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Executes a condensed, single-group run of the core estimation pipeline.\n",
        "\n",
        "    This helper function serves as the computational engine for a single bootstrap\n",
        "    iteration. It takes a DataFrame representing one bootstrap sample for a\n",
        "    specific estimation group and processes it through the essential pipeline\n",
        "    stages: RANSAC outlier detection, feature vector construction, closed-form\n",
        "    solution, and rate conversion. It is designed to fail gracefully, returning\n",
        "    None if any stage of the pipeline does not yield a valid result for the given\n",
        "    sample.\n",
        "\n",
        "    Args:\n",
        "        df_sample (pd.DataFrame):\n",
        "            A DataFrame containing a bootstrap sample of paired option data for a\n",
        "            single estimation group.\n",
        "        hyperparameters (Dict[str, Any]):\n",
        "            The configuration dictionary containing all necessary hyperparameters\n",
        "            for the RANSAC and estimation stages.\n",
        "        group_name (Tuple):\n",
        "            The unique identifier `(timestamp, asset, expiry_date)` for the\n",
        "            estimation group being processed.\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, float]]:\n",
        "            A dictionary containing the final estimated 'crypto_rate' and\n",
        "            'ref_rate' if the pipeline run is successful. Returns None if any\n",
        "            step fails (e.g., RANSAC finds no valid consensus, the linear\n",
        "            system is singular, or results are economically invalid).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- Tasks 7-9: RANSAC Outlier Detection ---\n",
        "        # Prepare the numerical inputs (X, Y arrays) for the RANSAC algorithm.\n",
        "        ransac_inputs, _ = prepare_ransac_inputs(df_paired=df_sample, min_samples_per_group=5)\n",
        "        # If the sample is too small or invalid, exit.\n",
        "        if not ransac_inputs: return None\n",
        "\n",
        "        # Execute the RANSAC algorithm on the prepared group.\n",
        "        ransac_results, _ = execute_ransac_on_all_groups(ransac_inputs, hyperparameters)\n",
        "        # If RANSAC fails to find a consensus model, exit.\n",
        "        if not ransac_results: return None\n",
        "\n",
        "        # Validate the RANSAC results based on economic and statistical criteria.\n",
        "        valid_ransac, _ = validate_and_filter_ransac_results(ransac_results, hyperparameters)\n",
        "        # If the found model is not valid (e.g., wrong slope), exit.\n",
        "        if not valid_ransac: return None\n",
        "\n",
        "        # --- Tasks 10-13: Estimation and Rate Conversion ---\n",
        "        # Construct the final feature vectors (y, m) using only the inlier data.\n",
        "        feature_vectors, _ = construct_feature_vectors(df_sample, valid_ransac)\n",
        "        # Compute the scalar summations required for the closed-form solution.\n",
        "        summations, _ = compute_summations_for_all_groups(feature_vectors, hyperparameters)\n",
        "        # Compute the closed-form solution for the zero-coupon bond prices.\n",
        "        solutions, _ = compute_closed_form_solutions(summations)\n",
        "        # If the system is singular or the solution is economically invalid, exit.\n",
        "        if not solutions: return None\n",
        "\n",
        "        # Convert the bond prices to continuously compounded interest rates.\n",
        "        rates, _ = convert_all_solutions_to_rates(solutions, hyperparameters)\n",
        "        # If the conversion fails (e.g., numerical error), exit.\n",
        "        if not rates: return None\n",
        "\n",
        "        # Since we are processing a single group, the result dictionary will have one key.\n",
        "        # Return the final rate estimates for this successful bootstrap iteration.\n",
        "        return rates[group_name]\n",
        "\n",
        "    except Exception:\n",
        "        # This broad exception catch ensures that any unexpected error within the\n",
        "        # complex pipeline (e.g., a rare numerical issue) results in a graceful\n",
        "        # failure for this single bootstrap iteration, rather than crashing the\n",
        "        # entire analysis.\n",
        "        return None\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 21, Step 1: Helper for a single bootstrap iteration\n",
        "# ======================================================================================\n",
        "\n",
        "def _run_single_bootstrap_iteration(\n",
        "    iteration_seed: int,\n",
        "    df_group_pivoted: pd.DataFrame,\n",
        "    hyperparameters: Dict[str, Any],\n",
        "    group_name: Tuple\n",
        ") -> Optional[Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Executes a single bootstrap iteration: resample, reformat, and re-estimate.\n",
        "\n",
        "    This function encapsulates the logic for one bootstrap trial. It takes the\n",
        "    original data for a group (in a pivoted format for efficient resampling),\n",
        "    creates a new sample by drawing with replacement, reconstructs the sample\n",
        "    into the long format expected by the pipeline, and then calls the condensed\n",
        "    pipeline function to get a new rate estimate.\n",
        "\n",
        "    Args:\n",
        "        iteration_seed (int):\n",
        "            The seed for the random number generator for this specific iteration,\n",
        "            ensuring that each parallel worker produces a different and\n",
        "            reproducible sample.\n",
        "        df_group_pivoted (pd.DataFrame):\n",
        "            The original, clean data for the estimation group, pivoted so that\n",
        "            each row represents a unique strike price with columns for call and\n",
        "            put data.\n",
        "        hyperparameters (Dict[str, Any]):\n",
        "            The configuration dictionary for the pipeline.\n",
        "        group_name (Tuple):\n",
        "            The unique identifier `(timestamp, asset, expiry_date)` for the\n",
        "            estimation group.\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, float]]:\n",
        "            The estimated rates for the bootstrap sample, or None if the\n",
        "            estimation fails for this sample.\n",
        "    \"\"\"\n",
        "    # --- Resampling ---\n",
        "    # Initialize a dedicated random number generator for this iteration to ensure\n",
        "    # that parallel processes are statistically independent.\n",
        "    rng = np.random.default_rng(iteration_seed)\n",
        "\n",
        "    # Determine the number of unique strike pairs in the original sample.\n",
        "    n_pairs = len(df_group_pivoted)\n",
        "    # Generate a new set of indices by sampling with replacement from the original indices.\n",
        "    resampled_indices = rng.choice(n_pairs, size=n_pairs, replace=True)\n",
        "\n",
        "    # Create the bootstrap sample by selecting rows from the pivoted DataFrame\n",
        "    # using the newly generated indices.\n",
        "    df_sample_pivoted = df_group_pivoted.iloc[resampled_indices]\n",
        "\n",
        "    # --- Reformatting ---\n",
        "    # Un-pivot (or \"melt\") the data to transform it from the wide format back to\n",
        "    # the long format required by the start of the estimation pipeline.\n",
        "    df_sample_long = df_sample_pivoted.stack(level='option_type').reset_index()\n",
        "    # Restore the original MultiIndex structure.\n",
        "    df_sample_long = df_sample_long.set_index(\n",
        "        ['timestamp', 'asset', 'expiry_date', 'strike_price', 'option_type']\n",
        "    )\n",
        "\n",
        "    # --- Data Enrichment ---\n",
        "    # The pivoted format loses some of the original columns that are constant\n",
        "    # across the group (e.g., spot_price). This step adds them back to the\n",
        "    # resampled DataFrame to ensure it has the correct schema for the pipeline.\n",
        "    # We can safely take these constant values from any row of the original data.\n",
        "    original_row = df_group_pivoted.iloc[0]\n",
        "\n",
        "    # Define the list of columns that were constant and need to be restored.\n",
        "    # Note: 'mid_price' is not included as it's specific to each option.\n",
        "    # It will be recalculated inside the condensed pipeline.\n",
        "    cols_to_restore = ['spot_price', 'futures_price', 'instrument_name', 'moneyness']\n",
        "\n",
        "    # Iterate and restore each required column.\n",
        "    for col in cols_to_restore:\n",
        "        # Check if the column exists in the hierarchical index of the pivoted frame.\n",
        "        if col in original_row.index.get_level_values(0):\n",
        "             # Assign the constant value to the new column. The value is the same\n",
        "             # for the call and put, so we can arbitrarily take it from the 'call' sub-column.\n",
        "             df_sample_long[col] = original_row[(col, 'call')]\n",
        "\n",
        "    # --- Re-estimation ---\n",
        "    # Run the condensed estimation pipeline on the fully prepared bootstrap sample.\n",
        "    return _run_condensed_pipeline_for_bootstrap(df_sample_long, hyperparameters, group_name)\n",
        "\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 21: Fused Orchestrator Function\n",
        "# ======================================================================================\n",
        "\n",
        "def construct_bootstrap_confidence_intervals(\n",
        "    df_paired: pd.DataFrame,\n",
        "    fused_input_state: Dict[str, Any],\n",
        "    group_to_analyze: Tuple,\n",
        "    n_bootstrap: int = 1000\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Constructs bootstrap confidence intervals for a single estimation group.\n",
        "\n",
        "    This function performs a non-parametric bootstrap procedure to quantify the\n",
        "    uncertainty in the estimated interest rates for a specific timestamp, asset,\n",
        "    and expiry date. It repeatedly resamples the underlying option pairs, re-runs\n",
        "    the core estimation pipeline, and computes confidence intervals from the\n",
        "    resulting distribution of estimates.\n",
        "\n",
        "    Args:\n",
        "        df_paired (pd.DataFrame):\n",
        "            The analysis-ready DataFrame of clean, paired options from Task 6.\n",
        "        fused_input_state (Dict[str, Any]):\n",
        "            The full configuration dictionary for the pipeline.\n",
        "        group_to_analyze (Tuple):\n",
        "            The specific group identifier `(timestamp, asset, expiry_date)` for\n",
        "            which to construct confidence intervals.\n",
        "        n_bootstrap (int, optional):\n",
        "            The number of bootstrap iterations to perform. Defaults to 1000.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A dictionary containing the original point estimate, the computed\n",
        "            confidence intervals, and diagnostic information about the\n",
        "            bootstrap procedure.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Bootstrap Resampling Framework ---\n",
        "    # Extract the data for the specific group to be analyzed.\n",
        "    try:\n",
        "        df_group = df_paired.loc[group_to_analyze]\n",
        "    except KeyError:\n",
        "        raise ValueError(f\"Group {group_to_analyze} not found in the input DataFrame.\")\n",
        "\n",
        "    # Get the original point estimate for comparison.\n",
        "    point_estimate_result = _run_condensed_pipeline_for_bootstrap(\n",
        "        df_group, fused_input_state['hyperparameters'], group_to_analyze\n",
        "    )\n",
        "    if point_estimate_result is None:\n",
        "        return {\"status\": \"FAILED\", \"reason\": \"Original point estimate could not be computed.\"}\n",
        "\n",
        "    # Pivot the group data once for efficient resampling.\n",
        "    df_group_pivoted = df_group.reset_index().pivot_table(\n",
        "        index=['timestamp', 'asset', 'expiry_date', 'strike_price'],\n",
        "        columns='option_type'\n",
        "    )\n",
        "\n",
        "    # Prepare arguments for the parallel bootstrap execution.\n",
        "    # Generate independent seeds for each worker process.\n",
        "    master_seed = 42\n",
        "    rng = np.random.default_rng(master_seed)\n",
        "    iteration_seeds = rng.integers(low=0, high=2**32 - 1, size=n_bootstrap)\n",
        "\n",
        "    args_for_pool = [\n",
        "        (seed, df_group_pivoted, fused_input_state['hyperparameters'], group_to_analyze)\n",
        "        for seed in iteration_seeds\n",
        "    ]\n",
        "\n",
        "    # Run bootstrap iterations in parallel.\n",
        "    with Pool(processes=max(1, cpu_count() - 1)) as pool:\n",
        "        bootstrap_results = pool.starmap(_run_single_bootstrap_iteration, args_for_pool)\n",
        "\n",
        "    # --- Step 2: Bootstrap Distribution Analysis and CI Computation ---\n",
        "    # Filter out failed iterations and collect the successful rate estimates.\n",
        "    successful_rates = [res for res in bootstrap_results if res is not None]\n",
        "\n",
        "    if not successful_rates:\n",
        "        return {\n",
        "            \"status\": \"FAILED\",\n",
        "            \"reason\": \"No bootstrap iterations were successful.\",\n",
        "            \"point_estimate\": point_estimate_result,\n",
        "            \"bootstrap_runs\": n_bootstrap,\n",
        "            \"successful_runs\": 0\n",
        "        }\n",
        "\n",
        "    crypto_rate_dist = [res['crypto_rate'] for res in successful_rates]\n",
        "    ref_rate_dist = [res['ref_rate'] for res in successful_rates]\n",
        "\n",
        "    # Compute percentile-based confidence intervals.\n",
        "    ci_95_crypto = np.percentile(crypto_rate_dist, [2.5, 97.5])\n",
        "    ci_68_crypto = np.percentile(crypto_rate_dist, [16, 84])\n",
        "    ci_95_ref = np.percentile(ref_rate_dist, [2.5, 97.5])\n",
        "    ci_68_ref = np.percentile(ref_rate_dist, [16, 84])\n",
        "\n",
        "    # --- Step 3: Bootstrap Validation and Quality Assessment ---\n",
        "    final_report = {\n",
        "        \"status\": \"SUCCESS\",\n",
        "        \"group\": group_to_analyze,\n",
        "        \"point_estimate\": {\n",
        "            \"crypto_rate\": point_estimate_result['crypto_rate'],\n",
        "            \"ref_rate\": point_estimate_result['ref_rate']\n",
        "        },\n",
        "        \"confidence_intervals\": {\n",
        "            \"crypto_rate_95_ci\": list(ci_95_crypto),\n",
        "            \"crypto_rate_68_ci\": list(ci_68_crypto),\n",
        "            \"ref_rate_95_ci\": list(ci_95_ref),\n",
        "            \"ref_rate_68_ci\": list(ci_68_ref)\n",
        "        },\n",
        "        \"diagnostics\": {\n",
        "            \"bootstrap_runs\": n_bootstrap,\n",
        "            \"successful_runs\": len(successful_rates),\n",
        "            \"success_rate_pct\": 100 * len(successful_rates) / n_bootstrap\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return final_report\n"
      ],
      "metadata": {
        "id": "4RnpbvuKlvaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22: Economic Validation and Benchmarking\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 22, Step 1: Theoretical Consistency Helper\n",
        "# ======================================================================================\n",
        "\n",
        "def _validate_theoretical_consistency(\n",
        "    yield_curves_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyzes term structure shapes and cross-asset consistency of rates.\n",
        "\n",
        "    This function performs two key theoretical checks on the final yield curve data:\n",
        "    1.  Term Structure Analysis: It calculates the slope of the yield curve\n",
        "        (e.g., 360-day rate vs. 90-day rate) for each day and asset, then\n",
        "        computes the historical distribution of curve shapes (Normal, Inverted, Flat).\n",
        "    2.  Cross-Asset Consistency: It compares the reference currency rates (ref_rate)\n",
        "        derived independently from different crypto assets (e.g., BTC and ETH).\n",
        "        Theoretically, these should be similar. The function calculates and\n",
        "        summarizes the spread between these rates.\n",
        "\n",
        "    Args:\n",
        "        yield_curves_df (pd.DataFrame):\n",
        "            The final time-series DataFrame, indexed by (date, asset), with\n",
        "            columns for rates at different tenors.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A dictionary containing the analysis results, including the\n",
        "            distribution of curve shapes and descriptive statistics of the\n",
        "            cross-asset rate spread.\n",
        "    \"\"\"\n",
        "    # Initialize the report dictionary to store results.\n",
        "    report: Dict[str, Any] = {}\n",
        "\n",
        "    # --- Term Structure Shape Analysis ---\n",
        "    # Identify the column names for the short (90d) and long (360d) ends of the curve.\n",
        "    # This approach is robust to the exact naming convention (e.g., 'crypto_rate_90d').\n",
        "    rate_cols_90d = [c for c in yield_curves_df.columns if '90d' in str(c) and 'rate' in str(c)]\n",
        "    rate_cols_360d = [c for c in yield_curves_df.columns if '360d' in str(c) and 'rate' in str(c)]\n",
        "\n",
        "    # Proceed only if both short and long tenor columns are found.\n",
        "    if rate_cols_90d and rate_cols_360d:\n",
        "        # Create a copy to avoid modifying the original DataFrame.\n",
        "        df = yield_curves_df.copy()\n",
        "        # Calculate the slope of the crypto and reference yield curves.\n",
        "        df['crypto_slope'] = df[rate_cols_360d[0]] - df[rate_cols_90d[0]]\n",
        "        df['ref_slope'] = df[rate_cols_360d[1]] - df[rate_cols_90d[1]]\n",
        "\n",
        "        # Define conditions for classifying the curve shape based on its slope.\n",
        "        # A small tolerance (0.001 or 10bps) is used to classify a curve as 'Flat'.\n",
        "        shape_conditions = [\n",
        "            (df['crypto_slope'] > 0.001),   # Steeply upward sloping\n",
        "            (df['crypto_slope'] < -0.001),  # Steeply inverted\n",
        "        ]\n",
        "        # Define the corresponding labels for each condition.\n",
        "        shape_choices = ['Normal/Upward', 'Inverted']\n",
        "        # Use numpy.select for efficient conditional assignment.\n",
        "        df['crypto_curve_shape'] = np.select(shape_conditions, shape_choices, default='Flat')\n",
        "\n",
        "        # Calculate the normalized frequency of each curve shape and store it.\n",
        "        report['term_structure_shape_dist'] = df['crypto_curve_shape'].value_counts(normalize=True).to_dict()\n",
        "\n",
        "    # --- Cross-Asset Consistency Analysis ---\n",
        "    # Check if data for both 'BTC' and 'ETH' exists to perform a comparison.\n",
        "    available_assets = set(yield_curves_df.index.get_level_values('asset'))\n",
        "    if {'BTC', 'ETH'}.issubset(available_assets):\n",
        "        # Unstack the DataFrame to place BTC and ETH data in adjacent columns.\n",
        "        df_unstacked = yield_curves_df.unstack(level='asset')\n",
        "        # Find all reference rate columns.\n",
        "        ref_rate_cols = [c for c in df_unstacked.columns if 'ref_rate' in c[0]]\n",
        "\n",
        "        # Proceed if reference rate columns for at least two assets are found.\n",
        "        if len(ref_rate_cols) > 1:\n",
        "            # Assume the first two ref_rate columns correspond to BTC and ETH.\n",
        "            # This relies on pandas' default sorting of the 'asset' level.\n",
        "            btc_ref_col = ref_rate_cols[0]\n",
        "            eth_ref_col = ref_rate_cols[1]\n",
        "\n",
        "            # Calculate the spread between the two independently derived reference rates.\n",
        "            spread = df_unstacked[btc_ref_col] - df_unstacked[eth_ref_col]\n",
        "            # Compute descriptive statistics for this spread and add to the report.\n",
        "            report['cross_asset_ref_rate_spread_stats'] = spread.describe().to_dict()\n",
        "\n",
        "    # Return the completed analysis report.\n",
        "    return report\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 22, Step 2: External Benchmark Comparison Helper\n",
        "# ======================================================================================\n",
        "\n",
        "def _compare_with_external_benchmark(\n",
        "    yield_curves_df: pd.DataFrame,\n",
        "    benchmark_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compares estimated rates with an external benchmark time series.\n",
        "\n",
        "    This function aligns the estimated yield curve data with a provided\n",
        "    benchmark time series (e.g., Aave lending rates) on their date index.\n",
        "    It then computes the correlation matrix between the estimated rates and\n",
        "    the benchmark rates to quantify their co-movement.\n",
        "\n",
        "    Args:\n",
        "        yield_curves_df (pd.DataFrame):\n",
        "            The final time-series DataFrame of estimated rates.\n",
        "        benchmark_df (pd.DataFrame):\n",
        "            A DataFrame containing the external benchmark series, indexed by date.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A dictionary containing the correlation matrix, or a status message\n",
        "            if there is no overlapping data between the inputs.\n",
        "    \"\"\"\n",
        "    # Align the two DataFrames using an inner join on their date indices.\n",
        "    # This ensures that only dates present in both datasets are included.\n",
        "    df_merged = yield_curves_df.join(benchmark_df, how='inner')\n",
        "\n",
        "    # If the merged DataFrame is empty, there's no overlapping data to analyze.\n",
        "    if df_merged.empty:\n",
        "        return {\"status\": \"No overlapping data with benchmark.\"}\n",
        "\n",
        "    # Identify all columns that represent a rate for the correlation analysis.\n",
        "    # This includes estimated rates and benchmark rates.\n",
        "    rate_cols = [c for c in df_merged.columns if 'rate' in str(c)]\n",
        "    # Calculate the pairwise correlation matrix for all rate columns.\n",
        "    correlation_matrix = df_merged[rate_cols].corr()\n",
        "\n",
        "    # Return the correlation matrix as a dictionary for easy serialization.\n",
        "    return {\"correlation_matrix\": correlation_matrix.to_dict()}\n",
        "\n",
        "# ======================================================================================\n",
        "# Task 22, Step 3: Statistical Significance Test Helper\n",
        "# ======================================================================================\n",
        "\n",
        "def _perform_statistical_tests(\n",
        "    yield_curves_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs stationarity (ADF) and cointegration tests on the rate series.\n",
        "\n",
        "    This function applies formal econometric tests to the time series of rates:\n",
        "    1.  Unit Root Test: Uses the Augmented Dickey-Fuller (ADF) test to check\n",
        "        if each rate series is stationary.\n",
        "    2.  Cointegration Test: Uses the Engle-Granger test to check for a long-run\n",
        "        equilibrium relationship between the reference rates derived from\n",
        "        BTC and ETH.\n",
        "\n",
        "    Args:\n",
        "        yield_curves_df (pd.DataFrame):\n",
        "            The final time-series DataFrame of estimated rates.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A nested dictionary containing the results and interpretations of\n",
        "            the statistical tests performed.\n",
        "    \"\"\"\n",
        "    # Initialize the report dictionary with sections for each test type.\n",
        "    report: Dict[str, Dict] = {\"unit_root_tests\": {}, \"cointegration_tests\": {}}\n",
        "\n",
        "    # --- Unit Root (ADF) Tests ---\n",
        "    # Identify all columns containing estimated rates.\n",
        "    rate_cols = [c for c in yield_curves_df.columns if 'rate' in str(c) and 'status' not in str(c)]\n",
        "    # Iterate through each rate column to test it for stationarity.\n",
        "    for col in rate_cols:\n",
        "        # Drop any missing values from the series.\n",
        "        series = yield_curves_df[col].dropna()\n",
        "        # Ensure there are enough data points for a meaningful statistical test.\n",
        "        if len(series) > 20:\n",
        "            # Perform the Augmented Dickey-Fuller test.\n",
        "            adf_result = adfuller(series)\n",
        "            # Extract the p-value from the test results.\n",
        "            p_value = adf_result[1]\n",
        "            # Interpret the result based on a 5% significance level.\n",
        "            interpretation = \"Stationary (Reject H0)\" if p_value < 0.05 else \"Non-Stationary (Fail to Reject H0)\"\n",
        "            # Store the p-value and interpretation in the report.\n",
        "            report[\"unit_root_tests\"][str(col)] = {\"p_value\": p_value, \"interpretation\": interpretation}\n",
        "\n",
        "    # --- Cointegration Tests ---\n",
        "    # Check if data for both 'BTC' and 'ETH' is available for comparison.\n",
        "    available_assets = set(yield_curves_df.index.get_level_values('asset'))\n",
        "    if {'BTC', 'ETH'}.issubset(available_assets):\n",
        "        # Unstack the DataFrame to align the BTC and ETH series.\n",
        "        df_unstacked = yield_curves_df.unstack(level='asset')\n",
        "        # Dynamically find the column names for the BTC and ETH reference rates.\n",
        "        btc_ref_cols = [c for c in df_unstacked.columns if 'ref_rate' in c[0] and 'BTC' in c[1]]\n",
        "        eth_ref_cols = [c for c in df_unstacked.columns if 'ref_rate' in c[0] and 'ETH' in c[1]]\n",
        "\n",
        "        # Proceed if both series were found.\n",
        "        if btc_ref_cols and eth_ref_cols:\n",
        "            btc_ref_col, eth_ref_col = btc_ref_cols[0], eth_ref_cols[0]\n",
        "            # Create a DataFrame with just the two series and drop any rows with missing data.\n",
        "            aligned_series = df_unstacked[[btc_ref_col, eth_ref_col]].dropna()\n",
        "            # Ensure there are enough data points for a meaningful test.\n",
        "            if len(aligned_series) > 20:\n",
        "                # Perform the Engle-Granger cointegration test.\n",
        "                coint_result = coint(aligned_series[btc_ref_col], aligned_series[eth_ref_col])\n",
        "                # Extract the p-value.\n",
        "                p_value = coint_result[1]\n",
        "                # Interpret the result based on a 5% significance level.\n",
        "                interpretation = \"Cointegrated (Reject H0)\" if p_value < 0.05 else \"Not Cointegrated (Fail to Reject H0)\"\n",
        "                # Store the result in the report.\n",
        "                report[\"cointegration_tests\"][\"BTC_vs_ETH_ref_rate\"] = {\"p_value\": p_value, \"interpretation\": interpretation}\n",
        "\n",
        "    # Return the completed report.\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "M_pEOll5oPGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "def run_full_analysis(\n",
        "    df_master: pd.DataFrame,\n",
        "    fused_input_state: Dict[str, Any],\n",
        "    parameter_grid: Optional[Dict[str, List[Any]]] = None,\n",
        "    bootstrap_group: Optional[Tuple] = None,\n",
        "    n_bootstrap: int = 1000,\n",
        "    benchmark_df: Optional[pd.DataFrame] = None,\n",
        "    verbose: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the entire yield curve analysis, from estimation to robustness checks.\n",
        "\n",
        "    This top-level orchestrator serves as the single entry point for a complete\n",
        "    research run. It coordinates the execution of the core estimation pipeline\n",
        "    and all subsequent, optional robustness and validation analyses.\n",
        "\n",
        "    The workflow is as follows:\n",
        "    1.  Runs the core estimation pipeline (Task 19) to generate the primary\n",
        "        yield curve time series.\n",
        "    2.  If a `parameter_grid` is provided, it runs the sensitivity analysis (Task 20).\n",
        "    3.  If a `bootstrap_group` is specified, it runs the bootstrap confidence\n",
        "        interval analysis (Task 21) for that specific group.\n",
        "    4.  If the core pipeline succeeds, it runs the economic and statistical\n",
        "        validation analysis (Task 22).\n",
        "\n",
        "    Args:\n",
        "        df_master (pd.DataFrame):\n",
        "            The primary input DataFrame containing raw market data.\n",
        "        fused_input_state (Dict[str, Any]):\n",
        "            The base configuration dictionary for the analysis.\n",
        "        parameter_grid (Optional[Dict[str, List[Any]]], optional):\n",
        "            The grid of parameters for the sensitivity analysis (Task 20).\n",
        "            If None, this analysis is skipped. Defaults to None.\n",
        "        bootstrap_group (Optional[Tuple], optional):\n",
        "            The specific group `(timestamp, asset, expiry_date)` for which to\n",
        "            construct bootstrap confidence intervals (Task 21). If None, this\n",
        "            analysis is skipped. Defaults to None.\n",
        "        n_bootstrap (int, optional):\n",
        "            The number of bootstrap iterations to perform if `bootstrap_group`\n",
        "            is specified. Defaults to 1000.\n",
        "        benchmark_df (Optional[pd.DataFrame], optional):\n",
        "            An external benchmark DataFrame for economic validation (Task 22).\n",
        "            If None, the benchmark comparison is skipped. Defaults to None.\n",
        "        verbose (bool, optional):\n",
        "            If True, prints the diagnostic report from the core pipeline.\n",
        "            Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]:\n",
        "            A comprehensive, nested dictionary containing all results and\n",
        "            diagnostics from the entire analysis run.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Initialize the master dictionary to store all results and diagnostics.\n",
        "    full_analysis_report: Dict[str, Any] = {}\n",
        "\n",
        "    # --- 1. Core Estimation Pipeline Execution (Task 19) ---\n",
        "    # This is the foundational step. All subsequent analyses depend on its output.\n",
        "    print(\"--- Starting Core Estimation Pipeline ---\")\n",
        "    yield_curves_df, core_diagnostics = orchestrate_yield_curve_estimation(\n",
        "        df_master=df_master,\n",
        "        fused_input_state=fused_input_state,\n",
        "        verbose=verbose\n",
        "    )\n",
        "    # Store the primary results and diagnostics.\n",
        "    full_analysis_report['core_pipeline_results'] = {\n",
        "        \"yield_curves_df\": yield_curves_df,\n",
        "        \"diagnostics\": core_diagnostics\n",
        "    }\n",
        "    print(\"--- Core Estimation Pipeline Complete ---\")\n",
        "\n",
        "    # --- 2. Parameter Sensitivity Analysis (Task 20) ---\n",
        "    # This analysis is optional and runs only if a parameter grid is provided.\n",
        "    if parameter_grid:\n",
        "        print(\"\\n--- Starting Parameter Sensitivity Analysis ---\")\n",
        "        try:\n",
        "            # Run the sensitivity analysis orchestrator.\n",
        "            rate_stability, coverage_stability = run_parameter_sensitivity_analysis(\n",
        "                df_master=df_master,\n",
        "                base_fused_input_state=fused_input_state,\n",
        "                parameter_grid=parameter_grid\n",
        "            )\n",
        "            # Store the results.\n",
        "            full_analysis_report['sensitivity_analysis'] = {\n",
        "                \"rate_stability_summary\": rate_stability,\n",
        "                \"coverage_stability_summary\": coverage_stability\n",
        "            }\n",
        "            print(\"--- Parameter Sensitivity Analysis Complete ---\")\n",
        "        except Exception as e:\n",
        "            # Ensure failure in an optional analysis does not stop the entire process.\n",
        "            print(f\"Parameter Sensitivity Analysis failed with error: {e}\")\n",
        "            full_analysis_report['sensitivity_analysis'] = {\"status\": \"FAILED\", \"error\": str(e)}\n",
        "\n",
        "    # --- 3. Bootstrap Confidence Interval Analysis (Task 21) ---\n",
        "    # This analysis is optional and runs only if a specific group is targeted.\n",
        "    if bootstrap_group:\n",
        "        print(f\"\\n--- Starting Bootstrap Analysis for group {bootstrap_group} ---\")\n",
        "        try:\n",
        "            # The bootstrap analysis requires the `df_paired` DataFrame, which is an\n",
        "            # intermediate result of the core pipeline. We regenerate it here to\n",
        "            # keep the main orchestrator's interface clean.\n",
        "            hyperparams = fused_input_state['hyperparameters']\n",
        "            df_temp, _ = filter_by_maturity(df_master, hyperparams['maturity_filter_days'])\n",
        "            df_temp, _ = filter_by_liquidity(df_temp, hyperparams['liquidity_filter_spread_pct'])\n",
        "            df_paired_for_bootstrap, _ = prepare_features_and_pairs(df_temp)\n",
        "\n",
        "            # Run the bootstrap orchestrator.\n",
        "            bootstrap_report = construct_bootstrap_confidence_intervals(\n",
        "                df_paired=df_paired_for_bootstrap,\n",
        "                fused_input_state=fused_input_state,\n",
        "                group_to_analyze=bootstrap_group,\n",
        "                n_bootstrap=n_bootstrap\n",
        "            )\n",
        "            # Store the results.\n",
        "            full_analysis_report['bootstrap_analysis'] = bootstrap_report\n",
        "            print(\"--- Bootstrap Analysis Complete ---\")\n",
        "        except Exception as e:\n",
        "            print(f\"Bootstrap Analysis failed with error: {e}\")\n",
        "            full_analysis_report['bootstrap_analysis'] = {\"status\": \"FAILED\", \"error\": str(e)}\n",
        "\n",
        "    # --- 4. Economic Validation and Benchmarking (Task 22) ---\n",
        "    # This analysis runs if the core pipeline produced a valid, non-empty result.\n",
        "    if not yield_curves_df.empty:\n",
        "        print(\"\\n--- Starting Economic Validation and Benchmarking ---\")\n",
        "        try:\n",
        "            # Run the economic validation orchestrator.\n",
        "            economic_report = run_economic_validation(\n",
        "                yield_curves_df=yield_curves_df,\n",
        "                benchmark_df=benchmark_df\n",
        "            )\n",
        "            # Store the results.\n",
        "            full_analysis_report['economic_validation'] = economic_report\n",
        "            print(\"--- Economic Validation and Benchmarking Complete ---\")\n",
        "        except Exception as e:\n",
        "            print(f\"Economic Validation failed with error: {e}\")\n",
        "            full_analysis_report['economic_validation'] = {\"status\": \"FAILED\", \"error\": str(e)}\n",
        "    else:\n",
        "        # If the core pipeline failed, skip this step.\n",
        "        print(\"\\n--- Skipping Economic Validation (No valid yield curves produced) ---\")\n",
        "        full_analysis_report['economic_validation'] = {\"status\": \"SKIPPED\"}\n",
        "\n",
        "    # Return the final, comprehensive report.\n",
        "    return full_analysis_report\n"
      ],
      "metadata": {
        "id": "PaGeBe2nqqot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}